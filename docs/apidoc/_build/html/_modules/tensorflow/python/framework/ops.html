

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>tensorflow.python.framework.ops &mdash; lingvo  documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 

  
  <script src="../../../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../../../index.html" class="icon icon-home"> lingvo
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../lingvo.html">lingvo package</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">lingvo</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
        
      <li>tensorflow.python.framework.ops</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for tensorflow.python.framework.ops</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2015 The TensorFlow Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ==============================================================================</span>
<span class="sd">&quot;&quot;&quot;Classes and functions used to construct graphs.&quot;&quot;&quot;</span>
<span class="c1"># pylint: disable=g-bad-name</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">threading</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">six</span>
<span class="kn">from</span> <span class="nn">six.moves</span> <span class="k">import</span> <span class="n">xrange</span>  <span class="c1"># pylint: disable=redefined-builtin</span>

<span class="kn">from</span> <span class="nn">tensorflow.core.framework</span> <span class="k">import</span> <span class="n">attr_value_pb2</span>
<span class="kn">from</span> <span class="nn">tensorflow.core.framework</span> <span class="k">import</span> <span class="n">function_pb2</span>
<span class="kn">from</span> <span class="nn">tensorflow.core.framework</span> <span class="k">import</span> <span class="n">graph_pb2</span>
<span class="kn">from</span> <span class="nn">tensorflow.core.framework</span> <span class="k">import</span> <span class="n">node_def_pb2</span>
<span class="kn">from</span> <span class="nn">tensorflow.core.framework</span> <span class="k">import</span> <span class="n">op_def_pb2</span>
<span class="kn">from</span> <span class="nn">tensorflow.core.framework</span> <span class="k">import</span> <span class="n">versions_pb2</span>
<span class="kn">from</span> <span class="nn">tensorflow.core.protobuf</span> <span class="k">import</span> <span class="n">config_pb2</span>
<span class="kn">from</span> <span class="nn">tensorflow.python</span> <span class="k">import</span> <span class="n">pywrap_tensorflow</span> <span class="k">as</span> <span class="n">c_api</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="k">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="k">import</span> <span class="n">core</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="k">import</span> <span class="n">tape</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">c_api_util</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">cpp_shape_inference_pb2</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">device</span> <span class="k">as</span> <span class="n">pydev</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">dtypes</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">errors</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">op_def_registry</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">registry</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">tensor_shape</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">traceable_stack</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">versions</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">control_flow_util</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.platform</span> <span class="k">import</span> <span class="n">app</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.platform</span> <span class="k">import</span> <span class="n">tf_logging</span> <span class="k">as</span> <span class="n">logging</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">compat</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">decorator_utils</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">function_utils</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">lock_util</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">tf_contextlib</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">tf_stack</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util.deprecation</span> <span class="k">import</span> <span class="n">deprecated_args</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util.tf_export</span> <span class="k">import</span> <span class="n">tf_export</span>


<span class="c1"># Temporary global switches determining if we should enable the work-in-progress</span>
<span class="c1"># calls to the C API. These will be removed once all functionality is supported.</span>
<span class="n">_USE_C_API</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">_USE_C_SHAPES</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;TF_C_API_GRAPH_CONSTRUCTION_SHAPES&quot;</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">)</span> <span class="o">!=</span> <span class="s2">&quot;0&quot;</span>


<span class="k">def</span> <span class="nf">tensor_id</span><span class="p">(</span><span class="n">tensor</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns a unique identifier for this Tensor.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">tensor</span><span class="o">.</span><span class="n">_id</span>  <span class="c1"># pylint: disable=protected-access</span>


<span class="k">class</span> <span class="nc">_UserDeviceSpec</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Store user-specified device and provide computation of merged device.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device_name_or_function</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_device_name_or_function</span> <span class="o">=</span> <span class="n">device_name_or_function</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">display_name</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device_name_or_function</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device_name_or_function</span><span class="p">):</span>
      <span class="n">dev_func</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device_name_or_function</span>
      <span class="n">func_name</span> <span class="o">=</span> <span class="n">function_utils</span><span class="o">.</span><span class="n">get_func_name</span><span class="p">(</span><span class="n">dev_func</span><span class="p">)</span>
      <span class="n">func_code</span> <span class="o">=</span> <span class="n">function_utils</span><span class="o">.</span><span class="n">get_func_code</span><span class="p">(</span><span class="n">dev_func</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">func_code</span><span class="p">:</span>
        <span class="n">fname</span> <span class="o">=</span> <span class="n">func_code</span><span class="o">.</span><span class="n">co_filename</span>
        <span class="n">lineno</span> <span class="o">=</span> <span class="n">func_code</span><span class="o">.</span><span class="n">co_firstlineno</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">fname</span> <span class="o">=</span> <span class="s2">&quot;unknown&quot;</span>
        <span class="n">lineno</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">display_name</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">&lt;</span><span class="si">%s</span><span class="s2">, </span><span class="si">%d</span><span class="s2">&gt;&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">func_name</span><span class="p">,</span> <span class="n">fname</span><span class="p">,</span> <span class="n">lineno</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">function</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device_name_or_function</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device_name_or_function</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span>
            <span class="n">callable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device_name_or_function</span><span class="p">)):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">function</span> <span class="o">=</span> <span class="n">pydev</span><span class="o">.</span><span class="n">merge_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device_name_or_function</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_NullContextmanager</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

  <span class="k">def</span> <span class="nf">__enter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">pass</span>

  <span class="k">def</span> <span class="nf">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">type_arg</span><span class="p">,</span> <span class="n">value_arg</span><span class="p">,</span> <span class="n">traceback_arg</span><span class="p">):</span>
    <span class="k">return</span> <span class="kc">False</span>  <span class="c1"># False values do not suppress exceptions</span>


<span class="k">def</span> <span class="nf">_override_helper</span><span class="p">(</span><span class="n">clazz_object</span><span class="p">,</span> <span class="n">operator</span><span class="p">,</span> <span class="n">func</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Overrides (string) operator on Tensors to call func.</span>

<span class="sd">  Args:</span>
<span class="sd">    clazz_object: the class to override for; either Tensor or SparseTensor.</span>
<span class="sd">    operator: the string name of the operator to override.</span>
<span class="sd">    func: the function that replaces the overridden operator.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If operator has already been overwritten,</span>
<span class="sd">      or if operator is not allowed to be overwritten.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">existing</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">clazz_object</span><span class="p">,</span> <span class="n">operator</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">existing</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="c1"># Check to see if this is a default method-wrapper or slot wrapper which</span>
    <span class="c1"># will be true for the comparison operators.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">existing</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="nb">object</span><span class="o">.</span><span class="fm">__lt__</span><span class="p">)):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;operator </span><span class="si">%s</span><span class="s2"> cannot be overwritten again on class </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span>
                       <span class="p">(</span><span class="n">operator</span><span class="p">,</span> <span class="n">clazz_object</span><span class="p">))</span>
  <span class="k">if</span> <span class="n">operator</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">OVERLOADABLE_OPERATORS</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Overriding </span><span class="si">%s</span><span class="s2"> is disallowed&quot;</span> <span class="o">%</span> <span class="n">operator</span><span class="p">)</span>
  <span class="nb">setattr</span><span class="p">(</span><span class="n">clazz_object</span><span class="p">,</span> <span class="n">operator</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_as_graph_element</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Convert `obj` to a graph element if possible, otherwise return `None`.</span>

<span class="sd">  Args:</span>
<span class="sd">    obj: Object to convert.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The result of `obj._as_graph_element()` if that method is available;</span>
<span class="sd">        otherwise `None`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">conv_fn</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="s2">&quot;_as_graph_element&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">conv_fn</span> <span class="ow">and</span> <span class="n">callable</span><span class="p">(</span><span class="n">conv_fn</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">conv_fn</span><span class="p">()</span>
  <span class="k">return</span> <span class="kc">None</span>


<span class="n">_TENSOR_LIKE_TYPES</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">is_dense_tensor_like</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;EXPERIMENTAL: Returns true if `t` implements the tensor interface.</span>

<span class="sd">  See `register_dense_tensor_like_type()` for the current definition of a</span>
<span class="sd">  &quot;tensor-like type&quot;.</span>

<span class="sd">  Args:</span>
<span class="sd">    t: An object.</span>

<span class="sd">  Returns:</span>
<span class="sd">    True iff `t` is an instance of one of the registered &quot;tensor-like&quot; types.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">_TENSOR_LIKE_TYPES</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">register_dense_tensor_like_type</span><span class="p">(</span><span class="n">tensor_type</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;EXPERIMENTAL: Registers `tensor_type` as implementing the tensor interface.</span>

<span class="sd">  A &quot;tensor-like type&quot; can represent a single dense tensor, and implements</span>
<span class="sd">  the `name` and `dtype` properties.</span>

<span class="sd">  Args:</span>
<span class="sd">    tensor_type: A type implementing the tensor interface.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If `tensor_type` does not implement the tensor interface.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor_type</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="nb">property</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Type </span><span class="si">%s</span><span class="s2"> does not define a `name` property&quot;</span> <span class="o">%</span>
                      <span class="n">tensor_type</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
  <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Type </span><span class="si">%s</span><span class="s2"> does not define a `name` property&quot;</span> <span class="o">%</span>
                    <span class="n">tensor_type</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor_type</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="nb">property</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Type </span><span class="si">%s</span><span class="s2"> does not define a `dtype` property&quot;</span> <span class="o">%</span>
                      <span class="n">tensor_type</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
  <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Type </span><span class="si">%s</span><span class="s2"> does not define a `dtype` property&quot;</span> <span class="o">%</span>
                    <span class="n">tensor_type</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
  <span class="c1"># We expect this list to be small, so choose quadratic complexity</span>
  <span class="c1"># for registration, so that we have a tuple that can be used for</span>
  <span class="c1"># more efficient `isinstance` checks later.</span>
  <span class="k">global</span> <span class="n">_TENSOR_LIKE_TYPES</span>
  <span class="n">_TENSOR_LIKE_TYPES</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">_TENSOR_LIKE_TYPES</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="n">tensor_type</span><span class="p">])</span>


<span class="k">def</span> <span class="nf">uid</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;A unique (within this program execution) integer.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">c_api</span><span class="o">.</span><span class="n">TFE_Py_UID</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">numpy_text</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">is_repr</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Human readable representation of a tensor&#39;s numpy value.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_numpy_compatible</span><span class="p">:</span>
    <span class="n">text</span> <span class="o">=</span> <span class="nb">repr</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span> <span class="k">if</span> <span class="n">is_repr</span> <span class="k">else</span> <span class="nb">str</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;&lt;unprintable&gt;&quot;</span>
  <span class="k">if</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="ow">in</span> <span class="n">text</span><span class="p">:</span>
    <span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="n">text</span>
  <span class="k">return</span> <span class="n">text</span>


<span class="c1"># NOTE(ebrevdo): Do not subclass this.  If you do, I will break you on purpose.</span>
<span class="k">class</span> <span class="nc">_TensorLike</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Internal cls for grouping Tensor, SparseTensor, ..., for is_instance.&quot;&quot;&quot;</span>
  <span class="k">pass</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;Tensor&quot;</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">Tensor</span><span class="p">(</span><span class="n">_TensorLike</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Represents one of the outputs of an `Operation`.</span>

<span class="sd">  A `Tensor` is a symbolic handle to one of the outputs of an</span>
<span class="sd">  `Operation`. It does not hold the values of that operation&#39;s output,</span>
<span class="sd">  but instead provides a means of computing those values in a</span>
<span class="sd">  TensorFlow @{tf.Session}.</span>

<span class="sd">  This class has two primary purposes:</span>

<span class="sd">  1. A `Tensor` can be passed as an input to another `Operation`.</span>
<span class="sd">     This builds a dataflow connection between operations, which</span>
<span class="sd">     enables TensorFlow to execute an entire `Graph` that represents a</span>
<span class="sd">     large, multi-step computation.</span>

<span class="sd">  2. After the graph has been launched in a session, the value of the</span>
<span class="sd">     `Tensor` can be computed by passing it to</span>
<span class="sd">     @{tf.Session.run}.</span>
<span class="sd">     `t.eval()` is a shortcut for calling</span>
<span class="sd">     `tf.get_default_session().run(t)`.</span>

<span class="sd">  In the following example, `c`, `d`, and `e` are symbolic `Tensor`</span>
<span class="sd">  objects, whereas `result` is a numpy array that stores a concrete</span>
<span class="sd">  value:</span>

<span class="sd">  ```python</span>
<span class="sd">  # Build a dataflow graph.</span>
<span class="sd">  c = tf.constant([[1.0, 2.0], [3.0, 4.0]])</span>
<span class="sd">  d = tf.constant([[1.0, 1.0], [0.0, 1.0]])</span>
<span class="sd">  e = tf.matmul(c, d)</span>

<span class="sd">  # Construct a `Session` to execute the graph.</span>
<span class="sd">  sess = tf.Session()</span>

<span class="sd">  # Execute the graph and store the value that `e` represents in `result`.</span>
<span class="sd">  result = sess.run(e)</span>
<span class="sd">  ```</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="c1"># List of Python operators that we allow to override.</span>
  <span class="n">OVERLOADABLE_OPERATORS</span> <span class="o">=</span> <span class="p">{</span>
      <span class="c1"># Binary.</span>
      <span class="s2">&quot;__add__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__radd__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__sub__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__rsub__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__mul__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__rmul__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__div__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__rdiv__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__truediv__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__rtruediv__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__floordiv__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__rfloordiv__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__mod__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__rmod__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__lt__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__le__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__gt__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__ge__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__and__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__rand__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__or__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__ror__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__xor__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__rxor__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__getitem__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__pow__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__rpow__&quot;</span><span class="p">,</span>
      <span class="c1"># Unary.</span>
      <span class="s2">&quot;__invert__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__neg__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__abs__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__matmul__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__rmatmul__&quot;</span>
  <span class="p">}</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="n">value_index</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates a new `Tensor`.</span>

<span class="sd">    Args:</span>
<span class="sd">      op: An `Operation`. `Operation` that computes this tensor.</span>
<span class="sd">      value_index: An `int`. Index of the operation&#39;s endpoint that produces</span>
<span class="sd">        this tensor.</span>
<span class="sd">      dtype: A `DType`. Type of elements stored in this tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: If the op is not an `Operation`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">Operation</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;op needs to be an Operation: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">op</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_op</span> <span class="o">=</span> <span class="n">op</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_value_index</span> <span class="o">=</span> <span class="n">value_index</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>

    <span class="c1"># This will be set by self.shape().</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_shape_val</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># List of operations that use this Tensor as input.  We maintain this list</span>
    <span class="c1"># to easily navigate a computation graph.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_consumers</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">_USE_C_SHAPES</span><span class="p">:</span>
      <span class="c1"># Attributes used for C++ shape inference. Not inspected, only forwarded.</span>
      <span class="c1"># If set, will be a HandleData object from cpp_shape_inference.proto.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_handle_data</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_id</span> <span class="o">=</span> <span class="n">uid</span><span class="p">()</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">op</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The `Operation` that produces this tensor as an output.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The `DType` of elements in this tensor.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">graph</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The `Graph` that contains this tensor.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="o">.</span><span class="n">graph</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The string name of this tensor.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="o">.</span><span class="n">name</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Operation was not named: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="p">)</span>
    <span class="k">return</span> <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">:</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_value_index</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">device</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The name of the device on which this tensor will be produced, or None.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="o">.</span><span class="n">device</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the `TensorShape` that represents the shape of this tensor.</span>

<span class="sd">    The shape is computed using shape inference functions that are</span>
<span class="sd">    registered in the Op for each `Operation`.  See</span>
<span class="sd">    @{tf.TensorShape}</span>
<span class="sd">    for more details of what a shape represents.</span>

<span class="sd">    The inferred shape of a tensor is used to provide shape</span>
<span class="sd">    information without having to launch the graph in a session. This</span>
<span class="sd">    can be used for debugging, and providing early error messages. For</span>
<span class="sd">    example:</span>

<span class="sd">    ```python</span>
<span class="sd">    c = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])</span>

<span class="sd">    print(c.shape)</span>
<span class="sd">    ==&gt; TensorShape([Dimension(2), Dimension(3)])</span>

<span class="sd">    d = tf.constant([[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]])</span>

<span class="sd">    print(d.shape)</span>
<span class="sd">    ==&gt; TensorShape([Dimension(4), Dimension(2)])</span>

<span class="sd">    # Raises a ValueError, because `c` and `d` do not have compatible</span>
<span class="sd">    # inner dimensions.</span>
<span class="sd">    e = tf.matmul(c, d)</span>

<span class="sd">    f = tf.matmul(c, d, transpose_a=True, transpose_b=True)</span>

<span class="sd">    print(f.shape)</span>
<span class="sd">    ==&gt; TensorShape([Dimension(3), Dimension(4)])</span>
<span class="sd">    ```</span>

<span class="sd">    In some cases, the inferred shape may have unknown dimensions. If</span>
<span class="sd">    the caller has additional information about the values of these</span>
<span class="sd">    dimensions, `Tensor.set_shape()` can be used to augment the</span>
<span class="sd">    inferred shape.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `TensorShape` representing the shape of this tensor.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape_val</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">_USE_C_SHAPES</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_shape_val</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_c_api_shape</span><span class="p">()</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Call set_shape_and_handle_data_for_outputs in topological order on all</span>
        <span class="c1"># ops that are needed to compute self.op&#39;s shape. We do this instead of</span>
        <span class="c1"># having set_shape_and_handle_data_for_outputs recursively call</span>
        <span class="c1"># Operation.shape on self.op.inputs to overflowing the call stack.</span>
        <span class="n">need_shapes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_input_ops_without_shapes</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">op</span><span class="p">)</span>
        <span class="n">need_shapes</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">op</span><span class="p">:</span> <span class="n">op</span><span class="o">.</span><span class="n">_id</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">op</span> <span class="ow">in</span> <span class="n">need_shapes</span><span class="p">:</span>
          <span class="n">set_shape_and_handle_data_for_outputs</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape_val</span>

  <span class="k">def</span> <span class="nf">_get_input_ops_without_shapes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target_op</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns ops needing shape inference to compute target_op&#39;s shape.&quot;&quot;&quot;</span>
    <span class="n">result</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">stack</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="p">]</span>
    <span class="n">visited</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="k">while</span> <span class="n">stack</span><span class="p">:</span>
      <span class="n">op</span> <span class="o">=</span> <span class="n">stack</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
      <span class="k">if</span> <span class="n">op</span> <span class="ow">in</span> <span class="n">visited</span><span class="p">:</span> <span class="k">continue</span>
      <span class="n">result</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>
      <span class="n">stack</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">op</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">op</span><span class="o">.</span><span class="n">inputs</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">_shape_val</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span>
      <span class="n">visited</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>

  <span class="k">def</span> <span class="nf">_c_api_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the TensorShape of this tensor according to the C API.&quot;&quot;&quot;</span>
    <span class="n">c_graph</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_c_graph</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="n">shape_vector</span><span class="p">,</span> <span class="n">unknown_shape</span> <span class="o">=</span> <span class="n">c_api</span><span class="o">.</span><span class="n">TF_GraphGetTensorShapeHelper</span><span class="p">(</span>
        <span class="n">c_graph</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_as_tf_output</span><span class="p">())</span>
    <span class="k">if</span> <span class="n">unknown_shape</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">unknown_shape</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">shape_vector</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span> <span class="k">if</span> <span class="n">d</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span> <span class="k">else</span> <span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">shape_vector</span><span class="p">]</span>
      <span class="k">return</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">(</span><span class="n">shape_vector</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Tensor._shape is private, use Tensor.shape &quot;</span>
                    <span class="s2">&quot;instead. Tensor._shape will eventually be removed.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span>

  <span class="nd">@_shape</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
        <span class="s2">&quot;Tensor._shape cannot be assigned, use Tensor.set_shape instead.&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
          <span class="s2">&quot;Tensor objects are not iterable when eager execution is not &quot;</span>
          <span class="s2">&quot;enabled. To iterate over this tensor use tf.map_fn.&quot;</span><span class="p">)</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape_tuple</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Cannot iterate over a tensor with unknown shape.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">shape</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Cannot iterate over a scalar tensor.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
          <span class="s2">&quot;Cannot iterate over a tensor with unknown first dimension.&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
      <span class="k">yield</span> <span class="bp">self</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

  <span class="k">def</span> <span class="nf">_shape_as_list</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="p">[</span><span class="n">dim</span><span class="o">.</span><span class="n">value</span> <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">dims</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="kc">None</span>

  <span class="k">def</span> <span class="nf">_shape_tuple</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape_as_list</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="kc">None</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_rank</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Integer rank of this Tensor, if known, else None.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Integer rank or None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span>

  <span class="k">def</span> <span class="nf">get_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Alias of Tensor.shape.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span>

  <span class="k">def</span> <span class="nf">set_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Updates the shape of this tensor.</span>

<span class="sd">    This method can be called multiple times, and will merge the given</span>
<span class="sd">    `shape` with the current shape of this tensor. It can be used to</span>
<span class="sd">    provide additional information about the shape of this tensor that</span>
<span class="sd">    cannot be inferred from the graph alone. For example, this can be used</span>
<span class="sd">    to provide additional information about the shapes of images:</span>

<span class="sd">    ```python</span>
<span class="sd">    _, image_data = tf.TFRecordReader(...).read(...)</span>
<span class="sd">    image = tf.image.decode_png(image_data, channels=3)</span>

<span class="sd">    # The height and width dimensions of `image` are data dependent, and</span>
<span class="sd">    # cannot be computed without executing the op.</span>
<span class="sd">    print(image.shape)</span>
<span class="sd">    ==&gt; TensorShape([Dimension(None), Dimension(None), Dimension(3)])</span>

<span class="sd">    # We know that each image in this dataset is 28 x 28 pixels.</span>
<span class="sd">    image.set_shape([28, 28, 3])</span>
<span class="sd">    print(image.shape)</span>
<span class="sd">    ==&gt; TensorShape([Dimension(28), Dimension(28), Dimension(3)])</span>
<span class="sd">    ```</span>

<span class="sd">    Args:</span>
<span class="sd">      shape: A `TensorShape` representing the shape of this tensor, a</span>
<span class="sd">      `TensorShapeProto`, a list, a tuple, or None.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If `shape` is not compatible with the current shape of</span>
<span class="sd">        this tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">_USE_C_SHAPES</span><span class="p">:</span>  <span class="c1"># pylint: disable=protected-access</span>
      <span class="c1"># Reset cached shape.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_shape_val</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_shape_val</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">merge_with</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>

    <span class="c1"># Update C shape even if _USE_C_SHAPES = False, since we still want</span>
    <span class="c1"># set_shape to be reflected in the C API graph for when we run it.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">):</span>
      <span class="n">shape</span> <span class="o">=</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">dim_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="n">shape</span><span class="o">.</span><span class="n">dims</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">unknown_shape</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">unknown_shape</span> <span class="o">=</span> <span class="kc">False</span>
      <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="n">shape</span><span class="o">.</span><span class="n">dims</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">dim</span><span class="o">.</span><span class="n">value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
          <span class="n">dim_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">dim_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dim</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">c_api</span><span class="o">.</span><span class="n">TF_GraphSetTensorShape_wrapper</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_c_graph</span><span class="p">,</span>  <span class="c1"># pylint: disable=protected-access</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_as_tf_output</span><span class="p">(),</span>
          <span class="n">dim_list</span><span class="p">,</span>
          <span class="n">unknown_shape</span><span class="p">)</span>
    <span class="k">except</span> <span class="n">errors</span><span class="o">.</span><span class="n">InvalidArgumentError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="c1"># Convert to ValueError for backwards compatibility.</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">))</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">value_index</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The index of this tensor in the outputs of its `Operation`.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_value_index</span>

  <span class="k">def</span> <span class="nf">consumers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a list of `Operation`s that consume this tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A list of `Operation`s.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">consumer_names</span> <span class="o">=</span> <span class="n">c_api</span><span class="o">.</span><span class="n">TF_OperationOutputConsumers_wrapper</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_as_tf_output</span><span class="p">())</span>
    <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">return</span> <span class="p">[</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">_get_operation_by_name_unsafe</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">consumer_names</span>
    <span class="p">]</span>
    <span class="c1"># pylint: enable=protected-access</span>

  <span class="k">def</span> <span class="nf">_as_node_def_input</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Return a value to use for the NodeDef &quot;input&quot; attribute.</span>

<span class="sd">    The returned string can be used in a NodeDef &quot;input&quot; attribute</span>
<span class="sd">    to indicate that the NodeDef uses this Tensor as input.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: if this Tensor&#39;s Operation does not have a name.</span>

<span class="sd">    Returns:</span>
<span class="sd">      a string.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="o">.</span><span class="n">name</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Operation was not named: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_value_index</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="o">.</span><span class="n">name</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">:</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_value_index</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_as_tf_output</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">return</span> <span class="n">c_api_util</span><span class="o">.</span><span class="n">tf_output</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">_c_op</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value_index</span><span class="p">)</span>
    <span class="c1"># pylint: enable=protected-access</span>

  <span class="k">def</span> <span class="nf">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="s2">&quot;Tensor(</span><span class="se">\&quot;</span><span class="si">%s</span><span class="se">\&quot;</span><span class="si">%s%s%s</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="p">(</span><span class="s2">&quot;, shape=</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_shape</span><span class="p">())</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
        <span class="p">(</span><span class="s2">&quot;, dtype=</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="p">(</span><span class="s2">&quot;, device=</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="s2">&quot;&lt;tf.Tensor &#39;</span><span class="si">%s</span><span class="s2">&#39; shape=</span><span class="si">%s</span><span class="s2"> dtype=</span><span class="si">%s</span><span class="s2">&gt;&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_shape</span><span class="p">(),</span>
                                                   <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">__hash__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># Necessary to support Python&#39;s collection membership operators</span>
    <span class="k">return</span> <span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">__eq__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
    <span class="c1"># Necessary to support Python&#39;s collection membership operators</span>
    <span class="k">return</span> <span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">==</span> <span class="nb">id</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">__copy__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># Make sure _shape_val is computed before we copy.</span>
    <span class="c1"># TODO(b/77597810): get rid of Tensor copies.</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape_val</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">set_shape_and_handle_data_for_outputs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">op</span><span class="p">)</span>
    <span class="bp">cls</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span>
    <span class="n">result</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span>
    <span class="n">result</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>

  <span class="c1"># NOTE(mrry): This enables the Tensor&#39;s overloaded &quot;right&quot; binary</span>
  <span class="c1"># operators to run when the left operand is an ndarray, because it</span>
  <span class="c1"># accords the Tensor class higher priority than an ndarray, or a</span>
  <span class="c1"># numpy matrix.</span>
  <span class="c1"># TODO(mrry): Convert this to using numpy&#39;s __numpy_ufunc__</span>
  <span class="c1"># mechanism, which allows more control over how Tensors interact</span>
  <span class="c1"># with ndarrays.</span>
  <span class="n">__array_priority__</span> <span class="o">=</span> <span class="mi">100</span>

  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">_override_operator</span><span class="p">(</span><span class="n">operator</span><span class="p">,</span> <span class="n">func</span><span class="p">):</span>
    <span class="n">_override_helper</span><span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">operator</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">__bool__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Dummy method to prevent a tensor from being used as a Python `bool`.</span>

<span class="sd">    This overload raises a `TypeError` when the user inadvertently</span>
<span class="sd">    treats a `Tensor` as a boolean (e.g. in an `if` statement). For</span>
<span class="sd">    example:</span>

<span class="sd">    ```python</span>
<span class="sd">    if tf.constant(True):  # Will raise.</span>
<span class="sd">      # ...</span>

<span class="sd">    if tf.constant(5) &lt; tf.constant(7):  # Will raise.</span>
<span class="sd">      # ...</span>
<span class="sd">    ```</span>

<span class="sd">    This disallows ambiguities between testing the Python value vs testing the</span>
<span class="sd">    dynamic condition of the `Tensor`.</span>

<span class="sd">    Raises:</span>
<span class="sd">      `TypeError`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Using a `tf.Tensor` as a Python `bool` is not allowed. &quot;</span>
                    <span class="s2">&quot;Use `if t is not None:` instead of `if t:` to test if a &quot;</span>
                    <span class="s2">&quot;tensor is defined, and use TensorFlow ops such as &quot;</span>
                    <span class="s2">&quot;tf.cond to execute subgraphs conditioned on the value of &quot;</span>
                    <span class="s2">&quot;a tensor.&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">__nonzero__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Dummy method to prevent a tensor from being used as a Python `bool`.</span>

<span class="sd">    This is the Python 2.x counterpart to `__bool__()` above.</span>

<span class="sd">    Raises:</span>
<span class="sd">      `TypeError`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Using a `tf.Tensor` as a Python `bool` is not allowed. &quot;</span>
                    <span class="s2">&quot;Use `if t is not None:` instead of `if t:` to test if a &quot;</span>
                    <span class="s2">&quot;tensor is defined, and use TensorFlow ops such as &quot;</span>
                    <span class="s2">&quot;tf.cond to execute subgraphs conditioned on the value of &quot;</span>
                    <span class="s2">&quot;a tensor.&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">eval</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">session</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Evaluates this tensor in a `Session`.</span>

<span class="sd">    Calling this method will execute all preceding operations that</span>
<span class="sd">    produce the inputs needed for the operation that produces this</span>
<span class="sd">    tensor.</span>

<span class="sd">    *N.B.* Before invoking `Tensor.eval()`, its graph must have been</span>
<span class="sd">    launched in a session, and either a default session must be</span>
<span class="sd">    available, or `session` must be specified explicitly.</span>

<span class="sd">    Args:</span>
<span class="sd">      feed_dict: A dictionary that maps `Tensor` objects to feed values.</span>
<span class="sd">        See @{tf.Session.run} for a</span>
<span class="sd">        description of the valid feed values.</span>
<span class="sd">      session: (Optional.) The `Session` to be used to evaluate this tensor. If</span>
<span class="sd">        none, the default session will be used.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A numpy array corresponding to the value of this tensor.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_eval_using_default_session</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">graph</span><span class="p">,</span> <span class="n">session</span><span class="p">)</span>


<span class="c1"># TODO(agarwal): consider getting rid of this.</span>
<span class="k">class</span> <span class="nc">_EagerTensorBase</span><span class="p">(</span><span class="n">Tensor</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Base class for EagerTensor.&quot;&quot;&quot;</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># Note: using the intern table directly here as this is</span>
    <span class="c1"># performance-sensitive in some models.</span>
    <span class="k">return</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">_INTERN_TABLE</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_datatype_enum</span><span class="p">()]</span>  <span class="c1"># pylint: disable=protected-access</span>

  <span class="k">def</span> <span class="nf">numpy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a numpy array or a scalar with the same contents as the Tensor.</span>

<span class="sd">    TODO(ashankar,agarwal): Perhaps this should NOT reference the underlying</span>
<span class="sd">    buffer but instead always explicitly copy? Note that currently it may or may</span>
<span class="sd">    not copy based on whether the numpy data is properly aligned or not.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A numpy array or a scalar. Numpy array may share memory with the</span>
<span class="sd">      Tensor object. Any changes to one may be reflected in the other. A scalar</span>
<span class="sd">      value is returned when self has rank 0.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: if the type of this Tensor is not representable in numpy.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">resource</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Resource handles are not convertible to numpy.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cpu_nograd</span><span class="p">()</span><span class="o">.</span><span class="n">_numpy</span><span class="p">()</span>  <span class="c1"># pylint: disable=protected-access</span>

  <span class="c1"># __int__, __float__ and __index__ may copy the tensor to CPU and</span>
  <span class="c1"># only work for scalars; values are cast as per numpy.</span>
  <span class="k">def</span> <span class="nf">__int__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>

  <span class="k">def</span> <span class="nf">__float__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>

  <span class="k">def</span> <span class="nf">__index__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>

  <span class="k">def</span> <span class="nf">__array__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">__format__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">format_spec</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="fm">__format__</span><span class="p">(</span><span class="n">format_spec</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_numpy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">__copy__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># Eager Tensors are immutable so it&#39;s safe to return themselves as a copy.</span>
    <span class="k">return</span> <span class="bp">self</span>

  <span class="k">def</span> <span class="nf">__deepcopy__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memo</span><span class="p">):</span>
    <span class="c1"># Eager Tensors are immutable so it&#39;s safe to return themselves as a copy.</span>
    <span class="k">del</span> <span class="n">memo</span>
    <span class="k">return</span> <span class="bp">self</span>

  <span class="k">def</span> <span class="nf">_datatype_enum</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">_shape_tuple</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The shape of this Tensor, as a tuple.</span>

<span class="sd">    This is more performant than tuple(shape().as_list()) as it avoids</span>
<span class="sd">    two list and one object creation. Marked private for now as from an API</span>
<span class="sd">    perspective, it would be better to have a single performant way of</span>
<span class="sd">    getting a shape rather than exposing shape() and shape_tuple()</span>
<span class="sd">    (and heaven forbid, shape_list() etc. as well!). Punting on that for now,</span>
<span class="sd">    but ideally one would work things out and remove the need for this method.</span>

<span class="sd">    Returns:</span>
<span class="sd">      tuple with the shape.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">_rank</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Integer rank of this Tensor.</span>

<span class="sd">    Unlike regular Tensors, the rank is always known for EagerTensors.</span>

<span class="sd">    This is more performant than len(self._shape_tuple())</span>

<span class="sd">    Returns:</span>
<span class="sd">      Integer rank</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">_copy_to_device</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>  <span class="c1"># pylint: disable=redefined-outer-name</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="s2">&quot;tf.Tensor(</span><span class="si">%s</span><span class="s2">, shape=</span><span class="si">%s</span><span class="s2">, dtype=</span><span class="si">%s</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">numpy_text</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="s2">&quot;&lt;tf.Tensor: id=</span><span class="si">%s</span><span class="s2">, shape=</span><span class="si">%s</span><span class="s2">, dtype=</span><span class="si">%s</span><span class="s2">, numpy=</span><span class="si">%s</span><span class="s2">&gt;&quot;</span> <span class="o">%</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_id</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">numpy_text</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">is_repr</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>

  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">_override_operator</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">func</span><span class="p">):</span>
    <span class="nb">setattr</span><span class="p">(</span><span class="n">_EagerTensorBase</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_copy_nograd</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">device_name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Copies tensor to dest device, but doesn&#39;t record the operation.&quot;&quot;&quot;</span>
    <span class="c1"># pylint: disable=protected-access</span>
    <span class="c1"># Creates a new tensor on the dest device.</span>
    <span class="k">if</span> <span class="n">ctx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">ctx</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">device_name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">device_name</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">device_name</span>
    <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">new_tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_copy_to_device</span><span class="p">(</span><span class="n">context</span><span class="o">=</span><span class="n">ctx</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device_name</span><span class="p">)</span>
    <span class="k">except</span> <span class="n">core</span><span class="o">.</span><span class="n">_NotOkStatusException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="n">six</span><span class="o">.</span><span class="n">raise_from</span><span class="p">(</span><span class="n">core</span><span class="o">.</span><span class="n">_status_to_exception</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">code</span><span class="p">,</span> <span class="n">e</span><span class="o">.</span><span class="n">message</span><span class="p">),</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">new_tensor</span>

  <span class="k">def</span> <span class="nf">_copy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">device_name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Copies tensor to dest device.&quot;&quot;&quot;</span>
    <span class="n">new_tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_copy_nograd</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">device_name</span><span class="p">)</span>
    <span class="c1"># Record the copy on tape and define backprop copy as well.</span>
    <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
      <span class="n">self_device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
      <span class="k">def</span> <span class="nf">grad_fun</span><span class="p">(</span><span class="n">dresult</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">dresult</span><span class="o">.</span><span class="n">_copy</span><span class="p">(</span><span class="n">device_name</span><span class="o">=</span><span class="n">self_device</span><span class="p">)]</span>
      <span class="n">tape</span><span class="o">.</span><span class="n">record_operation</span><span class="p">(</span><span class="s2">&quot;_copy&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">new_tensor</span><span class="p">],</span> <span class="p">[</span><span class="bp">self</span><span class="p">],</span> <span class="n">grad_fun</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">new_tensor</span>
    <span class="c1"># pylint: enable=protected-access</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># pylint: disable=access-member-before-definition</span>
      <span class="c1"># `_tensor_shape` is declared and defined in the definition of</span>
      <span class="c1"># `EagerTensor`, in C.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_shape</span> <span class="o">=</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape_tuple</span><span class="p">())</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_shape</span>

  <span class="k">def</span> <span class="nf">get_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Alias of Tensor.shape.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span>

  <span class="k">def</span> <span class="nf">_shape_as_list</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The shape of the tensor as a list.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape_tuple</span><span class="p">())</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">ndim</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the number of Tensor dimensions.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span>

  <span class="k">def</span> <span class="nf">_cpu_nograd</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A copy of this Tensor with contents backed by host memory.</span>

<span class="sd">    The copy cannot be differentiated through.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A CPU-memory backed Tensor object with the same contents as this Tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_copy_nograd</span><span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">context</span><span class="p">(),</span> <span class="s2">&quot;CPU:0&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">cpu</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A copy of this Tensor with contents backed by host memory.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_copy</span><span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">context</span><span class="p">(),</span> <span class="s2">&quot;CPU:0&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">gpu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gpu_index</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A copy of this Tensor with contents backed by memory on the GPU.</span>

<span class="sd">    Arguments:</span>
<span class="sd">      gpu_index: Identifies which GPU to place the contents on the returned</span>
<span class="sd">        Tensor in.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A GPU-memory backed Tensor object initialized with the same contents</span>
<span class="sd">      as this Tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_copy</span><span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">context</span><span class="p">(),</span> <span class="s2">&quot;GPU:&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">gpu_index</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">__bool__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape_tuple</span><span class="p">()</span> <span class="o">!=</span> <span class="p">():</span>  <span class="c1"># pylint: disable=g-explicit-bool-comparison</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;Non-scalar tensor </span><span class="si">%s</span><span class="s2"> cannot be converted to boolean.&quot;</span> <span class="o">%</span> <span class="nb">repr</span><span class="p">(</span><span class="bp">self</span><span class="p">))</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;Non-boolean tensor </span><span class="si">%s</span><span class="s2"> cannot be converted to boolean.&quot;</span> <span class="o">%</span> <span class="nb">repr</span><span class="p">(</span><span class="bp">self</span><span class="p">))</span>
    <span class="k">return</span> <span class="nb">bool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>

  <span class="k">def</span> <span class="nf">__nonzero__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__bool__</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">set_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">is_compatible_with</span><span class="p">(</span><span class="n">shape</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;Tensor&#39;s shape </span><span class="si">%s</span><span class="s2"> is not compatible with supplied shape </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span>
          <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">shape</span><span class="p">))</span>

  <span class="c1"># Methods not supported / implemented for Eager Tensors.</span>
  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">op</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
        <span class="s2">&quot;Tensor.op is meaningless when eager execution is enabled.&quot;</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">graph</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
        <span class="s2">&quot;Tensor.graph is meaningless when eager execution is enabled.&quot;</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
        <span class="s2">&quot;Tensor.name is meaningless when eager execution is enabled.&quot;</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">value_index</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
        <span class="s2">&quot;Tensor.value_index is meaningless when eager execution is enabled.&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">consumers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
        <span class="s2">&quot;Tensor.consumers is meaningless when eager execution is enabled.&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_add_consumer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">consumer</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
        <span class="s2">&quot;_add_consumer not supported when eager execution is enabled.&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_as_node_def_input</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
        <span class="s2">&quot;_as_node_def_input not supported when eager execution is enabled.&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_as_tf_output</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
        <span class="s2">&quot;_as_tf_output not supported when eager execution is enabled.&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">eval</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">session</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
        <span class="s2">&quot;eval is not supported when eager execution is enabled, &quot;</span>
        <span class="s2">&quot;is .numpy() what you&#39;re looking for?&quot;</span>
    <span class="p">)</span>


<span class="c1"># This call creates an EagerTensor class, as a subclass of _EagerTensorBase, and</span>
<span class="c1"># registers it with the current module.</span>
<span class="n">EagerTensor</span> <span class="o">=</span> <span class="n">c_api</span><span class="o">.</span><span class="n">TFE_Py_InitEagerTensor</span><span class="p">(</span><span class="n">_EagerTensorBase</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_TensorTensorConversionFunction</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">as_ref</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="n">_</span> <span class="o">=</span> <span class="n">name</span><span class="p">,</span> <span class="n">as_ref</span>
  <span class="k">if</span> <span class="n">dtype</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_compatible_with</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">dtype</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
        <span class="s2">&quot;Tensor conversion requested dtype </span><span class="si">%s</span><span class="s2"> for Tensor with dtype </span><span class="si">%s</span><span class="s2">: </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span>
        <span class="p">(</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">t</span><span class="p">)))</span>
  <span class="k">return</span> <span class="n">t</span>


<span class="n">_tensor_conversion_func_registry</span> <span class="o">=</span> <span class="p">{</span>
    <span class="mi">0</span><span class="p">:</span> <span class="p">[(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">_TensorTensorConversionFunction</span><span class="p">)]</span>
<span class="p">}</span>
<span class="n">_tensor_conversion_func_cache</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">_tensor_conversion_func_lock</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Lock</span><span class="p">()</span>
<span class="n">register_dense_tensor_like_type</span><span class="p">(</span><span class="n">Tensor</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;convert_to_tensor&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">convert_to_tensor</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">preferred_dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Converts the given `value` to a `Tensor`.</span>

<span class="sd">  This function converts Python objects of various types to `Tensor`</span>
<span class="sd">  objects. It accepts `Tensor` objects, numpy arrays, Python lists,</span>
<span class="sd">  and Python scalars. For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  import numpy as np</span>

<span class="sd">  def my_func(arg):</span>
<span class="sd">    arg = tf.convert_to_tensor(arg, dtype=tf.float32)</span>
<span class="sd">    return tf.matmul(arg, arg) + arg</span>

<span class="sd">  # The following calls are equivalent.</span>
<span class="sd">  value_1 = my_func(tf.constant([[1.0, 2.0], [3.0, 4.0]]))</span>
<span class="sd">  value_2 = my_func([[1.0, 2.0], [3.0, 4.0]])</span>
<span class="sd">  value_3 = my_func(np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.float32))</span>
<span class="sd">  ```</span>

<span class="sd">  This function can be useful when composing a new operation in Python</span>
<span class="sd">  (such as `my_func` in the example above). All standard Python op</span>
<span class="sd">  constructors apply this function to each of their Tensor-valued</span>
<span class="sd">  inputs, which allows those ops to accept numpy arrays, Python lists,</span>
<span class="sd">  and scalars in addition to `Tensor` objects.</span>

<span class="sd">  Note: This function diverges from default Numpy behavior for `float` and</span>
<span class="sd">    `string` types when `None` is present in a Python list or scalar. Rather</span>
<span class="sd">    than silently converting `None` values, an error will be thrown.</span>

<span class="sd">  Args:</span>
<span class="sd">    value: An object whose type has a registered `Tensor` conversion function.</span>
<span class="sd">    dtype: Optional element type for the returned tensor. If missing, the</span>
<span class="sd">      type is inferred from the type of `value`.</span>
<span class="sd">    name: Optional name to use if a new `Tensor` is created.</span>
<span class="sd">    preferred_dtype: Optional element type for the returned tensor,</span>
<span class="sd">      used when dtype is None. In some cases, a caller may not have a</span>
<span class="sd">      dtype in mind when converting to a tensor, so preferred_dtype</span>
<span class="sd">      can be used as a soft preference.  If the conversion to</span>
<span class="sd">      `preferred_dtype` is not possible, this argument has no effect.</span>

<span class="sd">  Returns:</span>
<span class="sd">    An `Output` based on `value`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If no conversion function is registered for `value`.</span>
<span class="sd">    RuntimeError: If a registered conversion function returns an invalid value.</span>

<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">internal_convert_to_tensor</span><span class="p">(</span>
      <span class="n">value</span><span class="o">=</span><span class="n">value</span><span class="p">,</span>
      <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
      <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
      <span class="n">preferred_dtype</span><span class="o">=</span><span class="n">preferred_dtype</span><span class="p">,</span>
      <span class="n">as_ref</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_error_prefix</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
  <span class="k">return</span> <span class="s2">&quot;&quot;</span> <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">: &quot;</span> <span class="o">%</span> <span class="n">name</span>


<span class="k">def</span> <span class="nf">internal_convert_to_tensor</span><span class="p">(</span><span class="n">value</span><span class="p">,</span>
                               <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                               <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                               <span class="n">as_ref</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                               <span class="n">preferred_dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                               <span class="n">ctx</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Converts the given `value` to an `Tensor`.</span>

<span class="sd">  This function converts Python objects of various types to `Tensor`</span>
<span class="sd">  objects. It accepts `Tensor` objects, numpy arrays, Python lists,</span>
<span class="sd">  and Python scalars. For example:</span>

<span class="sd">  This function can be useful when composing a new operation in Python</span>
<span class="sd">  All standard Python op constructors apply this function to each of their</span>
<span class="sd">  Tensor-valued inputs, which allows those ops to accept numpy arrays, Python</span>
<span class="sd">  lists, and scalars in addition to `Tensor` objects.</span>

<span class="sd">  Args:</span>
<span class="sd">    value: An object whose type has a registered `Tensor` conversion function.</span>
<span class="sd">    dtype: Optional element type for the returned tensor. If missing, the</span>
<span class="sd">      type is inferred from the type of `value`.</span>
<span class="sd">    name: Optional name to use if a new `Tensor` is created.</span>
<span class="sd">    as_ref: True if we want the mutable view of Variables, if applicable.</span>
<span class="sd">    preferred_dtype: Optional element type for the returned tensor,</span>
<span class="sd">      used when dtype is None. In some cases, a caller may not have a</span>
<span class="sd">      dtype in mind when converting to a tensor, so preferred_dtype</span>
<span class="sd">      can be used as a soft preference.  If the conversion to</span>
<span class="sd">      `preferred_dtype` is not possible, this argument has no effect.</span>
<span class="sd">    ctx: Optional: The value of context.context().</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` based on `value`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If no conversion function is registered for `value`.</span>
<span class="sd">    RuntimeError: If a registered conversion function returns an invalid value.</span>

<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">ctx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span> <span class="n">ctx</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">EagerTensor</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
      <span class="c1"># Fast path for EagerTensors that don&#39;t need any conversion.</span>
      <span class="c1"># Note that we don&#39;t check that value&#39;s dtype matches the dtype</span>
      <span class="c1"># argument.  We expect that the C runtime will do that checking</span>
      <span class="c1"># when we execute the kernel.</span>
      <span class="k">return</span> <span class="n">value</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">graph</span> <span class="o">=</span> <span class="n">get_default_graph</span><span class="p">()</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">graph</span><span class="o">.</span><span class="n">building_function</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Attempting to capture an EagerTensor without &quot;</span>
                           <span class="s2">&quot;building a function.&quot;</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">graph</span><span class="o">.</span><span class="n">capture</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
  <span class="n">unwrapped_type</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
  <span class="n">conversion_func_list</span> <span class="o">=</span> <span class="n">_tensor_conversion_func_cache</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">unwrapped_type</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">conversion_func_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">_tensor_conversion_func_lock</span><span class="p">:</span>
      <span class="n">conversion_func_list</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">funcs_at_priority</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span>
          <span class="n">_tensor_conversion_func_registry</span><span class="o">.</span><span class="n">items</span><span class="p">()):</span>
        <span class="k">for</span> <span class="n">base_type</span><span class="p">,</span> <span class="n">conversion_func</span> <span class="ow">in</span> <span class="n">funcs_at_priority</span><span class="p">:</span>
          <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">base_type</span><span class="p">):</span>
            <span class="n">conversion_func_list</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">base_type</span><span class="p">,</span> <span class="n">conversion_func</span><span class="p">))</span>
      <span class="n">_tensor_conversion_func_cache</span><span class="p">[</span><span class="n">unwrapped_type</span><span class="p">]</span> <span class="o">=</span> <span class="n">conversion_func_list</span>

  <span class="k">for</span> <span class="n">base_type</span><span class="p">,</span> <span class="n">conversion_func</span> <span class="ow">in</span> <span class="n">conversion_func_list</span><span class="p">:</span>
    <span class="c1"># If dtype is None but preferred_dtype is not None, we try to</span>
    <span class="c1"># cast to preferred_dtype first.</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">preferred_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">conversion_func</span><span class="p">(</span>
            <span class="n">value</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">preferred_dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">as_ref</span><span class="o">=</span><span class="n">as_ref</span><span class="p">)</span>
      <span class="k">except</span> <span class="p">(</span><span class="ne">TypeError</span><span class="p">,</span> <span class="ne">ValueError</span><span class="p">,</span> <span class="n">errors</span><span class="o">.</span><span class="n">UnimplementedError</span><span class="p">,</span>
              <span class="n">errors</span><span class="o">.</span><span class="n">InvalidArgumentError</span><span class="p">):</span>
        <span class="c1"># Could not coerce the conversion to use the preferred dtype.</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="kc">None</span>

      <span class="k">if</span> <span class="n">ret</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">ret</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">NotImplemented</span><span class="p">:</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">ret</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span> <span class="o">!=</span>
            <span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">preferred_dtype</span><span class="p">)</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">):</span>
          <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;convert_to_tensor did not convert to &quot;</span>
                          <span class="s2">&quot;the preferred dtype: </span><span class="si">%s</span><span class="s2"> vs </span><span class="si">%s</span><span class="s2"> &quot;</span> <span class="o">%</span>
                          <span class="p">(</span><span class="n">ret</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">,</span>
                           <span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">preferred_dtype</span><span class="p">)</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">ret</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">ret</span> <span class="o">=</span> <span class="n">conversion_func</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">as_ref</span><span class="o">=</span><span class="n">as_ref</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">ret</span> <span class="ow">is</span> <span class="bp">NotImplemented</span><span class="p">:</span>
      <span class="k">continue</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
          <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">Conversion function </span><span class="si">%r</span><span class="s2"> for type </span><span class="si">%s</span><span class="s2"> returned non-Tensor: </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span>
          <span class="p">(</span><span class="n">_error_prefix</span><span class="p">(</span><span class="n">name</span><span class="p">),</span> <span class="n">conversion_func</span><span class="p">,</span> <span class="n">base_type</span><span class="p">,</span> <span class="n">ret</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_compatible_with</span><span class="p">(</span><span class="n">ret</span><span class="o">.</span><span class="n">dtype</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
          <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">Conversion function </span><span class="si">%r</span><span class="s2"> for type </span><span class="si">%s</span><span class="s2"> returned incompatible &quot;</span>
          <span class="s2">&quot;dtype: requested = </span><span class="si">%s</span><span class="s2">, actual = </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span>
          <span class="p">(</span><span class="n">_error_prefix</span><span class="p">(</span><span class="n">name</span><span class="p">),</span> <span class="n">conversion_func</span><span class="p">,</span> <span class="n">base_type</span><span class="p">,</span> <span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
           <span class="n">ret</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">ret</span>
  <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2">Cannot convert </span><span class="si">%r</span><span class="s2"> with type </span><span class="si">%s</span><span class="s2"> to Tensor: &quot;</span>
                  <span class="s2">&quot;no conversion function registered.&quot;</span> <span class="o">%</span>
                  <span class="p">(</span><span class="n">_error_prefix</span><span class="p">(</span><span class="n">name</span><span class="p">),</span> <span class="n">value</span><span class="p">,</span> <span class="n">unwrapped_type</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">internal_convert_n_to_tensor</span><span class="p">(</span><span class="n">values</span><span class="p">,</span>
                                 <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                 <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                 <span class="n">as_ref</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                 <span class="n">preferred_dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                 <span class="n">ctx</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Converts `values` to a list of `Tensor` objects.</span>

<span class="sd">  Args:</span>
<span class="sd">    values: A list of objects that can be consumed by `tf.convert_to_tensor()`.</span>
<span class="sd">    dtype: (Optional.) The required `DType` of the returned `Tensor` objects.</span>
<span class="sd">    name: (Optional.) A name prefix to used when a new `Tensor` is</span>
<span class="sd">      created, in which case element `i` will be given the name `name</span>
<span class="sd">      + &#39;_&#39; + i`.</span>
<span class="sd">    as_ref: True if the caller wants the results as ref tensors.</span>
<span class="sd">    preferred_dtype: Optional element type for the returned tensors,</span>
<span class="sd">      used when dtype is None. In some cases, a caller may not have a</span>
<span class="sd">      dtype in mind when converting to a tensor, so preferred_dtype</span>
<span class="sd">      can be used as a soft preference.  If the conversion to</span>
<span class="sd">      `preferred_dtype` is not possible, this argument has no effect.</span>
<span class="sd">    ctx: The value of context.context().</span>

<span class="sd">  Returns:</span>
<span class="sd">    A list of `Tensor` and/or `IndexedSlices` objects.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If no conversion function is registered for an element in</span>
<span class="sd">      `values`.</span>
<span class="sd">    RuntimeError: If a registered conversion function returns an invalid</span>
<span class="sd">      value.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">Sequence</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;values must be a list.&quot;</span><span class="p">)</span>
  <span class="n">ret</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">if</span> <span class="n">ctx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span> <span class="n">ctx</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">values</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="kc">None</span> <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">_</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
    <span class="n">ret</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="n">internal_convert_to_tensor</span><span class="p">(</span>
            <span class="n">value</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="n">n</span><span class="p">,</span>
            <span class="n">as_ref</span><span class="o">=</span><span class="n">as_ref</span><span class="p">,</span>
            <span class="n">preferred_dtype</span><span class="o">=</span><span class="n">preferred_dtype</span><span class="p">,</span>
            <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">ret</span>


<span class="k">def</span> <span class="nf">convert_n_to_tensor</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">preferred_dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Converts `values` to a list of `Tensor` objects.</span>

<span class="sd">  Args:</span>
<span class="sd">    values: A list of objects that can be consumed by `tf.convert_to_tensor()`.</span>
<span class="sd">    dtype: (Optional.) The required `DType` of the returned `Tensor` objects.</span>
<span class="sd">    name: (Optional.) A name prefix to used when a new `Tensor` is</span>
<span class="sd">      created, in which case element `i` will be given the name `name</span>
<span class="sd">      + &#39;_&#39; + i`.</span>
<span class="sd">    preferred_dtype: Optional element type for the returned tensors,</span>
<span class="sd">      used when dtype is None. In some cases, a caller may not have a</span>
<span class="sd">      dtype in mind when converting to a tensor, so preferred_dtype</span>
<span class="sd">      can be used as a soft preference.  If the conversion to</span>
<span class="sd">      `preferred_dtype` is not possible, this argument has no effect.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A list of `Tensor` and/or `IndexedSlices` objects.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If no conversion function is registered for an element in</span>
<span class="sd">      `values`.</span>
<span class="sd">    RuntimeError: If a registered conversion function returns an invalid</span>
<span class="sd">      value.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">internal_convert_n_to_tensor</span><span class="p">(</span>
      <span class="n">values</span><span class="o">=</span><span class="n">values</span><span class="p">,</span>
      <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
      <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
      <span class="n">preferred_dtype</span><span class="o">=</span><span class="n">preferred_dtype</span><span class="p">,</span>
      <span class="n">as_ref</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;convert_to_tensor_or_indexed_slices&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">convert_to_tensor_or_indexed_slices</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Converts the given object to a `Tensor` or an `IndexedSlices`.</span>

<span class="sd">  If `value` is an `IndexedSlices` or `SparseTensor` it is returned</span>
<span class="sd">  unmodified. Otherwise, it is converted to a `Tensor` using</span>
<span class="sd">  `convert_to_tensor()`.</span>

<span class="sd">  Args:</span>
<span class="sd">    value: An `IndexedSlices`, `SparseTensor`, or an object that can be consumed</span>
<span class="sd">      by `convert_to_tensor()`.</span>
<span class="sd">    dtype: (Optional.) The required `DType` of the returned `Tensor` or</span>
<span class="sd">      `IndexedSlices`.</span>
<span class="sd">    name: (Optional.) A name to use if a new `Tensor` is created.</span>

<span class="sd">  Returns:</span>
<span class="sd">    An `Tensor`, `IndexedSlices`, or `SparseTensor` based on `value`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If `dtype` does not match the element type of `value`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">internal_convert_to_tensor_or_indexed_slices</span><span class="p">(</span>
      <span class="n">value</span><span class="o">=</span><span class="n">value</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">as_ref</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">internal_convert_to_tensor_or_indexed_slices</span><span class="p">(</span><span class="n">value</span><span class="p">,</span>
                                                 <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                                 <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                                 <span class="n">as_ref</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Converts the given object to an `Tensor` or an `IndexedSlices`.</span>

<span class="sd">  If `value` is an `IndexedSlices` or `SparseTensor` it is returned</span>
<span class="sd">  unmodified. Otherwise, it is converted to a `Tensor` using</span>
<span class="sd">  `convert_to_tensor()`.</span>

<span class="sd">  Args:</span>
<span class="sd">    value: An `IndexedSlices`, `SparseTensor`, or an object that can be consumed</span>
<span class="sd">      by `convert_to_tensor()`.</span>
<span class="sd">    dtype: (Optional.) The required `DType` of the returned `Tensor` or</span>
<span class="sd">      `IndexedSlices`.</span>
<span class="sd">    name: (Optional.) A name to use if a new `Tensor` is created.</span>
<span class="sd">    as_ref: True if the caller wants the results as ref tensors.</span>

<span class="sd">  Returns:</span>
<span class="sd">    An `Tensor`, `IndexedSlices`, or `SparseTensor` based on `value`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If `dtype` does not match the element type of `value`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">EagerTensor</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">internal_convert_to_tensor</span><span class="p">(</span>
        <span class="n">value</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">as_ref</span><span class="o">=</span><span class="n">as_ref</span><span class="p">)</span>
  <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">_TensorLike</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">is_compatible_with</span><span class="p">(</span><span class="n">value</span><span class="o">.</span><span class="n">dtype</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;Tensor conversion requested dtype </span><span class="si">%s</span><span class="s2"> for Tensor with dtype </span><span class="si">%s</span><span class="s2">: </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span>
          <span class="p">(</span><span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">value</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">value</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">internal_convert_to_tensor</span><span class="p">(</span>
        <span class="n">value</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">as_ref</span><span class="o">=</span><span class="n">as_ref</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">internal_convert_n_to_tensor_or_indexed_slices</span><span class="p">(</span><span class="n">values</span><span class="p">,</span>
                                                   <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                                   <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                                   <span class="n">as_ref</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Converts `values` to a list of `Tensor` or `IndexedSlices` objects.</span>

<span class="sd">  Any `IndexedSlices` or `SparseTensor` objects in `values` are returned</span>
<span class="sd">  unmodified.</span>

<span class="sd">  Args:</span>
<span class="sd">    values: A list of `None`, `IndexedSlices`, `SparseTensor`, or objects that</span>
<span class="sd">      can be consumed by `convert_to_tensor()`.</span>
<span class="sd">    dtype: (Optional.) The required `DType` of the returned `Tensor`</span>
<span class="sd">      `IndexedSlices`.</span>
<span class="sd">    name: (Optional.) A name prefix to used when a new `Tensor` is</span>
<span class="sd">      created, in which case element `i` will be given the name `name</span>
<span class="sd">      + &#39;_&#39; + i`.</span>
<span class="sd">    as_ref: True if the caller wants the results as ref tensors.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A list of `Tensor`, `IndexedSlices`, and/or `SparseTensor` objects.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If no conversion function is registered for an element in</span>
<span class="sd">      `values`.</span>
<span class="sd">    RuntimeError: If a registered conversion function returns an invalid</span>
<span class="sd">      value.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">Sequence</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;values must be a list.&quot;</span><span class="p">)</span>
  <span class="n">ret</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">values</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">ret</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">n</span> <span class="o">=</span> <span class="kc">None</span> <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">_</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
      <span class="n">ret</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
          <span class="n">internal_convert_to_tensor_or_indexed_slices</span><span class="p">(</span>
              <span class="n">value</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">as_ref</span><span class="o">=</span><span class="n">as_ref</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">ret</span>


<span class="k">def</span> <span class="nf">convert_n_to_tensor_or_indexed_slices</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Converts `values` to a list of `Output` or `IndexedSlices` objects.</span>

<span class="sd">  Any `IndexedSlices` or `SparseTensor` objects in `values` are returned</span>
<span class="sd">  unmodified.</span>

<span class="sd">  Args:</span>
<span class="sd">    values: A list of `None`, `IndexedSlices`, `SparseTensor`, or objects that</span>
<span class="sd">      can be consumed by `convert_to_tensor()`.</span>
<span class="sd">    dtype: (Optional.) The required `DType` of the returned `Tensor`</span>
<span class="sd">      `IndexedSlices`.</span>
<span class="sd">    name: (Optional.) A name prefix to used when a new `Tensor` is</span>
<span class="sd">      created, in which case element `i` will be given the name `name</span>
<span class="sd">      + &#39;_&#39; + i`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A list of `Tensor`, `IndexedSlices`, and/or `SparseTensor` objects.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If no conversion function is registered for an element in</span>
<span class="sd">      `values`.</span>
<span class="sd">    RuntimeError: If a registered conversion function returns an invalid</span>
<span class="sd">      value.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">internal_convert_n_to_tensor_or_indexed_slices</span><span class="p">(</span>
      <span class="n">values</span><span class="o">=</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">as_ref</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>


<span class="c1"># TODO(josh11b): Add ctx argument to conversion_func() signature.</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;register_tensor_conversion_function&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">register_tensor_conversion_function</span><span class="p">(</span><span class="n">base_type</span><span class="p">,</span>
                                        <span class="n">conversion_func</span><span class="p">,</span>
                                        <span class="n">priority</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Registers a function for converting objects of `base_type` to `Tensor`.</span>

<span class="sd">  The conversion function must have the following signature:</span>

<span class="sd">  ```python</span>
<span class="sd">      def conversion_func(value, dtype=None, name=None, as_ref=False):</span>
<span class="sd">        # ...</span>
<span class="sd">  ```</span>

<span class="sd">  It must return a `Tensor` with the given `dtype` if specified. If the</span>
<span class="sd">  conversion function creates a new `Tensor`, it should use the given</span>
<span class="sd">  `name` if specified. All exceptions will be propagated to the caller.</span>

<span class="sd">  The conversion function may return `NotImplemented` for some</span>
<span class="sd">  inputs. In this case, the conversion process will continue to try</span>
<span class="sd">  subsequent conversion functions.</span>

<span class="sd">  If `as_ref` is true, the function must return a `Tensor` reference,</span>
<span class="sd">  such as a `Variable`.</span>

<span class="sd">  NOTE: The conversion functions will execute in order of priority,</span>
<span class="sd">  followed by order of registration. To ensure that a conversion function</span>
<span class="sd">  `F` runs before another conversion function `G`, ensure that `F` is</span>
<span class="sd">  registered with a smaller priority than `G`.</span>

<span class="sd">  Args:</span>
<span class="sd">    base_type: The base type or tuple of base types for all objects that</span>
<span class="sd">      `conversion_func` accepts.</span>
<span class="sd">    conversion_func: A function that converts instances of `base_type` to</span>
<span class="sd">      `Tensor`.</span>
<span class="sd">    priority: Optional integer that indicates the priority for applying this</span>
<span class="sd">      conversion function. Conversion functions with smaller priority values</span>
<span class="sd">      run earlier than conversion functions with larger priority values.</span>
<span class="sd">      Defaults to 100.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If the arguments do not have the appropriate type.</span>

<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">global</span> <span class="n">_tensor_conversion_func_cache</span>
  <span class="k">with</span> <span class="n">_tensor_conversion_func_lock</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">base_type</span><span class="p">,</span> <span class="nb">type</span><span class="p">)</span> <span class="ow">or</span>
            <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">base_type</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="ow">and</span>
             <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">type</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">base_type</span><span class="p">))):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;base_type must be a type or a tuple of types.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="n">conversion_func</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;conversion_func must be callable.&quot;</span><span class="p">)</span>

    <span class="c1"># context._context is checked so that we don&#39;t inadvertently create it.</span>
    <span class="c1"># This is because enable_eager_execution will fail when called from the main</span>
    <span class="c1"># function if the context._context is already created, and the</span>
    <span class="c1"># register_tensor_conversion_function calls happen when the module is</span>
    <span class="c1"># imported.</span>
    <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">_context</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">(</span>
    <span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">base_type</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">integer_types</span> <span class="o">+</span> <span class="p">(</span>
        <span class="nb">float</span><span class="p">,</span>
        <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="p">)):</span>
      <span class="c1"># TODO(nareshmodi): consider setting a context variable which disables the</span>
      <span class="c1"># fastpath instead.</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
          <span class="s2">&quot;Cannot register conversions for numpy arrays, python number types &quot;</span>
          <span class="s2">&quot;when executing eagerly.&quot;</span><span class="p">)</span>

    <span class="k">try</span><span class="p">:</span>
      <span class="n">funcs_at_priority</span> <span class="o">=</span> <span class="n">_tensor_conversion_func_registry</span><span class="p">[</span><span class="n">priority</span><span class="p">]</span>
    <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
      <span class="n">funcs_at_priority</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="n">_tensor_conversion_func_registry</span><span class="p">[</span><span class="n">priority</span><span class="p">]</span> <span class="o">=</span> <span class="n">funcs_at_priority</span>
    <span class="n">funcs_at_priority</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">base_type</span><span class="p">,</span> <span class="n">conversion_func</span><span class="p">))</span>
    <span class="n">_tensor_conversion_func_cache</span> <span class="o">=</span> <span class="p">{}</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;IndexedSlices&quot;</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">IndexedSlices</span><span class="p">(</span><span class="n">_TensorLike</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A sparse representation of a set of tensor slices at given indices.</span>

<span class="sd">  This class is a simple wrapper for a pair of `Tensor` objects:</span>

<span class="sd">  * `values`: A `Tensor` of any dtype with shape `[D0, D1, ..., Dn]`.</span>
<span class="sd">  * `indices`: A 1-D integer `Tensor` with shape `[D0]`.</span>

<span class="sd">  An `IndexedSlices` is typically used to represent a subset of a larger</span>
<span class="sd">  tensor `dense` of shape `[LARGE0, D1, .. , DN]` where `LARGE0 &gt;&gt; D0`.</span>
<span class="sd">  The values in `indices` are the indices in the first dimension of</span>
<span class="sd">  the slices that have been extracted from the larger tensor.</span>

<span class="sd">  The dense tensor `dense` represented by an `IndexedSlices` `slices` has</span>

<span class="sd">  ```python</span>
<span class="sd">  dense[slices.indices[i], :, :, :, ...] = slices.values[i, :, :, :, ...]</span>
<span class="sd">  ```</span>

<span class="sd">  The `IndexedSlices` class is used principally in the definition of</span>
<span class="sd">  gradients for operations that have sparse gradients</span>
<span class="sd">  (e.g. @{tf.gather}).</span>

<span class="sd">  Contrast this representation with</span>
<span class="sd">  @{tf.SparseTensor},</span>
<span class="sd">  which uses multi-dimensional indices and scalar values.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">dense_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates an `IndexedSlices`.&quot;&quot;&quot;</span>
    <span class="n">_get_graph_from_inputs</span><span class="p">([</span><span class="n">values</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">dense_shape</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_values</span> <span class="o">=</span> <span class="n">values</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_indices</span> <span class="o">=</span> <span class="n">indices</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_dense_shape</span> <span class="o">=</span> <span class="n">dense_shape</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">values</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A `Tensor` containing the values of the slices.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_values</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">indices</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A 1-D `Tensor` containing the indices of the slices.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_indices</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">dense_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A 1-D `Tensor` containing the shape of the corresponding dense tensor.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dense_shape</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The name of this `IndexedSlices`.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">name</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">device</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The name of the device on which `values` will be produced, or `None`.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">device</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">op</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The `Operation` that produces `values` as an output.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">op</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The `DType` of elements in this tensor.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">dtype</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">graph</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The `Graph` that contains the values, indices, and shape tensors.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_values</span><span class="o">.</span><span class="n">graph</span>

  <span class="k">def</span> <span class="nf">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="s2">&quot;IndexedSlices(indices=</span><span class="si">%s</span><span class="s2">, values=</span><span class="si">%s%s</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_values</span><span class="p">,</span> <span class="p">(</span><span class="s2">&quot;, dense_shape=</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dense_shape</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dense_shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">__neg__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">IndexedSlices</span><span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense_shape</span><span class="p">)</span>


<span class="n">IndexedSlicesValue</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">namedtuple</span><span class="p">(</span>
    <span class="s2">&quot;IndexedSlicesValue&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;values&quot;</span><span class="p">,</span> <span class="s2">&quot;indices&quot;</span><span class="p">,</span> <span class="s2">&quot;dense_shape&quot;</span><span class="p">])</span>


<span class="k">def</span> <span class="nf">_device_string</span><span class="p">(</span><span class="n">dev_spec</span><span class="p">):</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dev_spec</span><span class="p">,</span> <span class="n">pydev</span><span class="o">.</span><span class="n">DeviceSpec</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">dev_spec</span><span class="o">.</span><span class="n">to_string</span><span class="p">()</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">dev_spec</span>


<span class="k">def</span> <span class="nf">_NodeDef</span><span class="p">(</span><span class="n">op_type</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attrs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># pylint: disable=redefined-outer-name</span>
  <span class="sd">&quot;&quot;&quot;Create a NodeDef proto.</span>

<span class="sd">  Args:</span>
<span class="sd">    op_type: Value for the &quot;op&quot; attribute of the NodeDef proto.</span>
<span class="sd">    name: Value for the &quot;name&quot; attribute of the NodeDef proto.</span>
<span class="sd">    device: string, device, or function from NodeDef to string.</span>
<span class="sd">      Value for the &quot;device&quot; attribute of the NodeDef proto.</span>
<span class="sd">    attrs: Optional dictionary where the key is the attribute name (a string)</span>
<span class="sd">      and the value is the respective &quot;attr&quot; attribute of the NodeDef proto (an</span>
<span class="sd">      AttrValue).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A node_def_pb2.NodeDef protocol buffer.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">node_def</span> <span class="o">=</span> <span class="n">node_def_pb2</span><span class="o">.</span><span class="n">NodeDef</span><span class="p">()</span>
  <span class="n">node_def</span><span class="o">.</span><span class="n">op</span> <span class="o">=</span> <span class="n">compat</span><span class="o">.</span><span class="n">as_bytes</span><span class="p">(</span><span class="n">op_type</span><span class="p">)</span>
  <span class="n">node_def</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">compat</span><span class="o">.</span><span class="n">as_bytes</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">attrs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">iteritems</span><span class="p">(</span><span class="n">attrs</span><span class="p">):</span>
      <span class="n">node_def</span><span class="o">.</span><span class="n">attr</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">CopyFrom</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="n">device</span><span class="p">):</span>
      <span class="n">node_def</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span><span class="p">(</span><span class="n">node_def</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">node_def</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">_device_string</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">node_def</span>


<span class="c1"># Copied from core/framework/node_def_util.cc</span>
<span class="c1"># TODO(mrry,josh11b): Consolidate this validation in C++ code.</span>
<span class="n">_VALID_OP_NAME_REGEX</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s2">&quot;^[A-Za-z0-9.][A-Za-z0-9_.</span><span class="se">\\</span><span class="s2">-/]*$&quot;</span><span class="p">)</span>
<span class="n">_VALID_SCOPE_NAME_REGEX</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s2">&quot;^[A-Za-z0-9_.</span><span class="se">\\</span><span class="s2">-/]*$&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_create_c_op</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">node_def</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">control_inputs</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Creates a TF_Operation.</span>

<span class="sd">  Args:</span>
<span class="sd">    graph: a `Graph`.</span>
<span class="sd">    node_def: `node_def_pb2.NodeDef` for the operation to create.</span>
<span class="sd">    inputs: A list of `Tensor`s (corresponding to scalar inputs) and lists of</span>
<span class="sd">      `Tensor`s (corresponding to sequence inputs, e.g. &quot;int64 * N&quot;,</span>
<span class="sd">      &quot;list(int64)&quot;). The length of the list should be equal to the number of</span>
<span class="sd">      inputs specified by this operation&#39;s op def.</span>
<span class="sd">    control_inputs: A list of `Operation`s to set as control dependencies.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A wrapped TF_Operation*.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># pylint: disable=protected-access</span>
  <span class="n">op_desc</span> <span class="o">=</span> <span class="n">c_api</span><span class="o">.</span><span class="n">TF_NewOperation</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">_c_graph</span><span class="p">,</span>
                                  <span class="n">compat</span><span class="o">.</span><span class="n">as_str</span><span class="p">(</span><span class="n">node_def</span><span class="o">.</span><span class="n">op</span><span class="p">),</span>
                                  <span class="n">compat</span><span class="o">.</span><span class="n">as_str</span><span class="p">(</span><span class="n">node_def</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>
  <span class="c1"># Add inputs</span>
  <span class="k">for</span> <span class="n">op_input</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op_input</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
      <span class="n">c_api</span><span class="o">.</span><span class="n">TF_AddInputList</span><span class="p">(</span><span class="n">op_desc</span><span class="p">,</span> <span class="p">[</span><span class="n">t</span><span class="o">.</span><span class="n">_as_tf_output</span><span class="p">()</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">op_input</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">c_api</span><span class="o">.</span><span class="n">TF_AddInput</span><span class="p">(</span><span class="n">op_desc</span><span class="p">,</span> <span class="n">op_input</span><span class="o">.</span><span class="n">_as_tf_output</span><span class="p">())</span>

  <span class="c1"># Add control inputs</span>
  <span class="k">for</span> <span class="n">control_input</span> <span class="ow">in</span> <span class="n">control_inputs</span><span class="p">:</span>
    <span class="n">c_api</span><span class="o">.</span><span class="n">TF_AddControlInput</span><span class="p">(</span><span class="n">op_desc</span><span class="p">,</span> <span class="n">control_input</span><span class="o">.</span><span class="n">_c_op</span><span class="p">)</span>
  <span class="c1"># pylint: enable=protected-access</span>

  <span class="c1"># Add attrs</span>
  <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">attr_value</span> <span class="ow">in</span> <span class="n">node_def</span><span class="o">.</span><span class="n">attr</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">serialized</span> <span class="o">=</span> <span class="n">attr_value</span><span class="o">.</span><span class="n">SerializeToString</span><span class="p">()</span>
    <span class="c1"># TODO(skyewm): this creates and deletes a new TF_Status for every attr.</span>
    <span class="c1"># It might be worth creating a convenient way to re-use the same status.</span>
    <span class="n">c_api</span><span class="o">.</span><span class="n">TF_SetAttrValueProto</span><span class="p">(</span><span class="n">op_desc</span><span class="p">,</span> <span class="n">compat</span><span class="o">.</span><span class="n">as_str</span><span class="p">(</span><span class="n">name</span><span class="p">),</span> <span class="n">serialized</span><span class="p">)</span>

  <span class="k">try</span><span class="p">:</span>
    <span class="n">c_op</span> <span class="o">=</span> <span class="n">c_api</span><span class="o">.</span><span class="n">TF_FinishOperation</span><span class="p">(</span><span class="n">op_desc</span><span class="p">)</span>
  <span class="k">except</span> <span class="n">errors</span><span class="o">.</span><span class="n">InvalidArgumentError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="c1"># Convert to ValueError for backwards compatibility.</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">))</span>

  <span class="k">return</span> <span class="n">c_op</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;Operation&quot;</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">Operation</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Represents a graph node that performs computation on tensors.</span>

<span class="sd">  An `Operation` is a node in a TensorFlow `Graph` that takes zero or</span>
<span class="sd">  more `Tensor` objects as input, and produces zero or more `Tensor`</span>
<span class="sd">  objects as output. Objects of type `Operation` are created by</span>
<span class="sd">  calling a Python op constructor (such as</span>
<span class="sd">  @{tf.matmul})</span>
<span class="sd">  or @{tf.Graph.create_op}.</span>

<span class="sd">  For example `c = tf.matmul(a, b)` creates an `Operation` of type</span>
<span class="sd">  &quot;MatMul&quot; that takes tensors `a` and `b` as input, and produces `c`</span>
<span class="sd">  as output.</span>

<span class="sd">  After the graph has been launched in a session, an `Operation` can</span>
<span class="sd">  be executed by passing it to</span>
<span class="sd">  @{tf.Session.run}.</span>
<span class="sd">  `op.run()` is a shortcut for calling `tf.get_default_session().run(op)`.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">node_def</span><span class="p">,</span>
               <span class="n">g</span><span class="p">,</span>
               <span class="n">inputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">output_types</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">control_inputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">input_types</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">original_op</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">op_def</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates an `Operation`.</span>

<span class="sd">    NOTE: This constructor validates the name of the `Operation` (passed</span>
<span class="sd">    as `node_def.name`). Valid `Operation` names match the following</span>
<span class="sd">    regular expression:</span>

<span class="sd">        [A-Za-z0-9.][A-Za-z0-9_.\\-/]*</span>

<span class="sd">    Args:</span>
<span class="sd">      node_def: `node_def_pb2.NodeDef`.  `NodeDef` for the `Operation`.</span>
<span class="sd">        Used for attributes of `node_def_pb2.NodeDef`, typically `name`,</span>
<span class="sd">        `op`, and `device`.  The `input` attribute is irrelevant here</span>
<span class="sd">        as it will be computed when generating the model.</span>
<span class="sd">      g: `Graph`. The parent graph.</span>
<span class="sd">      inputs: list of `Tensor` objects. The inputs to this `Operation`.</span>
<span class="sd">      output_types: list of `DType` objects.  List of the types of the</span>
<span class="sd">        `Tensors` computed by this operation.  The length of this list indicates</span>
<span class="sd">        the number of output endpoints of the `Operation`.</span>
<span class="sd">      control_inputs: list of operations or tensors from which to have a</span>
<span class="sd">        control dependency.</span>
<span class="sd">      input_types: List of `DType` objects representing the</span>
<span class="sd">        types of the tensors accepted by the `Operation`.  By default</span>
<span class="sd">        uses `[x.dtype.base_dtype for x in inputs]`.  Operations that expect</span>
<span class="sd">        reference-typed inputs must specify these explicitly.</span>
<span class="sd">      original_op: Optional. Used to associate the new `Operation` with an</span>
<span class="sd">        existing `Operation` (for example, a replica with the op that was</span>
<span class="sd">        replicated).</span>
<span class="sd">      op_def: Optional. The `op_def_pb2.OpDef` proto that describes the</span>
<span class="sd">        op type that this `Operation` represents.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if control inputs are not Operations or Tensors,</span>
<span class="sd">        or if `node_def` is not a `NodeDef`,</span>
<span class="sd">        or if `g` is not a `Graph`,</span>
<span class="sd">        or if `inputs` are not tensors,</span>
<span class="sd">        or if `inputs` and `input_types` are incompatible.</span>
<span class="sd">      ValueError: if the `node_def` name is not valid.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># For internal use only: `node_def` can be set to a TF_Operation to create</span>
    <span class="c1"># an Operation for that op. This is useful for creating Operations for ops</span>
    <span class="c1"># indirectly created by C API methods, e.g. the ops created by</span>
    <span class="c1"># TF_ImportGraphDef. When `node_def` is a TF_Operation, all optional fields</span>
    <span class="c1"># should be None.</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node_def</span><span class="p">,</span> <span class="n">node_def_pb2</span><span class="o">.</span><span class="n">NodeDef</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">node_def</span><span class="o">.</span><span class="n">ByteSize</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">31</span><span class="p">)</span> <span class="ow">or</span> <span class="n">node_def</span><span class="o">.</span><span class="n">ByteSize</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Cannot create a tensor proto whose content is larger than 2GB.&quot;</span><span class="p">)</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">_VALID_OP_NAME_REGEX</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">node_def</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;</span><span class="si">%s</span><span class="s2">&#39; is not a valid node name&quot;</span> <span class="o">%</span> <span class="n">node_def</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
      <span class="n">c_op</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">node_def</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;SwigPyObject&quot;</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">inputs</span> <span class="ow">is</span> <span class="kc">None</span>
      <span class="k">assert</span> <span class="n">output_types</span> <span class="ow">is</span> <span class="kc">None</span>
      <span class="k">assert</span> <span class="n">control_inputs</span> <span class="ow">is</span> <span class="kc">None</span>
      <span class="k">assert</span> <span class="n">input_types</span> <span class="ow">is</span> <span class="kc">None</span>
      <span class="k">assert</span> <span class="n">original_op</span> <span class="ow">is</span> <span class="kc">None</span>
      <span class="k">assert</span> <span class="n">op_def</span> <span class="ow">is</span> <span class="kc">None</span>
      <span class="n">c_op</span> <span class="o">=</span> <span class="n">node_def</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;node_def needs to be a NodeDef: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">node_def</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">Graph</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;g needs to be a Graph: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">g</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span> <span class="o">=</span> <span class="n">g</span>

    <span class="k">if</span> <span class="n">inputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;inputs needs to be a list of Tensors: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">inputs</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;input needs to be a Tensor: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">a</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">input_types</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">input_types</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span>
          <span class="n">x</span><span class="o">.</span><span class="n">is_compatible_with</span><span class="p">(</span><span class="n">i</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
          <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">input_types</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;In op &#39;</span><span class="si">%s</span><span class="s2">&#39;, input types (</span><span class="si">%s</span><span class="s2">) are not compatible &quot;</span>
                        <span class="s2">&quot;with expected types (</span><span class="si">%s</span><span class="s2">)&quot;</span> <span class="o">%</span>
                        <span class="p">(</span><span class="n">node_def</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="o">.</span><span class="n">dtype</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">],</span>
                         <span class="n">input_types</span><span class="p">))</span>

    <span class="c1"># Build the list of control inputs.</span>
    <span class="n">control_input_ops</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="n">control_inputs</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">control_inputs</span><span class="p">:</span>
        <span class="n">control_op</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">Operation</span><span class="p">):</span>
          <span class="n">control_op</span> <span class="o">=</span> <span class="n">c</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">IndexedSlices</span><span class="p">)):</span>
          <span class="n">control_op</span> <span class="o">=</span> <span class="n">c</span><span class="o">.</span><span class="n">op</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Control input must be an Operation, &quot;</span>
                          <span class="s2">&quot;a Tensor, or IndexedSlices: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">c</span><span class="p">)</span>
        <span class="n">control_input_ops</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">control_op</span><span class="p">)</span>

    <span class="c1"># This will be set by self.inputs.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_inputs_val</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># pylint: disable=protected-access</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_id_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_next_id</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_original_op</span> <span class="o">=</span> <span class="n">original_op</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_traceback</span> <span class="o">=</span> <span class="n">tf_stack</span><span class="o">.</span><span class="n">extract_stack</span><span class="p">()</span>

    <span class="c1"># List of _UserDevSpecs holding code location of device context manager</span>
    <span class="c1"># invocations and the users original argument to them.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_device_code_locations</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># Dict mapping op name to file and line information for op colocation</span>
    <span class="c1"># context managers.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_colocation_code_locations</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_control_flow_context</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">_get_control_flow_context</span><span class="p">()</span>
    <span class="c1"># pylint: enable=protected-access</span>

    <span class="c1"># Initialize self._c_op.</span>
    <span class="k">if</span> <span class="n">c_op</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span> <span class="o">=</span> <span class="n">c_op</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">op_def</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">op_def</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_get_op_def</span><span class="p">(</span><span class="n">node_def</span><span class="o">.</span><span class="n">op</span><span class="p">)</span>
      <span class="c1"># TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.</span>
      <span class="c1"># Refactor so we don&#39;t have to do this here.</span>
      <span class="n">grouped_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reconstruct_sequence_inputs</span><span class="p">(</span>
          <span class="n">op_def</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">node_def</span><span class="o">.</span><span class="n">attr</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span> <span class="o">=</span> <span class="n">_create_c_op</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="p">,</span> <span class="n">node_def</span><span class="p">,</span> <span class="n">grouped_inputs</span><span class="p">,</span>
                                <span class="n">control_input_ops</span><span class="p">)</span>

    <span class="c1"># Initialize self._outputs.</span>
    <span class="n">num_outputs</span> <span class="o">=</span> <span class="n">c_api</span><span class="o">.</span><span class="n">TF_OperationNumOutputs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span><span class="p">)</span>
    <span class="n">output_types</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">c_api</span><span class="o">.</span><span class="n">TF_OperationOutputType</span><span class="p">(</span><span class="n">c_api_util</span><span class="o">.</span><span class="n">tf_output</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span><span class="p">,</span> <span class="n">i</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_outputs</span><span class="p">)]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_outputs</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">output_type</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">output_type</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">output_types</span><span class="p">)</span>
    <span class="p">]</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_add_op</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">c_op</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_control_flow_post_processing</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">_control_flow_post_processing</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Add this op to its control flow context.</span>

<span class="sd">    This may add new ops and change this op&#39;s inputs. self.inputs must be</span>
<span class="sd">    available before calling this method.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">input_tensor</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">:</span>
      <span class="n">control_flow_util</span><span class="o">.</span><span class="n">CheckInputFromValidContext</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">op</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_control_flow_context</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_control_flow_context</span><span class="o">.</span><span class="n">AddOp</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_reconstruct_sequence_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op_def</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">attrs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Regroups a flat list of input tensors into scalar and sequence inputs.</span>

<span class="sd">    Args:</span>
<span class="sd">      op_def: The `op_def_pb2.OpDef` (for knowing the input types)</span>
<span class="sd">      inputs: a list of input `Tensor`s to the op.</span>
<span class="sd">      attrs: mapping from attr name to `attr_value_pb2.AttrValue` (these define</span>
<span class="sd">        how long each sequence is)</span>

<span class="sd">    Returns:</span>
<span class="sd">      A list of `Tensor`s (corresponding to scalar inputs) and lists of</span>
<span class="sd">      `Tensor`s (corresponding to sequence inputs).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">grouped_inputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">input_arg</span> <span class="ow">in</span> <span class="n">op_def</span><span class="o">.</span><span class="n">input_arg</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">input_arg</span><span class="o">.</span><span class="n">number_attr</span><span class="p">:</span>
        <span class="n">input_len</span> <span class="o">=</span> <span class="n">attrs</span><span class="p">[</span><span class="n">input_arg</span><span class="o">.</span><span class="n">number_attr</span><span class="p">]</span><span class="o">.</span><span class="n">i</span>
        <span class="n">is_sequence</span> <span class="o">=</span> <span class="kc">True</span>
      <span class="k">elif</span> <span class="n">input_arg</span><span class="o">.</span><span class="n">type_list_attr</span><span class="p">:</span>
        <span class="n">input_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">attrs</span><span class="p">[</span><span class="n">input_arg</span><span class="o">.</span><span class="n">type_list_attr</span><span class="p">]</span><span class="o">.</span><span class="n">list</span><span class="o">.</span><span class="n">type</span><span class="p">)</span>
        <span class="n">is_sequence</span> <span class="o">=</span> <span class="kc">True</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">input_len</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">is_sequence</span> <span class="o">=</span> <span class="kc">False</span>

      <span class="k">if</span> <span class="n">is_sequence</span><span class="p">:</span>
        <span class="n">grouped_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">input_len</span><span class="p">])</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">grouped_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
      <span class="n">i</span> <span class="o">+=</span> <span class="n">input_len</span>

    <span class="k">assert</span> <span class="n">i</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">grouped_inputs</span>

  <span class="k">def</span> <span class="nf">colocation_groups</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the list of colocation groups of the op.&quot;&quot;&quot;</span>
    <span class="n">default_colocation_group</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">compat</span><span class="o">.</span><span class="n">as_bytes</span><span class="p">(</span><span class="s2">&quot;loc:@</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">class_attr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_attr</span><span class="p">(</span><span class="s2">&quot;_class&quot;</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
      <span class="c1"># This op has no explicit colocation group, so it is itself its</span>
      <span class="c1"># own root of a colocation group.</span>
      <span class="k">return</span> <span class="n">default_colocation_group</span>

    <span class="n">attr_groups</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">class_name</span> <span class="k">for</span> <span class="n">class_name</span> <span class="ow">in</span> <span class="n">class_attr</span>
        <span class="k">if</span> <span class="n">class_name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="sa">b</span><span class="s2">&quot;loc:@&quot;</span><span class="p">)</span>
    <span class="p">]</span>

    <span class="c1"># If there are no colocation groups in the explicit _class field,</span>
    <span class="c1"># return the default colocation group.</span>
    <span class="k">return</span> <span class="n">attr_groups</span> <span class="k">if</span> <span class="n">attr_groups</span> <span class="k">else</span> <span class="n">default_colocation_group</span>

  <span class="k">def</span> <span class="nf">values</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;DEPRECATED: Use outputs.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_get_control_flow_context</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the control flow context of this op.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A context object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_control_flow_context</span>

  <span class="k">def</span> <span class="nf">_set_control_flow_context</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Sets the current control flow context of this op.</span>

<span class="sd">    Args:</span>
<span class="sd">      ctx: a context object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_control_flow_context</span> <span class="o">=</span> <span class="n">ctx</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The full name of this operation.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">c_api</span><span class="o">.</span><span class="n">TF_OperationName</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_id</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The unique integer id of this operation.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_id_value</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">device</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The name of the device to which this op has been assigned, if any.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The string name of the device to which this op has been</span>
<span class="sd">      assigned, or an empty string if it has not been assigned to a</span>
<span class="sd">      device.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">c_api</span><span class="o">.</span><span class="n">TF_OperationDevice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_device_assignments</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Code locations for device context managers active at op creation.</span>

<span class="sd">    This property will return a list of traceable_stack.TraceableObject</span>
<span class="sd">    instances where .obj is a string representing the assigned device</span>
<span class="sd">    (or information about the function that would be applied to this op</span>
<span class="sd">    to compute the desired device) and the filename and lineno members</span>
<span class="sd">    record the location of the relevant device context manager.</span>

<span class="sd">    For example, suppose file_a contained these lines:</span>

<span class="sd">      file_a.py:</span>
<span class="sd">        15: with tf.device(&#39;/gpu:0&#39;):</span>
<span class="sd">        16:   node_b = tf.constant(4, name=&#39;NODE_B&#39;)</span>

<span class="sd">    Then a TraceableObject t_obj representing the device context manager</span>
<span class="sd">    would have these member values:</span>

<span class="sd">      t_obj.obj -&gt; &#39;/gpu:0&#39;</span>
<span class="sd">      t_obj.filename = &#39;file_a.py&#39;</span>
<span class="sd">      t_obj.lineno = 15</span>

<span class="sd">    and node_b.op._device_assignments would return the list [t_obj].</span>

<span class="sd">    Returns:</span>
<span class="sd">      [str: traceable_stack.TraceableObject, ...] as per this method&#39;s</span>
<span class="sd">      description, above.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device_code_locations</span> <span class="ow">or</span> <span class="p">[]</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_colocation_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Code locations for colocation context managers active at op creation.</span>

<span class="sd">    This property will return a dictionary for which the keys are nodes with</span>
<span class="sd">    which this Operation is colocated, and for which the values are</span>
<span class="sd">    traceable_stack.TraceableObject instances.  The TraceableObject instances</span>
<span class="sd">    record the location of the relevant colocation context manager but have the</span>
<span class="sd">    &quot;obj&quot; field set to None to prevent leaking private data.</span>

<span class="sd">    For example, suppose file_a contained these lines:</span>

<span class="sd">      file_a.py:</span>
<span class="sd">        14: node_a = tf.constant(3, name=&#39;NODE_A&#39;)</span>
<span class="sd">        15: with tf.colocate_with(node_a):</span>
<span class="sd">        16:   node_b = tf.constant(4, name=&#39;NODE_B&#39;)</span>

<span class="sd">    Then a TraceableObject t_obj representing the colocation context manager</span>
<span class="sd">    would have these member values:</span>

<span class="sd">      t_obj.obj -&gt; None</span>
<span class="sd">      t_obj.filename = &#39;file_a.py&#39;</span>
<span class="sd">      t_obj.lineno = 15</span>

<span class="sd">    and node_b.op._colocation_dict would return the dictionary</span>

<span class="sd">      { &#39;NODE_A&#39;: t_obj }</span>

<span class="sd">    Returns:</span>
<span class="sd">      {str: traceable_stack.TraceableObject} as per this method&#39;s description,</span>
<span class="sd">      above.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">locations_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_colocation_code_locations</span> <span class="ow">or</span> <span class="p">{}</span>
    <span class="k">return</span> <span class="n">locations_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_output_types</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;List this operation&#39;s output types.</span>

<span class="sd">    Returns:</span>
<span class="sd">      List of the types of the Tensors computed by this operation.</span>
<span class="sd">      Each element in the list is an integer whose value is one of</span>
<span class="sd">      the TF_DataType enums defined in c_api.h</span>
<span class="sd">      The length of this list indicates the number of output endpoints</span>
<span class="sd">      of the operation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">num_outputs</span> <span class="o">=</span> <span class="n">c_api</span><span class="o">.</span><span class="n">TF_OperationNumOutputs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span><span class="p">)</span>
    <span class="n">output_types</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">c_api</span><span class="o">.</span><span class="n">TF_OperationOutputType</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_tf_output</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="n">num_outputs</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="c1"># In all the tests we have output_types that are passed into</span>
    <span class="c1"># Operation.__init__ are a list of ints (which is illegal according</span>
    <span class="c1"># to the docstring), but input_types are instances of DType.</span>
    <span class="c1"># This extra assert is to catch if we ever use DType for output_types.</span>
    <span class="k">if</span> <span class="n">output_types</span><span class="p">:</span>
      <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_types</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">int</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output_types</span>

  <span class="k">def</span> <span class="nf">_tf_output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_idx</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Create and return a new TF_Output for output_idx&#39;th output of this op.&quot;&quot;&quot;</span>
    <span class="n">tf_output</span> <span class="o">=</span> <span class="n">c_api</span><span class="o">.</span><span class="n">TF_Output</span><span class="p">()</span>
    <span class="n">tf_output</span><span class="o">.</span><span class="n">oper</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span>
    <span class="n">tf_output</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="n">output_idx</span>
    <span class="k">return</span> <span class="n">tf_output</span>

  <span class="k">def</span> <span class="nf">_tf_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_idx</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Create and return a new TF_Input for input_idx&#39;th input of this op.&quot;&quot;&quot;</span>
    <span class="n">tf_input</span> <span class="o">=</span> <span class="n">c_api</span><span class="o">.</span><span class="n">TF_Input</span><span class="p">()</span>
    <span class="n">tf_input</span><span class="o">.</span><span class="n">oper</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span>
    <span class="n">tf_input</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="n">input_idx</span>
    <span class="k">return</span> <span class="n">tf_input</span>

  <span class="k">def</span> <span class="nf">_set_device</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>  <span class="c1"># pylint: disable=redefined-outer-name</span>
    <span class="sd">&quot;&quot;&quot;Set the device of this operation.</span>

<span class="sd">    Args:</span>
<span class="sd">      device: string or device..  The device to set.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">c_api</span><span class="o">.</span><span class="n">SetRequestedDevice</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_c_graph</span><span class="p">,</span>  <span class="c1"># pylint: disable=protected-access</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span><span class="p">,</span>  <span class="c1"># pylint: disable=protected-access</span>
        <span class="n">compat</span><span class="o">.</span><span class="n">as_str</span><span class="p">(</span><span class="n">_device_string</span><span class="p">(</span><span class="n">device</span><span class="p">)))</span>

  <span class="k">def</span> <span class="nf">_update_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">tensor</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Update the input to this operation at the given index.</span>

<span class="sd">    NOTE: This is for TF internal use only. Please don&#39;t use it.</span>

<span class="sd">    Args:</span>
<span class="sd">      index: the index of the input to update.</span>
<span class="sd">      tensor: the Tensor to be used as the input at the given index.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if tensor is not a Tensor,</span>
<span class="sd">        or if input tensor type is not convertible to dtype.</span>
<span class="sd">      ValueError: if the Tensor is from a different graph.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;tensor must be a Tensor: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">tensor</span><span class="p">)</span>
    <span class="n">_assert_same_graph</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">)</span>

    <span class="c1"># Make sure output shapes are already computed for this op in case we create</span>
    <span class="c1"># a cycle (we cannot compute shapes for cycles). Usually shapes are computed</span>
    <span class="c1"># lazily upon request.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_USE_C_SHAPES</span><span class="p">:</span>
      <span class="n">set_shape_and_handle_data_for_outputs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="c1"># Reset cached inputs.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_inputs_val</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">c_api</span><span class="o">.</span><span class="n">UpdateEdge</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_c_graph</span><span class="p">,</span>  <span class="c1"># pylint: disable=protected-access</span>
        <span class="n">tensor</span><span class="o">.</span><span class="n">_as_tf_output</span><span class="p">(),</span>  <span class="c1"># pylint: disable=protected-access</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tf_input</span><span class="p">(</span><span class="n">index</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">_add_control_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ops</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Add a list of new control inputs to this operation.</span>

<span class="sd">    Args:</span>
<span class="sd">      ops: the list of Operations to add as control input.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if ops is not a list of Operations.</span>
<span class="sd">      ValueError: if any op in ops is from a different graph.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">op</span> <span class="ow">in</span> <span class="n">ops</span><span class="p">:</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">Operation</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;op must be an Operation: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">op</span><span class="p">)</span>
      <span class="n">c_api</span><span class="o">.</span><span class="n">AddControlInput</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_c_graph</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span><span class="p">,</span> <span class="n">op</span><span class="o">.</span><span class="n">_c_op</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>

  <span class="k">def</span> <span class="nf">_add_control_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Add a new control input to this operation.</span>

<span class="sd">    Args:</span>
<span class="sd">      op: the Operation to add as control input.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if op is not an Operation.</span>
<span class="sd">      ValueError: if op is from a different graph.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">Operation</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;op must be an Operation: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">op</span><span class="p">)</span>
    <span class="n">c_api</span><span class="o">.</span><span class="n">AddControlInput</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_c_graph</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span><span class="p">,</span> <span class="n">op</span><span class="o">.</span><span class="n">_c_op</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>

  <span class="k">def</span> <span class="nf">_remove_all_control_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Removes any control inputs to this operation.&quot;&quot;&quot;</span>
    <span class="n">c_api</span><span class="o">.</span><span class="n">RemoveAllControlInputs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_c_graph</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>

  <span class="k">def</span> <span class="nf">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">node_def</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="s2">&quot;&lt;tf.Operation &#39;</span><span class="si">%s</span><span class="s2">&#39; type=</span><span class="si">%s</span><span class="s2">&gt;&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">type</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">outputs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The list of `Tensor` objects representing the outputs of this op.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_outputs</span>

<span class="c1"># pylint: disable=protected-access</span>

  <span class="k">class</span> <span class="nc">_InputList</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Immutable input list wrapper.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_inputs</span> <span class="o">=</span> <span class="n">inputs</span>

    <span class="k">def</span> <span class="nf">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
      <span class="k">return</span> <span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_inputs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
      <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_inputs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__bool__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
      <span class="k">return</span> <span class="nb">bool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_inputs</span><span class="p">)</span>

    <span class="c1"># Python 3 wants __bool__, Python 2.7 wants __nonzero__</span>
    <span class="n">__nonzero__</span> <span class="o">=</span> <span class="fm">__bool__</span>

    <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

<span class="c1"># pylint: enable=protected-access</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The list of `Tensor` objects representing the data inputs of this op.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inputs_val</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">tf_outputs</span> <span class="o">=</span> <span class="n">c_api</span><span class="o">.</span><span class="n">GetOperationInputs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span><span class="p">)</span>
      <span class="c1"># pylint: disable=protected-access</span>
      <span class="n">retval</span> <span class="o">=</span> <span class="p">[</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">_get_tensor_by_tf_output</span><span class="p">(</span><span class="n">tf_output</span><span class="p">)</span>
          <span class="k">for</span> <span class="n">tf_output</span> <span class="ow">in</span> <span class="n">tf_outputs</span>
      <span class="p">]</span>
      <span class="c1"># pylint: enable=protected-access</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_inputs_val</span> <span class="o">=</span> <span class="n">Operation</span><span class="o">.</span><span class="n">_InputList</span><span class="p">(</span><span class="n">retval</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inputs_val</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Operation._inputs is private, use Operation.inputs &quot;</span>
                    <span class="s2">&quot;instead. Operation._inputs will eventually be removed.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span>

  <span class="nd">@_inputs</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot assign _inputs&quot;</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_input_types</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">num_inputs</span> <span class="o">=</span> <span class="n">c_api</span><span class="o">.</span><span class="n">TF_OperationNumInputs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span><span class="p">)</span>
    <span class="n">input_types</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">c_api</span><span class="o">.</span><span class="n">TF_OperationInputType</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_tf_input</span><span class="p">(</span><span class="n">i</span><span class="p">)))</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="n">num_inputs</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="k">return</span> <span class="n">input_types</span>

  <span class="nd">@_input_types</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">_input_types</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot assign _input_types&quot;</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">control_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The `Operation` objects on which this op has a control dependency.</span>

<span class="sd">    Before this op is executed, TensorFlow will ensure that the</span>
<span class="sd">    operations in `self.control_inputs` have finished executing. This</span>
<span class="sd">    mechanism can be used to run ops sequentially for performance</span>
<span class="sd">    reasons, or to ensure that the side effects of an op are observed</span>
<span class="sd">    in the correct order.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A list of `Operation` objects.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">control_c_ops</span> <span class="o">=</span> <span class="n">c_api</span><span class="o">.</span><span class="n">TF_OperationGetControlInputs_wrapper</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span><span class="p">)</span>
    <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">return</span> <span class="p">[</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">_get_operation_by_name_unsafe</span><span class="p">(</span>
            <span class="n">c_api</span><span class="o">.</span><span class="n">TF_OperationName</span><span class="p">(</span><span class="n">c_op</span><span class="p">))</span> <span class="k">for</span> <span class="n">c_op</span> <span class="ow">in</span> <span class="n">control_c_ops</span>
    <span class="p">]</span>
    <span class="c1"># pylint: enable=protected-access</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_control_outputs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The `Operation` objects which have a control dependency on this op.</span>

<span class="sd">    Before any of the ops in self._control_outputs can execute tensorflow will</span>
<span class="sd">    ensure self has finished executing.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A list of `Operation` objects.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">control_c_ops</span> <span class="o">=</span> <span class="n">c_api</span><span class="o">.</span><span class="n">TF_OperationGetControlOutputs_wrapper</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span><span class="p">)</span>
    <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">return</span> <span class="p">[</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">_get_operation_by_name_unsafe</span><span class="p">(</span>
            <span class="n">c_api</span><span class="o">.</span><span class="n">TF_OperationName</span><span class="p">(</span><span class="n">c_op</span><span class="p">))</span> <span class="k">for</span> <span class="n">c_op</span> <span class="ow">in</span> <span class="n">control_c_ops</span>
    <span class="p">]</span>
    <span class="c1"># pylint: enable=protected-access</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_control_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Operation._control_inputs is private, use &quot;</span>
                    <span class="s2">&quot;Operation.control_inputs instead. &quot;</span>
                    <span class="s2">&quot;Operation._control_inputs will eventually be removed.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">control_inputs</span>

  <span class="nd">@_control_inputs</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">_control_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Operation._control_inputs is private, use &quot;</span>
                    <span class="s2">&quot;Operation.control_inputs instead. &quot;</span>
                    <span class="s2">&quot;Operation._control_inputs will eventually be removed.&quot;</span><span class="p">)</span>
    <span class="c1"># Copy value because it may be self._control_inputs_val (in particular if</span>
    <span class="c1"># this is called from self._control_inputs += ...), and we don&#39;t want to</span>
    <span class="c1"># clear value below.</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_remove_all_control_inputs</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_add_control_inputs</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">type</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The type of the op (e.g. `&quot;MatMul&quot;`).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">c_api</span><span class="o">.</span><span class="n">TF_OperationOpType</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">graph</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The `Graph` that contains this operation.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">node_def</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># pylint: disable=line-too-long</span>
    <span class="sd">&quot;&quot;&quot;Returns the `NodeDef` representation of this operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A</span>
<span class="sd">      [`NodeDef`](https://www.tensorflow.org/code/tensorflow/core/framework/node_def.proto)</span>
<span class="sd">      protocol buffer.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># pylint: enable=line-too-long</span>
    <span class="k">with</span> <span class="n">c_api_util</span><span class="o">.</span><span class="n">tf_buffer</span><span class="p">()</span> <span class="k">as</span> <span class="n">buf</span><span class="p">:</span>
      <span class="n">c_api</span><span class="o">.</span><span class="n">TF_OperationToNodeDef</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span><span class="p">,</span> <span class="n">buf</span><span class="p">)</span>
      <span class="n">data</span> <span class="o">=</span> <span class="n">c_api</span><span class="o">.</span><span class="n">TF_GetBuffer</span><span class="p">(</span><span class="n">buf</span><span class="p">)</span>
    <span class="n">node_def</span> <span class="o">=</span> <span class="n">node_def_pb2</span><span class="o">.</span><span class="n">NodeDef</span><span class="p">()</span>
    <span class="n">node_def</span><span class="o">.</span><span class="n">ParseFromString</span><span class="p">(</span><span class="n">compat</span><span class="o">.</span><span class="n">as_bytes</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">node_def</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_node_def</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Operation._node_def is private, use Operation.node_def &quot;</span>
                    <span class="s2">&quot;instead. Operation._node_def will eventually be removed.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">node_def</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">op_def</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># pylint: disable=line-too-long</span>
    <span class="sd">&quot;&quot;&quot;Returns the `OpDef` proto that represents the type of this op.</span>

<span class="sd">    Returns:</span>
<span class="sd">      An</span>
<span class="sd">      [`OpDef`](https://www.tensorflow.org/code/tensorflow/core/framework/op_def.proto)</span>
<span class="sd">      protocol buffer.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># pylint: enable=line-too-long</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_get_op_def</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">type</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_op_def</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Operation._op_def is private, use Operation.op_def &quot;</span>
                    <span class="s2">&quot;instead. Operation._op_def will eventually be removed.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">op_def</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">traceback</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the call stack from when this operation was constructed.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tf_stack</span><span class="o">.</span><span class="n">convert_stack</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_traceback</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">traceback_with_start_lines</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Same as traceback but includes start line of function definition.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A list of 5-tuples (filename, lineno, name, code, func_start_lineno).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tf_stack</span><span class="o">.</span><span class="n">convert_stack</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_traceback</span><span class="p">,</span>
                                  <span class="n">include_func_start_lineno</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_set_attr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">,</span> <span class="n">attr_value</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Private method used to set an attribute in the node_def.&quot;&quot;&quot;</span>
    <span class="n">buf</span> <span class="o">=</span> <span class="n">c_api</span><span class="o">.</span><span class="n">TF_NewBufferFromString</span><span class="p">(</span>
        <span class="n">compat</span><span class="o">.</span><span class="n">as_bytes</span><span class="p">(</span><span class="n">attr_value</span><span class="o">.</span><span class="n">SerializeToString</span><span class="p">()))</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="c1"># pylint: disable=protected-access</span>
      <span class="n">c_api</span><span class="o">.</span><span class="n">SetAttr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_c_graph</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">,</span> <span class="n">buf</span><span class="p">)</span>
      <span class="c1"># pylint: enable=protected-access</span>
    <span class="k">finally</span><span class="p">:</span>
      <span class="n">c_api</span><span class="o">.</span><span class="n">TF_DeleteBuffer</span><span class="p">(</span><span class="n">buf</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">get_attr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the value of the attr of this op with the given `name`.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: The name of the attr to fetch.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The value of the attr, as a Python object.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If this op does not have an attr with the given `name`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">fields</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;s&quot;</span><span class="p">,</span> <span class="s2">&quot;i&quot;</span><span class="p">,</span> <span class="s2">&quot;f&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;type&quot;</span><span class="p">,</span> <span class="s2">&quot;shape&quot;</span><span class="p">,</span> <span class="s2">&quot;tensor&quot;</span><span class="p">,</span> <span class="s2">&quot;func&quot;</span><span class="p">]</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">with</span> <span class="n">c_api_util</span><span class="o">.</span><span class="n">tf_buffer</span><span class="p">()</span> <span class="k">as</span> <span class="n">buf</span><span class="p">:</span>
        <span class="n">c_api</span><span class="o">.</span><span class="n">TF_OperationGetAttrValueProto</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">buf</span><span class="p">)</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">c_api</span><span class="o">.</span><span class="n">TF_GetBuffer</span><span class="p">(</span><span class="n">buf</span><span class="p">)</span>
    <span class="k">except</span> <span class="n">errors</span><span class="o">.</span><span class="n">InvalidArgumentError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="c1"># Convert to ValueError for backwards compatibility.</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">attr_value_pb2</span><span class="o">.</span><span class="n">AttrValue</span><span class="p">()</span>
    <span class="n">x</span><span class="o">.</span><span class="n">ParseFromString</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="c1"># Treat an empty oneof value as an empty list.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">x</span><span class="o">.</span><span class="n">WhichOneof</span><span class="p">(</span><span class="s2">&quot;value&quot;</span><span class="p">):</span>
      <span class="k">return</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">HasField</span><span class="p">(</span><span class="s2">&quot;list&quot;</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">fields</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">list</span><span class="p">,</span> <span class="n">f</span><span class="p">):</span>
          <span class="k">if</span> <span class="n">f</span> <span class="o">==</span> <span class="s2">&quot;type&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">list</span><span class="p">,</span> <span class="n">f</span><span class="p">))]</span>
          <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">list</span><span class="p">,</span> <span class="n">f</span><span class="p">))</span>
      <span class="k">return</span> <span class="p">[]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">fields</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">HasField</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
          <span class="k">if</span> <span class="n">f</span> <span class="o">==</span> <span class="s2">&quot;type&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">))</span>
          <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
      <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;Unsupported field type in &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">session</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Runs this operation in a `Session`.</span>

<span class="sd">    Calling this method will execute all preceding operations that</span>
<span class="sd">    produce the inputs needed for this operation.</span>

<span class="sd">    *N.B.* Before invoking `Operation.run()`, its graph must have been</span>
<span class="sd">    launched in a session, and either a default session must be</span>
<span class="sd">    available, or `session` must be specified explicitly.</span>

<span class="sd">    Args:</span>
<span class="sd">      feed_dict: A dictionary that maps `Tensor` objects to feed values.</span>
<span class="sd">        See @{tf.Session.run}</span>
<span class="sd">        for a description of the valid feed values.</span>
<span class="sd">      session: (Optional.) The `Session` to be used to run to this operation. If</span>
<span class="sd">        none, the default session will be used.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_run_using_default_session</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">graph</span><span class="p">,</span> <span class="n">session</span><span class="p">)</span>

<span class="n">_gradient_registry</span> <span class="o">=</span> <span class="n">registry</span><span class="o">.</span><span class="n">Registry</span><span class="p">(</span><span class="s2">&quot;gradient&quot;</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;RegisterGradient&quot;</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">RegisterGradient</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A decorator for registering the gradient function for an op type.</span>

<span class="sd">  This decorator is only used when defining a new op type. For an op</span>
<span class="sd">  with `m` inputs and `n` outputs, the gradient function is a function</span>
<span class="sd">  that takes the original `Operation` and `n` `Tensor` objects</span>
<span class="sd">  (representing the gradients with respect to each output of the op),</span>
<span class="sd">  and returns `m` `Tensor` objects (representing the partial gradients</span>
<span class="sd">  with respect to each input of the op).</span>

<span class="sd">  For example, assuming that operations of type `&quot;Sub&quot;` take two</span>
<span class="sd">  inputs `x` and `y`, and return a single output `x - y`, the</span>
<span class="sd">  following gradient function would be registered:</span>

<span class="sd">  ```python</span>
<span class="sd">  @tf.RegisterGradient(&quot;Sub&quot;)</span>
<span class="sd">  def _sub_grad(unused_op, grad):</span>
<span class="sd">    return grad, tf.negative(grad)</span>
<span class="sd">  ```</span>

<span class="sd">  The decorator argument `op_type` is the string type of an</span>
<span class="sd">  operation. This corresponds to the `OpDef.name` field for the proto</span>
<span class="sd">  that defines the operation.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op_type</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates a new decorator with `op_type` as the Operation type.</span>

<span class="sd">    Args:</span>
<span class="sd">      op_type: The string type of an operation. This corresponds to the</span>
<span class="sd">        `OpDef.name` field for the proto that defines the operation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op_type</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;op_type must be a string&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_op_type</span> <span class="o">=</span> <span class="n">op_type</span>

  <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Registers the function `f` as gradient function for `op_type`.&quot;&quot;&quot;</span>
    <span class="n">_gradient_registry</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op_type</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;NoGradient&quot;</span><span class="p">,</span> <span class="s2">&quot;NotDifferentiable&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">NotDifferentiable</span><span class="p">(</span><span class="n">op_type</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Specifies that ops of type `op_type` is not differentiable.</span>

<span class="sd">  This function should *not* be used for operations that have a</span>
<span class="sd">  well-defined gradient that is not yet implemented.</span>

<span class="sd">  This function is only used when defining a new op type. It may be</span>
<span class="sd">  used for ops such as `tf.size()` that are not differentiable.  For</span>
<span class="sd">  example:</span>

<span class="sd">  ```python</span>
<span class="sd">  tf.NotDifferentiable(&quot;Size&quot;)</span>
<span class="sd">  ```</span>

<span class="sd">  The gradient computed for &#39;op_type&#39; will then propagate zeros.</span>

<span class="sd">  For ops that have a well-defined gradient but are not yet implemented,</span>
<span class="sd">  no declaration should be made, and an error *must* be thrown if</span>
<span class="sd">  an attempt to request its gradient is made.</span>

<span class="sd">  Args:</span>
<span class="sd">    op_type: The string type of an operation. This corresponds to the</span>
<span class="sd">      `OpDef.name` field for the proto that defines the operation.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If `op_type` is not a string.</span>

<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op_type</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;op_type must be a string&quot;</span><span class="p">)</span>
  <span class="n">_gradient_registry</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">op_type</span><span class="p">)</span>


<span class="c1"># Alias for the old name, will be eventually removed.</span>
<span class="n">NoGradient</span> <span class="o">=</span> <span class="n">NotDifferentiable</span>


<span class="k">def</span> <span class="nf">get_gradient_function</span><span class="p">(</span><span class="n">op</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns the function that computes gradients for &quot;op&quot;.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">:</span>
    <span class="k">return</span> <span class="kc">None</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">op_type</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">get_attr</span><span class="p">(</span><span class="s2">&quot;_gradient_op_type&quot;</span><span class="p">)</span>
  <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
    <span class="n">op_type</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">type</span>
  <span class="k">return</span> <span class="n">_gradient_registry</span><span class="o">.</span><span class="n">lookup</span><span class="p">(</span><span class="n">op_type</span><span class="p">)</span>


<span class="n">_shape_registry</span> <span class="o">=</span> <span class="n">registry</span><span class="o">.</span><span class="n">Registry</span><span class="p">(</span><span class="s2">&quot;shape functions&quot;</span><span class="p">)</span>
<span class="n">_default_shape_function_registry</span> <span class="o">=</span> <span class="n">registry</span><span class="o">.</span><span class="n">Registry</span><span class="p">(</span><span class="s2">&quot;default shape functions&quot;</span><span class="p">)</span>

<span class="c1"># These are set to common_shapes.call_cpp_shape_fn by op generated code</span>
<span class="c1"># (generated by python_op_gen.cc).</span>
<span class="c1"># It is set outside ops.py to avoid a circular dependency.</span>
<span class="n">_call_cpp_shape_fn</span> <span class="o">=</span> <span class="kc">None</span>
<span class="n">_call_cpp_shape_fn_and_require_op</span> <span class="o">=</span> <span class="kc">None</span>


<span class="k">def</span> <span class="nf">_set_call_cpp_shape_fn</span><span class="p">(</span><span class="n">call_cpp_shape_fn</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Sets default shape fns from passed common_shapes.call_cpp_shape_fn.&quot;&quot;&quot;</span>
  <span class="k">global</span> <span class="n">_call_cpp_shape_fn</span><span class="p">,</span> <span class="n">_call_cpp_shape_fn_and_require_op</span>
  <span class="k">if</span> <span class="n">_call_cpp_shape_fn</span><span class="p">:</span>
    <span class="k">return</span>  <span class="c1"># already registered</span>

  <span class="k">def</span> <span class="nf">call_without_requiring</span><span class="p">(</span><span class="n">op</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">call_cpp_shape_fn</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">require_shape_fn</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

  <span class="n">_call_cpp_shape_fn</span> <span class="o">=</span> <span class="n">call_without_requiring</span>

  <span class="k">def</span> <span class="nf">call_with_requiring</span><span class="p">(</span><span class="n">op</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">call_cpp_shape_fn</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">require_shape_fn</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

  <span class="n">_call_cpp_shape_fn_and_require_op</span> <span class="o">=</span> <span class="n">call_with_requiring</span>


<span class="k">class</span> <span class="nc">RegisterShape</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;No longer used.  Was: A decorator for registering a shape function.</span>

<span class="sd">  Shape functions must now be registered via the SetShapeFn on the</span>
<span class="sd">  original Op specification in C++.</span>

<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op_type</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Saves the `op_type` as the `Operation` type.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op_type</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;op_type must be a string&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_op_type</span> <span class="o">=</span> <span class="n">op_type</span>

  <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Registers &quot;f&quot; as the shape function for &quot;op_type&quot;.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">f</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">_call_cpp_shape_fn</span>

      <span class="c1"># None is a special &quot;weak&quot; value that provides a default shape function,</span>
      <span class="c1"># and can be overridden by a non-None registration.</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="n">_default_shape_function_registry</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="n">_call_cpp_shape_fn</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">_op_type</span><span class="p">)</span>
      <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
        <span class="c1"># Ignore duplicate registrations of the weak value. This can</span>
        <span class="c1"># occur if the op library input to wrapper generation</span>
        <span class="c1"># inadvertently links in one or more of the standard op</span>
        <span class="c1"># libraries.</span>
        <span class="k">pass</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">_shape_registry</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op_type</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span>


<span class="c1"># TODO(b/74620627): remove when _USE_C_SHAPES is removed</span>
<span class="k">def</span> <span class="nf">_set_shape_and_handle_data_for_outputs_c_api</span><span class="p">(</span><span class="n">op</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Set shapes and resource handle data using info from the C API.&quot;&quot;&quot;</span>
  <span class="k">assert</span> <span class="ow">not</span> <span class="n">_USE_C_SHAPES</span>
  <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">op</span><span class="o">.</span><span class="n">outputs</span><span class="p">:</span>
    <span class="n">output</span><span class="o">.</span><span class="n">_shape_val</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">_c_api_shape</span><span class="p">()</span>
    <span class="c1"># Set the resource handle data for compatibility with the Python shape</span>
    <span class="c1"># inference code.</span>
    <span class="n">serialized</span> <span class="o">=</span> <span class="n">c_api</span><span class="o">.</span><span class="n">GetResourceHandleShapeAndType</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_c_graph</span><span class="p">,</span>
                                                     <span class="n">output</span><span class="o">.</span><span class="n">_as_tf_output</span><span class="p">())</span>
    <span class="k">if</span> <span class="n">serialized</span><span class="p">:</span>
      <span class="n">output</span><span class="o">.</span><span class="n">_handle_data</span> <span class="o">=</span> <span class="p">(</span>
          <span class="n">cpp_shape_inference_pb2</span><span class="o">.</span><span class="n">CppShapeInferenceResult</span><span class="o">.</span><span class="n">HandleData</span>
          <span class="o">.</span><span class="n">FromString</span><span class="p">(</span><span class="n">compat</span><span class="o">.</span><span class="n">as_bytes</span><span class="p">(</span><span class="n">serialized</span><span class="p">)))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">output</span><span class="o">.</span><span class="n">_handle_data</span> <span class="o">=</span> <span class="kc">None</span>


<span class="c1"># TODO(b/74620627): remove when _USE_C_SHAPES is removed</span>
<span class="k">def</span> <span class="nf">set_shape_and_handle_data_for_outputs</span><span class="p">(</span><span class="n">op</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Set the shapes and resource handle data for op&#39;s outputs.</span>

<span class="sd">  When _USE_C_SHAPES = False, this is lazily called when a tensor&#39;s shape is</span>
<span class="sd">  first requested. Usually this should work automatically, but some edge cases</span>
<span class="sd">  may require manually calling this first to make sure Tensor._shape_val and</span>
<span class="sd">  Tensor._handle_data are set (e.g. manually overriding _handle_data, copying a</span>
<span class="sd">  Tensor).</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">_USE_C_SHAPES</span><span class="p">:</span> <span class="k">return</span>

  <span class="k">if</span> <span class="n">op</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">_is_function</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">type</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">op</span><span class="o">.</span><span class="n">outputs</span><span class="p">:</span>
      <span class="n">output</span><span class="o">.</span><span class="n">_shape_val</span> <span class="o">=</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">unknown_shape</span><span class="p">()</span>
    <span class="k">return</span>

  <span class="k">try</span><span class="p">:</span>
    <span class="n">shape_func</span> <span class="o">=</span> <span class="n">_shape_registry</span><span class="o">.</span><span class="n">lookup</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">type</span><span class="p">)</span>
  <span class="k">except</span> <span class="ne">LookupError</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">shape_func</span> <span class="o">=</span> <span class="n">_default_shape_function_registry</span><span class="o">.</span><span class="n">lookup</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">type</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">LookupError</span><span class="p">:</span>
      <span class="n">shape_func</span> <span class="o">=</span> <span class="n">_call_cpp_shape_fn_and_require_op</span>

  <span class="n">shapes</span> <span class="o">=</span> <span class="n">shape_func</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">shapes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
        <span class="s2">&quot;Shape function for op </span><span class="si">%s</span><span class="s2"> did not return any shapes&quot;</span> <span class="o">%</span> <span class="n">op</span><span class="p">)</span>
  <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shapes</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
    <span class="c1"># Returned by call_cpp_shape_fn</span>
    <span class="n">shapes_dict</span> <span class="o">=</span> <span class="n">shapes</span>
    <span class="n">shapes</span> <span class="o">=</span> <span class="n">shapes_dict</span><span class="p">[</span><span class="s2">&quot;shapes&quot;</span><span class="p">]</span>
    <span class="n">handle_datas</span> <span class="o">=</span> <span class="n">shapes_dict</span><span class="p">[</span><span class="s2">&quot;handle_data&quot;</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">output</span><span class="p">,</span> <span class="n">handle_data</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">outputs</span><span class="p">,</span> <span class="n">handle_datas</span><span class="p">):</span>
      <span class="c1"># Don&#39;t override any existing handle data that may have been manually set.</span>
      <span class="c1"># pylint: disable=protected-access</span>
      <span class="k">if</span> <span class="n">output</span><span class="o">.</span><span class="n">_handle_data</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">output</span><span class="o">.</span><span class="n">_handle_data</span> <span class="o">=</span> <span class="n">handle_data</span>
      <span class="c1"># pylint: enable=protected-access</span>

  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">outputs</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">shapes</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
        <span class="s2">&quot;Shape function for op </span><span class="si">%s</span><span class="s2"> returned </span><span class="si">%d</span><span class="s2"> shapes but expected </span><span class="si">%d</span><span class="s2"> </span><span class="si">%s</span><span class="s2"> </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span>
        <span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">shapes</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">outputs</span><span class="p">),</span> <span class="n">shape_func</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">shapes</span><span class="p">)))</span>
  <span class="k">for</span> <span class="n">output</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">outputs</span><span class="p">,</span> <span class="n">shapes</span><span class="p">):</span>
    <span class="n">output</span><span class="o">.</span><span class="n">_shape_val</span> <span class="o">=</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">unknown_shape</span><span class="p">()</span>
    <span class="n">output</span><span class="o">.</span><span class="n">_shape_val</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">_shape_val</span><span class="o">.</span><span class="n">merge_with</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">OpStats</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A holder for statistics about an operator.</span>

<span class="sd">  This class holds information about the resource requirements for an op,</span>
<span class="sd">  including the size of its weight parameters on-disk and how many FLOPS it</span>
<span class="sd">  requires to execute forward inference.</span>

<span class="sd">  If you define a new operation, you can create a function that will return a</span>
<span class="sd">  set of information about its usage of the CPU and disk space when serialized.</span>
<span class="sd">  The function itself takes a Graph object that&#39;s been set up so you can call</span>
<span class="sd">  methods like get_tensor_by_name to help calculate the results, and a NodeDef</span>
<span class="sd">  argument.</span>

<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">statistic_type</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Sets up the initial placeholders for the statistics.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">statistic_type</span> <span class="o">=</span> <span class="n">statistic_type</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">value</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">statistic_type</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_statistic_type</span>

  <span class="nd">@statistic_type</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">statistic_type</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">statistic_type</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_statistic_type</span> <span class="o">=</span> <span class="n">statistic_type</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">value</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_value</span>

  <span class="nd">@value</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_value</span> <span class="o">=</span> <span class="n">value</span>

  <span class="k">def</span> <span class="nf">__iadd__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">other</span><span class="o">.</span><span class="n">statistic_type</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">statistic_type</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Can&#39;t add an OpStat of type </span><span class="si">%s</span><span class="s2"> to one of </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span>
                       <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">statistic_type</span><span class="p">,</span> <span class="n">other</span><span class="o">.</span><span class="n">statistic_type</span><span class="p">))</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">other</span><span class="o">.</span><span class="n">value</span>
    <span class="k">elif</span> <span class="n">other</span><span class="o">.</span><span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_value</span> <span class="o">+=</span> <span class="n">other</span><span class="o">.</span><span class="n">value</span>
    <span class="k">return</span> <span class="bp">self</span>


<span class="n">_stats_registry</span> <span class="o">=</span> <span class="n">registry</span><span class="o">.</span><span class="n">Registry</span><span class="p">(</span><span class="s2">&quot;statistical functions&quot;</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">RegisterStatistics</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A decorator for registering the statistics function for an op type.</span>

<span class="sd">  This decorator can be defined for an op type so that it gives a</span>
<span class="sd">  report on the resources used by an instance of an operator, in the</span>
<span class="sd">  form of an OpStats object.</span>

<span class="sd">  Well-known types of statistics include these so far:</span>

<span class="sd">  - flops: When running a graph, the bulk of the computation happens doing</span>
<span class="sd">    numerical calculations like matrix multiplications. This type allows a node</span>
<span class="sd">    to return how many floating-point operations it takes to complete. The</span>
<span class="sd">    total number of FLOPs for a graph is a good guide to its expected latency.</span>

<span class="sd">  You can add your own statistics just by picking a new type string, registering</span>
<span class="sd">  functions for the ops you care about, and then calling get_stats_for_node_def.</span>

<span class="sd">  If a statistic for an op is registered multiple times, a KeyError will be</span>
<span class="sd">  raised.</span>

<span class="sd">  Since the statistics is counted on a per-op basis. It is not suitable for</span>
<span class="sd">  model parameters (capacity), which is expected to be counted only once, even</span>
<span class="sd">  if it is shared by multiple ops. (e.g. RNN)</span>

<span class="sd">  For example, you can define a new metric called doohickey for a Foo operation</span>
<span class="sd">  by placing this in your code:</span>

<span class="sd">  ```python</span>
<span class="sd">  @ops.RegisterStatistics(&quot;Foo&quot;, &quot;doohickey&quot;)</span>
<span class="sd">  def _calc_foo_bojangles(unused_graph, unused_node_def):</span>
<span class="sd">    return ops.OpStats(&quot;doohickey&quot;, 20)</span>
<span class="sd">  ```</span>

<span class="sd">  Then in client code you can retrieve the value by making this call:</span>

<span class="sd">  ```python</span>
<span class="sd">  doohickey = ops.get_stats_for_node_def(graph, node_def, &quot;doohickey&quot;)</span>
<span class="sd">  ```</span>

<span class="sd">  If the NodeDef is for an op with a registered doohickey function, you&#39;ll get</span>
<span class="sd">  back the calculated amount in doohickey.value, or None if it&#39;s not defined.</span>

<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op_type</span><span class="p">,</span> <span class="n">statistic_type</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Saves the `op_type` as the `Operation` type.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op_type</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;op_type must be a string.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="s2">&quot;,&quot;</span> <span class="ow">in</span> <span class="n">op_type</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;op_type must not contain a comma.&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_op_type</span> <span class="o">=</span> <span class="n">op_type</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">statistic_type</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;statistic_type must be a string.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="s2">&quot;,&quot;</span> <span class="ow">in</span> <span class="n">statistic_type</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;statistic_type must not contain a comma.&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_statistic_type</span> <span class="o">=</span> <span class="n">statistic_type</span>

  <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Registers &quot;f&quot; as the statistics function for &quot;op_type&quot;.&quot;&quot;&quot;</span>
    <span class="n">_stats_registry</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op_type</span> <span class="o">+</span> <span class="s2">&quot;,&quot;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_statistic_type</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span>


<span class="k">def</span> <span class="nf">get_stats_for_node_def</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="n">statistic_type</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Looks up the node&#39;s statistics function in the registry and calls it.</span>

<span class="sd">  This function takes a Graph object and a NodeDef from a GraphDef, and if</span>
<span class="sd">  there&#39;s an associated statistics method, calls it and returns a result. If no</span>
<span class="sd">  function has been registered for the particular node type, it returns an empty</span>
<span class="sd">  statistics object.</span>

<span class="sd">  Args:</span>
<span class="sd">    graph: A Graph object that&#39;s been set up with the node&#39;s graph.</span>
<span class="sd">    node: A NodeDef describing the operator.</span>
<span class="sd">    statistic_type: A string identifying the statistic we&#39;re interested in.</span>
<span class="sd">  Returns:</span>
<span class="sd">    An OpStats object containing information about resource usage.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">try</span><span class="p">:</span>
    <span class="n">stats_func</span> <span class="o">=</span> <span class="n">_stats_registry</span><span class="o">.</span><span class="n">lookup</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">+</span> <span class="s2">&quot;,&quot;</span> <span class="o">+</span> <span class="n">statistic_type</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">stats_func</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">node</span><span class="p">)</span>
  <span class="k">except</span> <span class="ne">LookupError</span><span class="p">:</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">OpStats</span><span class="p">(</span><span class="n">statistic_type</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">result</span>


<span class="k">def</span> <span class="nf">_name_from_scope_name</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns the name of an op given the name of its scope.</span>

<span class="sd">  Args:</span>
<span class="sd">    name: the name of the scope.</span>

<span class="sd">  Returns:</span>
<span class="sd">    the name of the op (equal to scope name minus any trailing slash).</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">name</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="p">(</span><span class="n">name</span> <span class="ow">and</span> <span class="n">name</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;/&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="n">name</span>


<span class="n">_MUTATION_LOCK_GROUP</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">_SESSION_RUN_LOCK_GROUP</span> <span class="o">=</span> <span class="mi">1</span>

<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;Graph&quot;</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">Graph</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A TensorFlow computation, represented as a dataflow graph.</span>

<span class="sd">  A `Graph` contains a set of</span>
<span class="sd">  @{tf.Operation} objects,</span>
<span class="sd">  which represent units of computation; and</span>
<span class="sd">  @{tf.Tensor} objects, which represent</span>
<span class="sd">  the units of data that flow between operations.</span>

<span class="sd">  A default `Graph` is always registered, and accessible by calling</span>
<span class="sd">  @{tf.get_default_graph}.</span>
<span class="sd">  To add an operation to the default graph, simply call one of the functions</span>
<span class="sd">  that defines a new `Operation`:</span>

<span class="sd">  ```python</span>
<span class="sd">  c = tf.constant(4.0)</span>
<span class="sd">  assert c.graph is tf.get_default_graph()</span>
<span class="sd">  ```</span>

<span class="sd">  Another typical usage involves the</span>
<span class="sd">  @{tf.Graph.as_default}</span>
<span class="sd">  context manager, which overrides the current default graph for the</span>
<span class="sd">  lifetime of the context:</span>

<span class="sd">  ```python</span>
<span class="sd">  g = tf.Graph()</span>
<span class="sd">  with g.as_default():</span>
<span class="sd">    # Define operations and tensors in `g`.</span>
<span class="sd">    c = tf.constant(30.0)</span>
<span class="sd">    assert c.graph is g</span>
<span class="sd">  ```</span>

<span class="sd">  Important note: This class *is not* thread-safe for graph construction. All</span>
<span class="sd">  operations should be created from a single thread, or external</span>
<span class="sd">  synchronization must be provided. Unless otherwise specified, all methods</span>
<span class="sd">  are not thread-safe.</span>

<span class="sd">  A `Graph` instance supports an arbitrary number of &quot;collections&quot;</span>
<span class="sd">  that are identified by name. For convenience when building a large</span>
<span class="sd">  graph, collections can store groups of related objects: for</span>
<span class="sd">  example, the `tf.Variable` uses a collection (named</span>
<span class="sd">  @{tf.GraphKeys.GLOBAL_VARIABLES}) for</span>
<span class="sd">  all variables that are created during the construction of a graph. The caller</span>
<span class="sd">  may define additional collections by specifying a new name.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates a new, empty Graph.&quot;&quot;&quot;</span>
    <span class="c1"># Protects core state that can be returned via public accessors.</span>
    <span class="c1"># Thread-safety is provided on a best-effort basis to support buggy</span>
    <span class="c1"># programs, and is not guaranteed by the public `tf.Graph` API.</span>
    <span class="c1">#</span>
    <span class="c1"># NOTE(mrry): This does not protect the various stacks. A warning will</span>
    <span class="c1"># be reported if these are used from multiple threads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">RLock</span><span class="p">()</span>
    <span class="c1"># The group lock synchronizes Session.run calls with methods that create</span>
    <span class="c1"># and mutate ops (e.g. Graph.create_op()). This synchronization is</span>
    <span class="c1"># necessary because it&#39;s illegal to modify an operation after it&#39;s been run.</span>
    <span class="c1"># The group lock allows any number of threads to mutate ops at the same time</span>
    <span class="c1"># but if any modification is going on, all Session.run calls have to wait.</span>
    <span class="c1"># Similarly, if one or more Session.run calls are going on, all mutate ops</span>
    <span class="c1"># have to wait until all Session.run calls have finished.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_group_lock</span> <span class="o">=</span> <span class="n">lock_util</span><span class="o">.</span><span class="n">GroupLock</span><span class="p">(</span><span class="n">num_groups</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_nodes_by_id</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>  <span class="c1"># GUARDED_BY(self._lock)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_next_id_counter</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># GUARDED_BY(self._lock)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_nodes_by_name</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>  <span class="c1"># GUARDED_BY(self._lock)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_version</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># GUARDED_BY(self._lock)</span>
    <span class="c1"># Maps a name used in the graph to the next id to use for that name.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_names_in_use</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_stack_state_is_thread_local</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">local</span><span class="p">()</span>
    <span class="c1"># Functions that will be applied to choose a device if none is specified.</span>
    <span class="c1"># After switch_to_thread_local(), self._thread_local._device_function_stack</span>
    <span class="c1"># is used instead.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_graph_device_function_stack</span> <span class="o">=</span> <span class="n">traceable_stack</span><span class="o">.</span><span class="n">TraceableStack</span><span class="p">()</span>
    <span class="c1"># Default original_op applied to new ops.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_default_original_op</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># Current control flow context. It could be either CondContext or</span>
    <span class="c1"># WhileContext defined in ops/control_flow_ops.py</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_control_flow_context</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># A new node will depend of the union of all of the nodes in the stack.</span>
    <span class="c1"># After switch_to_thread_local(),</span>
    <span class="c1"># self._thread_local._control_dependencies_stack is used instead.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_graph_control_dependencies_stack</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># Arbitrary collections of objects.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_collections</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="c1"># The graph-level random seed</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_seed</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># A dictionary of attributes that should be applied to all ops.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_attr_scope_map</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="c1"># A map from op type to the kernel label that should be used.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_op_to_kernel_label_map</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="c1"># A map from op type to an alternative op type that should be used when</span>
    <span class="c1"># computing gradients.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_gradient_override_map</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="c1"># True if the graph is considered &quot;finalized&quot;.  In that case no</span>
    <span class="c1"># new operations can be added.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_finalized</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="c1"># Functions defined in the graph</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_functions</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">OrderedDict</span><span class="p">()</span>
    <span class="c1"># Default GraphDef versions</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_graph_def_versions</span> <span class="o">=</span> <span class="n">versions_pb2</span><span class="o">.</span><span class="n">VersionDef</span><span class="p">(</span>
        <span class="n">producer</span><span class="o">=</span><span class="n">versions</span><span class="o">.</span><span class="n">GRAPH_DEF_VERSION</span><span class="p">,</span>
        <span class="n">min_consumer</span><span class="o">=</span><span class="n">versions</span><span class="o">.</span><span class="n">GRAPH_DEF_VERSION_MIN_CONSUMER</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_building_function</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="c1"># Stack of colocate_with ops. After switch_to_thread_local(),</span>
    <span class="c1"># self._thread_local._colocation_stack is used instead.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_graph_colocation_stack</span> <span class="o">=</span> <span class="n">traceable_stack</span><span class="o">.</span><span class="n">TraceableStack</span><span class="p">()</span>
    <span class="c1"># Set of tensors that are dangerous to feed!</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_unfeedable_tensors</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="c1"># Set of operations that are dangerous to fetch!</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_unfetchable_ops</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="c1"># A map of tensor handle placeholder to tensor dtype.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_handle_feeders</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="c1"># A map from tensor handle to its read op.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_handle_readers</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="c1"># A map from tensor handle to its move op.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_handle_movers</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="c1"># A map from tensor handle to its delete op.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_handle_deleters</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="c1"># Allow optimizers and other objects to pseudo-uniquely key graphs (this key</span>
    <span class="c1"># will be shared when defining function graphs, for example, so optimizers</span>
    <span class="c1"># being called inside function definitions behave as if they were seeing the</span>
    <span class="c1"># actual outside graph).</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_graph_key</span> <span class="o">=</span> <span class="s2">&quot;grap-key-</span><span class="si">%d</span><span class="s2">/&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">uid</span><span class="p">(),)</span>
    <span class="c1"># A string with the last reduction method passed to</span>
    <span class="c1"># losses.compute_weighted_loss(), or None.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_last_loss_reduction</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_container</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_registered_ops</span> <span class="o">=</span> <span class="n">op_def_registry</span><span class="o">.</span><span class="n">get_registered_ops</span><span class="p">()</span>

    <span class="c1"># TODO(skyewm): fold as much of the above as possible into the C</span>
    <span class="c1"># implementation</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_c_api_hack</span><span class="p">():</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_scoped_c_graph</span> <span class="o">=</span> <span class="n">c_api_util</span><span class="o">.</span><span class="n">ScopedTFGraph</span><span class="p">()</span>
      <span class="c1"># The C API requires all ops to have shape functions. Disable this</span>
      <span class="c1"># requirement (many custom ops do not have shape functions, and we don&#39;t</span>
      <span class="c1"># want to break these existing cases).</span>
      <span class="n">c_api</span><span class="o">.</span><span class="n">SetRequireShapeInferenceFns</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_c_graph</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_scoped_c_graph</span> <span class="o">=</span> <span class="kc">None</span>

  <span class="c1"># TODO(apassos) remove once the C API is used by default.</span>
  <span class="k">def</span> <span class="nf">_use_c_api_hack</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Temporary hack; can be overridden to force C API usage.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_USE_C_API</span>

  <span class="c1"># Note: this method is private because the API of tf.Graph() is public and</span>
  <span class="c1"># frozen, and this functionality is still not ready for public visibility.</span>
  <span class="nd">@tf_contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
  <span class="k">def</span> <span class="nf">_variable_creator_scope</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">creator</span><span class="p">):</span>
    <span class="c1"># This step makes a copy of the existing stack, and it also initializes</span>
    <span class="c1"># self._thread_local._variable_creator_stack if it doesn&#39;t exist yet.</span>
    <span class="n">old</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_variable_creator_stack</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_variable_creator_stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">creator</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">yield</span>
    <span class="k">finally</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_variable_creator_stack</span> <span class="o">=</span> <span class="n">old</span>  <span class="c1"># pylint: disable=protected-access</span>

  <span class="c1"># Note: this method is private because the API of tf.Graph() is public and</span>
  <span class="c1"># frozen, and this functionality is still not ready for public visibility.</span>
  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_variable_creator_stack</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="p">,</span> <span class="s2">&quot;_variable_creator_stack&quot;</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_variable_creator_stack</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_variable_creator_stack</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>

  <span class="nd">@_variable_creator_stack</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">_variable_creator_stack</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">variable_creator_stack</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_variable_creator_stack</span> <span class="o">=</span> <span class="n">variable_creator_stack</span>  <span class="c1"># pylint: disable=protected-access</span>

  <span class="k">def</span> <span class="nf">_check_not_finalized</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Check if the graph is finalized.</span>

<span class="sd">    Raises:</span>
<span class="sd">      RuntimeError: If the graph finalized.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_finalized</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Graph is finalized and cannot be modified.&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_add_op</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Adds &#39;op&#39; to the graph.</span>

<span class="sd">    Args:</span>
<span class="sd">      op: the Operator or Tensor to add.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if op is not an Operation or Tensor.</span>
<span class="sd">      ValueError: if the op.name or op._id are already used.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_check_not_finalized</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Operation</span><span class="p">)):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;op must be a Tensor or Operation: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">op</span><span class="p">)</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span><span class="p">:</span>
      <span class="c1"># pylint: disable=protected-access</span>
      <span class="k">if</span> <span class="n">op</span><span class="o">.</span><span class="n">_id</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_nodes_by_id</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;cannot add an op with id </span><span class="si">%d</span><span class="s2"> as it already &quot;</span>
                         <span class="s2">&quot;exists in the graph&quot;</span> <span class="o">%</span> <span class="n">op</span><span class="o">.</span><span class="n">_id</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">op</span><span class="o">.</span><span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_nodes_by_name</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;cannot add op with name </span><span class="si">%s</span><span class="s2"> as that name &quot;</span>
                         <span class="s2">&quot;is already used&quot;</span> <span class="o">%</span> <span class="n">op</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_nodes_by_id</span><span class="p">[</span><span class="n">op</span><span class="o">.</span><span class="n">_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">op</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_nodes_by_name</span><span class="p">[</span><span class="n">op</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">op</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_version</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_version</span><span class="p">,</span> <span class="n">op</span><span class="o">.</span><span class="n">_id</span><span class="p">)</span>
      <span class="c1"># pylint: enable=protected-access</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_c_graph</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scoped_c_graph</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scoped_c_graph</span><span class="o">.</span><span class="n">graph</span>
    <span class="k">return</span> <span class="kc">None</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">version</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a version number that increases as ops are added to the graph.</span>

<span class="sd">    Note that this is unrelated to the</span>
<span class="sd">    @{tf.Graph.graph_def_versions}.</span>

<span class="sd">    Returns:</span>
<span class="sd">       An integer version that increases as ops are added to the graph.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_finalized</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_version</span>

    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_version</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">graph_def_versions</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># pylint: disable=line-too-long</span>
    <span class="sd">&quot;&quot;&quot;The GraphDef version information of this graph.</span>

<span class="sd">    For details on the meaning of each version, see</span>
<span class="sd">    [`GraphDef`](https://www.tensorflow.org/code/tensorflow/core/framework/graph.proto).</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `VersionDef`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># pylint: enable=line-too-long</span>
    <span class="k">with</span> <span class="n">c_api_util</span><span class="o">.</span><span class="n">tf_buffer</span><span class="p">()</span> <span class="k">as</span> <span class="n">buf</span><span class="p">:</span>
      <span class="n">c_api</span><span class="o">.</span><span class="n">TF_GraphVersions</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_c_graph</span><span class="p">,</span> <span class="n">buf</span><span class="p">)</span>
      <span class="n">data</span> <span class="o">=</span> <span class="n">c_api</span><span class="o">.</span><span class="n">TF_GetBuffer</span><span class="p">(</span><span class="n">buf</span><span class="p">)</span>
    <span class="n">version_def</span> <span class="o">=</span> <span class="n">versions_pb2</span><span class="o">.</span><span class="n">VersionDef</span><span class="p">()</span>
    <span class="n">version_def</span><span class="o">.</span><span class="n">ParseFromString</span><span class="p">(</span><span class="n">compat</span><span class="o">.</span><span class="n">as_bytes</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">version_def</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">seed</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The graph-level random seed of this graph.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_seed</span>

  <span class="nd">@seed</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">seed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seed</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_seed</span> <span class="o">=</span> <span class="n">seed</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">finalized</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;True if this graph has been finalized.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_finalized</span>

  <span class="k">def</span> <span class="nf">finalize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Finalizes this graph, making it read-only.</span>

<span class="sd">    After calling `g.finalize()`, no new operations can be added to</span>
<span class="sd">    `g`.  This method is used to ensure that no operations are added</span>
<span class="sd">    to a graph when it is shared between multiple threads, for example</span>
<span class="sd">    when using a @{tf.train.QueueRunner}.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_finalized</span> <span class="o">=</span> <span class="kc">True</span>

  <span class="k">def</span> <span class="nf">_unsafe_unfinalize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Opposite of `finalize`. Internal interface.</span>

<span class="sd">    NOTE: Unfinalizing a graph could have negative impact on performance,</span>
<span class="sd">    especially in a multi-threaded environment.  Unfinalizing a graph</span>
<span class="sd">    when it is in use by a Session may lead to undefined behavior. Ensure</span>
<span class="sd">    that all sessions using a graph are closed before calling this method.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_finalized</span> <span class="o">=</span> <span class="kc">False</span>

  <span class="k">def</span> <span class="nf">_get_control_flow_context</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the current control flow context.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A context object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_control_flow_context</span>

  <span class="k">def</span> <span class="nf">_set_control_flow_context</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Sets the current control flow context.</span>

<span class="sd">    Args:</span>
<span class="sd">      ctx: a context object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_control_flow_context</span> <span class="o">=</span> <span class="n">ctx</span>

  <span class="k">def</span> <span class="nf">_copy_functions_to_graph_def</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">graph_def</span><span class="p">,</span> <span class="n">starting_bytesize</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;If this graph contains functions, copy them to `graph_def`.&quot;&quot;&quot;</span>
    <span class="n">bytesize</span> <span class="o">=</span> <span class="n">starting_bytesize</span>
    <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_functions</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
      <span class="n">bytesize</span> <span class="o">+=</span> <span class="n">f</span><span class="o">.</span><span class="n">definition</span><span class="o">.</span><span class="n">ByteSize</span><span class="p">()</span>
      <span class="k">if</span> <span class="n">bytesize</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">31</span><span class="p">)</span> <span class="ow">or</span> <span class="n">bytesize</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;GraphDef cannot be larger than 2GB.&quot;</span><span class="p">)</span>
      <span class="n">graph_def</span><span class="o">.</span><span class="n">library</span><span class="o">.</span><span class="n">function</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">f</span><span class="o">.</span><span class="n">definition</span><span class="p">])</span>
      <span class="k">if</span> <span class="n">f</span><span class="o">.</span><span class="n">grad_func_name</span><span class="p">:</span>
        <span class="n">grad_def</span> <span class="o">=</span> <span class="n">function_pb2</span><span class="o">.</span><span class="n">GradientDef</span><span class="p">()</span>
        <span class="n">grad_def</span><span class="o">.</span><span class="n">function_name</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">name</span>
        <span class="n">grad_def</span><span class="o">.</span><span class="n">gradient_func</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">grad_func_name</span>
        <span class="n">graph_def</span><span class="o">.</span><span class="n">library</span><span class="o">.</span><span class="n">gradient</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">grad_def</span><span class="p">])</span>

  <span class="k">def</span> <span class="nf">_as_graph_def</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">from_version</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">add_shapes</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># pylint: disable=line-too-long</span>
    <span class="sd">&quot;&quot;&quot;Returns a serialized `GraphDef` representation of this graph.</span>

<span class="sd">    The serialized `GraphDef` can be imported into another `Graph`</span>
<span class="sd">    (using @{tf.import_graph_def}) or used with the</span>
<span class="sd">    [C++ Session API](../../../../api_docs/cc/index.md).</span>

<span class="sd">    This method is thread-safe.</span>

<span class="sd">    Args:</span>
<span class="sd">      from_version: Optional.  If this is set, returns a `GraphDef`</span>
<span class="sd">        containing only the nodes that were added to this graph since</span>
<span class="sd">        its `version` property had the given value.</span>
<span class="sd">      add_shapes: If true, adds an &quot;_output_shapes&quot; list attr to each</span>
<span class="sd">        node with the inferred shapes of each of its outputs.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tuple containing a</span>
<span class="sd">      [`GraphDef`](https://www.tensorflow.org/code/tensorflow/core/framework/graph.proto)</span>
<span class="sd">      protocol buffer, and the version of the graph to which that</span>
<span class="sd">      `GraphDef` corresponds.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If the `graph_def` would be too large.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># pylint: enable=line-too-long</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span><span class="p">:</span>
      <span class="k">with</span> <span class="n">c_api_util</span><span class="o">.</span><span class="n">tf_buffer</span><span class="p">()</span> <span class="k">as</span> <span class="n">buf</span><span class="p">:</span>
        <span class="n">c_api</span><span class="o">.</span><span class="n">TF_GraphToGraphDef</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_c_graph</span><span class="p">,</span> <span class="n">buf</span><span class="p">)</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">c_api</span><span class="o">.</span><span class="n">TF_GetBuffer</span><span class="p">(</span><span class="n">buf</span><span class="p">)</span>
      <span class="n">graph</span> <span class="o">=</span> <span class="n">graph_pb2</span><span class="o">.</span><span class="n">GraphDef</span><span class="p">()</span>
      <span class="n">graph</span><span class="o">.</span><span class="n">ParseFromString</span><span class="p">(</span><span class="n">compat</span><span class="o">.</span><span class="n">as_bytes</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
      <span class="c1"># Strip the experimental library field iff it&#39;s empty.</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">graph</span><span class="o">.</span><span class="n">library</span><span class="o">.</span><span class="n">function</span><span class="p">:</span>
        <span class="n">graph</span><span class="o">.</span><span class="n">ClearField</span><span class="p">(</span><span class="s2">&quot;library&quot;</span><span class="p">)</span>

      <span class="k">if</span> <span class="n">add_shapes</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">:</span>
          <span class="n">op</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_nodes_by_name</span><span class="p">[</span><span class="n">node</span><span class="o">.</span><span class="n">name</span><span class="p">]</span>
          <span class="k">if</span> <span class="n">op</span><span class="o">.</span><span class="n">outputs</span><span class="p">:</span>
            <span class="n">node</span><span class="o">.</span><span class="n">attr</span><span class="p">[</span><span class="s2">&quot;_output_shapes&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">list</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span>
                <span class="p">[</span><span class="n">output</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_proto</span><span class="p">()</span> <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">op</span><span class="o">.</span><span class="n">outputs</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">graph</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_version</span>

  <span class="k">def</span> <span class="nf">as_graph_def</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">from_version</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">add_shapes</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># pylint: disable=line-too-long</span>
    <span class="sd">&quot;&quot;&quot;Returns a serialized `GraphDef` representation of this graph.</span>

<span class="sd">    The serialized `GraphDef` can be imported into another `Graph`</span>
<span class="sd">    (using @{tf.import_graph_def}) or used with the</span>
<span class="sd">    [C++ Session API](../../api_docs/cc/index.md).</span>

<span class="sd">    This method is thread-safe.</span>

<span class="sd">    Args:</span>
<span class="sd">      from_version: Optional.  If this is set, returns a `GraphDef`</span>
<span class="sd">        containing only the nodes that were added to this graph since</span>
<span class="sd">        its `version` property had the given value.</span>
<span class="sd">      add_shapes: If true, adds an &quot;_output_shapes&quot; list attr to each</span>
<span class="sd">        node with the inferred shapes of each of its outputs.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A</span>
<span class="sd">      [`GraphDef`](https://www.tensorflow.org/code/tensorflow/core/framework/graph.proto)</span>
<span class="sd">      protocol buffer.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If the `graph_def` would be too large.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># pylint: enable=line-too-long</span>
    <span class="n">result</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_as_graph_def</span><span class="p">(</span><span class="n">from_version</span><span class="p">,</span> <span class="n">add_shapes</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>

  <span class="k">def</span> <span class="nf">_is_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Tests whether &#39;name&#39; is registered in this graph&#39;s function library.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: string op name.</span>
<span class="sd">    Returns:</span>
<span class="sd">      bool indicating whether or not &#39;name&#39; is registered in function library.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_functions</span>

  <span class="k">def</span> <span class="nf">_get_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the function definition for &#39;name&#39;.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: string function name.</span>
<span class="sd">    Returns:</span>
<span class="sd">      The function def proto.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_functions</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_add_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">function</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Adds a function to the graph.</span>

<span class="sd">    After the function has been added, you can call to the function by</span>
<span class="sd">    passing the function name in place of an op name to</span>
<span class="sd">    `Graph.create_op()`.</span>

<span class="sd">    Args:</span>
<span class="sd">      function: A `_DefinedFunction` object.</span>


<span class="sd">    Raises:</span>
<span class="sd">      ValueError: if another function is defined with the same name.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">function</span><span class="o">.</span><span class="n">name</span>
    <span class="c1"># Sanity checks on gradient definition.</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">function</span><span class="o">.</span><span class="n">grad_func_name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">function</span><span class="o">.</span><span class="n">python_grad_func</span> <span class="ow">is</span>
                                                  <span class="ow">not</span> <span class="kc">None</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Gradient defined twice for function </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">name</span><span class="p">)</span>

    <span class="c1"># Add function to graph</span>
    <span class="c1"># pylint: disable=protected-access</span>
    <span class="c1"># Handle functions created without using the C API. TODO(apassos,skyewm)</span>
    <span class="c1"># remove this when all functions are generated using the C API by default</span>
    <span class="c1"># as this will be unnecessary.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">function</span><span class="o">.</span><span class="n">_c_func</span><span class="p">:</span>
      <span class="n">serialized</span> <span class="o">=</span> <span class="n">function</span><span class="o">.</span><span class="n">definition</span><span class="o">.</span><span class="n">SerializeToString</span><span class="p">()</span>
      <span class="n">c_func</span> <span class="o">=</span> <span class="n">c_api</span><span class="o">.</span><span class="n">TF_FunctionImportFunctionDef</span><span class="p">(</span><span class="n">serialized</span><span class="p">)</span>
      <span class="n">function</span><span class="o">.</span><span class="n">_c_func</span> <span class="o">=</span> <span class="n">c_api_util</span><span class="o">.</span><span class="n">ScopedTFFunction</span><span class="p">(</span><span class="n">c_func</span><span class="p">)</span>
    <span class="n">gradient</span> <span class="o">=</span> <span class="p">(</span><span class="n">function</span><span class="o">.</span><span class="n">_grad_func</span><span class="o">.</span><span class="n">_c_func</span><span class="o">.</span><span class="n">func</span> <span class="k">if</span> <span class="n">function</span><span class="o">.</span><span class="n">_grad_func</span>
                <span class="k">else</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">c_api</span><span class="o">.</span><span class="n">TF_GraphCopyFunction</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_c_graph</span><span class="p">,</span> <span class="n">function</span><span class="o">.</span><span class="n">_c_func</span><span class="o">.</span><span class="n">func</span><span class="p">,</span> <span class="n">gradient</span><span class="p">)</span>
    <span class="c1"># pylint: enable=protected-access</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_functions</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">function</span>

    <span class="c1"># Need a new-enough consumer to support the functions we add to the graph.</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph_def_versions</span><span class="o">.</span><span class="n">min_consumer</span> <span class="o">&lt;</span> <span class="mi">12</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_graph_def_versions</span><span class="o">.</span><span class="n">min_consumer</span> <span class="o">=</span> <span class="mi">12</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">building_function</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns True iff this graph represents a function.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_building_function</span>

  <span class="c1"># Helper functions to create operations.</span>
  <span class="nd">@deprecated_args</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span>
                   <span class="s2">&quot;Shapes are always computed; don&#39;t use the compute_shapes &quot;</span>
                   <span class="s2">&quot;as it has no effect.&quot;</span><span class="p">,</span> <span class="s2">&quot;compute_shapes&quot;</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">create_op</span><span class="p">(</span>
      <span class="bp">self</span><span class="p">,</span>
      <span class="n">op_type</span><span class="p">,</span>
      <span class="n">inputs</span><span class="p">,</span>
      <span class="n">dtypes</span><span class="p">,</span>  <span class="c1"># pylint: disable=redefined-outer-name</span>
      <span class="n">input_types</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">attrs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">op_def</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">compute_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
      <span class="n">compute_device</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates an `Operation` in this graph.</span>

<span class="sd">    This is a low-level interface for creating an `Operation`. Most</span>
<span class="sd">    programs will not call this method directly, and instead use the</span>
<span class="sd">    Python op constructors, such as `tf.constant()`, which add ops to</span>
<span class="sd">    the default graph.</span>

<span class="sd">    Args:</span>
<span class="sd">      op_type: The `Operation` type to create. This corresponds to the</span>
<span class="sd">        `OpDef.name` field for the proto that defines the operation.</span>
<span class="sd">      inputs: A list of `Tensor` objects that will be inputs to the `Operation`.</span>
<span class="sd">      dtypes: A list of `DType` objects that will be the types of the tensors</span>
<span class="sd">        that the operation produces.</span>
<span class="sd">      input_types: (Optional.) A list of `DType`s that will be the types of</span>
<span class="sd">        the tensors that the operation consumes. By default, uses the base</span>
<span class="sd">        `DType` of each input in `inputs`. Operations that expect</span>
<span class="sd">        reference-typed inputs must specify `input_types` explicitly.</span>
<span class="sd">      name: (Optional.) A string name for the operation. If not specified, a</span>
<span class="sd">        name is generated based on `op_type`.</span>
<span class="sd">      attrs: (Optional.) A dictionary where the key is the attribute name (a</span>
<span class="sd">        string) and the value is the respective `attr` attribute of the</span>
<span class="sd">        `NodeDef` proto that will represent the operation (an `AttrValue`</span>
<span class="sd">        proto).</span>
<span class="sd">      op_def: (Optional.) The `OpDef` proto that describes the `op_type` that</span>
<span class="sd">        the operation will have.</span>
<span class="sd">      compute_shapes: (Optional.) Deprecated. Has no effect (shapes are always</span>
<span class="sd">        computed).</span>
<span class="sd">      compute_device: (Optional.) If True, device functions will be executed</span>
<span class="sd">        to compute the device property of the Operation.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if any of the inputs is not a `Tensor`.</span>
<span class="sd">      ValueError: if colocation conflicts with existing device assignment.</span>

<span class="sd">    Returns:</span>
<span class="sd">      An `Operation` object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">del</span> <span class="n">compute_shapes</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_check_not_finalized</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Input #</span><span class="si">%d</span><span class="s2"> is not a tensor: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">a</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">name</span> <span class="o">=</span> <span class="n">op_type</span>
    <span class="c1"># If a names ends with a &#39;/&#39; it is a &quot;name scope&quot; and we use it as-is,</span>
    <span class="c1"># after removing the trailing &#39;/&#39;.</span>
    <span class="k">if</span> <span class="n">name</span> <span class="ow">and</span> <span class="n">name</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;/&quot;</span><span class="p">:</span>
      <span class="n">name</span> <span class="o">=</span> <span class="n">_name_from_scope_name</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">unique_name</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>

    <span class="n">node_def</span> <span class="o">=</span> <span class="n">_NodeDef</span><span class="p">(</span><span class="n">op_type</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attrs</span><span class="o">=</span><span class="n">attrs</span><span class="p">)</span>

    <span class="n">input_ops</span> <span class="o">=</span> <span class="nb">set</span><span class="p">([</span><span class="n">t</span><span class="o">.</span><span class="n">op</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">])</span>
    <span class="n">control_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_control_dependencies_for_inputs</span><span class="p">(</span><span class="n">input_ops</span><span class="p">)</span>
    <span class="c1"># _create_op_helper mutates the new Operation. `_mutation_lock` ensures a</span>
    <span class="c1"># Session.run call cannot occur between creating and mutating the op.</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mutation_lock</span><span class="p">():</span>
      <span class="n">ret</span> <span class="o">=</span> <span class="n">Operation</span><span class="p">(</span>
          <span class="n">node_def</span><span class="p">,</span>
          <span class="bp">self</span><span class="p">,</span>
          <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
          <span class="n">output_types</span><span class="o">=</span><span class="n">dtypes</span><span class="p">,</span>
          <span class="n">control_inputs</span><span class="o">=</span><span class="n">control_inputs</span><span class="p">,</span>
          <span class="n">input_types</span><span class="o">=</span><span class="n">input_types</span><span class="p">,</span>
          <span class="n">original_op</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_default_original_op</span><span class="p">,</span>
          <span class="n">op_def</span><span class="o">=</span><span class="n">op_def</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_create_op_helper</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="n">compute_device</span><span class="o">=</span><span class="n">compute_device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span>

  <span class="k">def</span> <span class="nf">_create_op_from_tf_operation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">c_op</span><span class="p">,</span> <span class="n">compute_device</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates an `Operation` in this graph from the supplied TF_Operation.</span>

<span class="sd">    This method is like create_op() except the new Operation is constructed</span>
<span class="sd">    using `c_op`. The returned Operation will have `c_op` as its _c_op</span>
<span class="sd">    field. This is used to create Operation objects around TF_Operations created</span>
<span class="sd">    indirectly by the C API (e.g. by TF_ImportGraphDef, TF_FinishWhile).</span>

<span class="sd">    This function does not call Operation._control_flow_post_processing or</span>
<span class="sd">    Graph._control_dependencies_for_inputs (since the inputs may not be</span>
<span class="sd">    available yet). The caller is responsible for calling these methods.</span>

<span class="sd">    Args:</span>
<span class="sd">      c_op: a wrapped TF_Operation</span>
<span class="sd">      compute_device: (Optional.) If True, device functions will be executed</span>
<span class="sd">        to compute the device property of the Operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      An `Operation` object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_check_not_finalized</span><span class="p">()</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">Operation</span><span class="p">(</span><span class="n">c_op</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
    <span class="c1"># If a name_scope was created with ret.name but no nodes were created in it,</span>
    <span class="c1"># the name will still appear in _names_in_use even though the name hasn&#39;t</span>
    <span class="c1"># been used. This is ok, just leave _names_in_use as-is in this case.</span>
    <span class="c1"># TODO(skyewm): make the C API guarantee no name conflicts.</span>
    <span class="n">name_key</span> <span class="o">=</span> <span class="n">ret</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">name_key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_names_in_use</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_names_in_use</span><span class="p">[</span><span class="n">name_key</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_create_op_helper</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="n">compute_device</span><span class="o">=</span><span class="n">compute_device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span>

  <span class="k">def</span> <span class="nf">_create_op_helper</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="n">compute_device</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Common logic for creating an op in this graph.&quot;&quot;&quot;</span>
    <span class="c1"># Apply any additional attributes requested. Do not overwrite any existing</span>
    <span class="c1"># attributes.</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attr_scope_map</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="n">op</span><span class="o">.</span><span class="n">get_attr</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
      <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
          <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">node_def</span><span class="p">)</span>
          <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="kc">None</span><span class="p">),</span> <span class="n">attr_value_pb2</span><span class="o">.</span><span class="n">AttrValue</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="s2">&quot;Callable for scope map key &#39;</span><span class="si">%s</span><span class="s2">&#39; must return either None or &quot;</span>
                <span class="s2">&quot;an AttrValue protocol buffer; but it returned: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">key</span><span class="p">,</span>
                                                                       <span class="n">value</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">value</span><span class="p">:</span>
          <span class="n">op</span><span class="o">.</span><span class="n">_set_attr</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>

    <span class="c1"># Apply a kernel label if one has been specified for this op type.</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">kernel_label</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op_to_kernel_label_map</span><span class="p">[</span><span class="n">op</span><span class="o">.</span><span class="n">type</span><span class="p">]</span>
      <span class="n">op</span><span class="o">.</span><span class="n">_set_attr</span><span class="p">(</span><span class="s2">&quot;_kernel&quot;</span><span class="p">,</span>  <span class="c1"># pylint: disable=protected-access</span>
                   <span class="n">attr_value_pb2</span><span class="o">.</span><span class="n">AttrValue</span><span class="p">(</span><span class="n">s</span><span class="o">=</span><span class="n">compat</span><span class="o">.</span><span class="n">as_bytes</span><span class="p">(</span><span class="n">kernel_label</span><span class="p">)))</span>
    <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
      <span class="k">pass</span>

    <span class="c1"># Apply the overriding op type for gradients if one has been specified for</span>
    <span class="c1"># this op type.</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">mapped_op_type</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gradient_override_map</span><span class="p">[</span><span class="n">op</span><span class="o">.</span><span class="n">type</span><span class="p">]</span>
      <span class="n">op</span><span class="o">.</span><span class="n">_set_attr</span><span class="p">(</span><span class="s2">&quot;_gradient_op_type&quot;</span><span class="p">,</span>  <span class="c1"># pylint: disable=protected-access</span>
                   <span class="n">attr_value_pb2</span><span class="o">.</span><span class="n">AttrValue</span><span class="p">(</span><span class="n">s</span><span class="o">=</span><span class="n">compat</span><span class="o">.</span><span class="n">as_bytes</span><span class="p">(</span><span class="n">mapped_op_type</span><span class="p">)))</span>
    <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
      <span class="k">pass</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_record_op_seen_by_control_dependencies</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">compute_device</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_apply_device_functions</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_colocation_stack</span><span class="p">:</span>
      <span class="n">all_colocation_groups</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="k">for</span> <span class="n">colocation_op</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_colocation_stack</span><span class="o">.</span><span class="n">peek_objs</span><span class="p">():</span>
        <span class="n">all_colocation_groups</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">colocation_op</span><span class="o">.</span><span class="n">colocation_groups</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">colocation_op</span><span class="o">.</span><span class="n">device</span><span class="p">:</span>
          <span class="c1"># Make this device match the device of the colocated op, to provide</span>
          <span class="c1"># consistency between the device and the colocation property.</span>
          <span class="k">if</span> <span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">device</span> <span class="ow">and</span> <span class="n">pydev</span><span class="o">.</span><span class="n">canonical_name</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">!=</span>
              <span class="n">pydev</span><span class="o">.</span><span class="n">canonical_name</span><span class="p">(</span><span class="n">colocation_op</span><span class="o">.</span><span class="n">device</span><span class="p">)):</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Tried to colocate </span><span class="si">%s</span><span class="s2"> with an op </span><span class="si">%s</span><span class="s2"> that had &quot;</span>
                            <span class="s2">&quot;a different device: </span><span class="si">%s</span><span class="s2"> vs </span><span class="si">%s</span><span class="s2">. Postponing &quot;</span>
                            <span class="s2">&quot;error-checking until all devices are assigned.&quot;</span><span class="p">,</span>
                            <span class="n">op</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">colocation_op</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">op</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                            <span class="n">colocation_op</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
          <span class="k">else</span><span class="p">:</span>
            <span class="n">op</span><span class="o">.</span><span class="n">_set_device</span><span class="p">(</span><span class="n">colocation_op</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>

      <span class="n">all_colocation_groups</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">all_colocation_groups</span><span class="p">))</span>
      <span class="c1"># pylint: disable=protected-access</span>
      <span class="n">op</span><span class="o">.</span><span class="n">_set_attr</span><span class="p">(</span><span class="s2">&quot;_class&quot;</span><span class="p">,</span> <span class="n">attr_value_pb2</span><span class="o">.</span><span class="n">AttrValue</span><span class="p">(</span>
          <span class="nb">list</span><span class="o">=</span><span class="n">attr_value_pb2</span><span class="o">.</span><span class="n">AttrValue</span><span class="o">.</span><span class="n">ListValue</span><span class="p">(</span><span class="n">s</span><span class="o">=</span><span class="n">all_colocation_groups</span><span class="p">)))</span>
      <span class="n">op</span><span class="o">.</span><span class="n">_colocation_code_locations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_snapshot_colocation_stack_metadata</span><span class="p">()</span>
      <span class="c1"># pylint: enable=protected-access</span>

    <span class="c1"># Sets &quot;container&quot; attribute if</span>
    <span class="c1"># (1) self._container is not None</span>
    <span class="c1"># (2) &quot;is_stateful&quot; is set in OpDef</span>
    <span class="c1"># (3) &quot;container&quot; attribute is in OpDef</span>
    <span class="c1"># (4) &quot;container&quot; attribute is None</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_container</span> <span class="ow">and</span> <span class="n">op</span><span class="o">.</span><span class="n">op_def</span><span class="o">.</span><span class="n">is_stateful</span><span class="p">:</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="n">container_attr</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">get_attr</span><span class="p">(</span><span class="s2">&quot;container&quot;</span><span class="p">)</span>
      <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
        <span class="c1"># &quot;container&quot; attribute is not in OpDef</span>
        <span class="k">pass</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">container_attr</span><span class="p">:</span>
          <span class="n">op</span><span class="o">.</span><span class="n">_set_attr</span><span class="p">(</span><span class="s2">&quot;container&quot;</span><span class="p">,</span> <span class="n">attr_value_pb2</span><span class="o">.</span><span class="n">AttrValue</span><span class="p">(</span>  <span class="c1"># pylint: disable=protected-access</span>
              <span class="n">s</span><span class="o">=</span><span class="n">compat</span><span class="o">.</span><span class="n">as_bytes</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_container</span><span class="p">)))</span>

  <span class="k">def</span> <span class="nf">_add_new_tf_operations</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">compute_devices</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates `Operations` in this graph for any new TF_Operations.</span>

<span class="sd">    This is useful for when TF_Operations are indirectly created by the C API</span>
<span class="sd">    outside of the Operation constructor (e.g. by TF_ImportGraphDef,</span>
<span class="sd">    TF_FinishWhile). This ensures there are corresponding Operations for all</span>
<span class="sd">    TF_Operations in the underlying TF_Graph.</span>

<span class="sd">    Args:</span>
<span class="sd">      compute_devices: (Optional.) If True, device functions will be executed</span>
<span class="sd">        to compute the device properties of each new Operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A list of the new `Operation` objects.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Create all Operation objects before accessing their inputs since an op may</span>
    <span class="c1"># be created before its inputs.</span>
    <span class="n">new_ops</span> <span class="o">=</span> <span class="p">[</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_create_op_from_tf_operation</span><span class="p">(</span><span class="n">c_op</span><span class="p">,</span> <span class="n">compute_device</span><span class="o">=</span><span class="n">compute_devices</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">c_op</span> <span class="ow">in</span> <span class="n">c_api_util</span><span class="o">.</span><span class="n">new_tf_operations</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="p">]</span>

    <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">for</span> <span class="n">op</span> <span class="ow">in</span> <span class="n">new_ops</span><span class="p">:</span>
      <span class="c1"># Operations created by the C API always retrieve shapes from the C API so</span>
      <span class="c1"># we preserve the shapes of ops created in import_graph_def (from the</span>
      <span class="c1"># &quot;_output_shapes&quot; attr of the imported NodeDef).</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">_USE_C_SHAPES</span><span class="p">:</span>
        <span class="n">_set_shape_and_handle_data_for_outputs_c_api</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>
      <span class="n">new_control_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_control_dependencies_for_inputs</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">)</span>
      <span class="n">op</span><span class="o">.</span><span class="n">_add_control_inputs</span><span class="p">(</span><span class="n">new_control_inputs</span><span class="p">)</span>
      <span class="n">op</span><span class="o">.</span><span class="n">_control_flow_post_processing</span><span class="p">()</span>
    <span class="c1"># pylint: enable=protected-access</span>

    <span class="k">return</span> <span class="n">new_ops</span>

  <span class="k">def</span> <span class="nf">as_graph_element</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obj</span><span class="p">,</span> <span class="n">allow_tensor</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">allow_operation</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the object referred to by `obj`, as an `Operation` or `Tensor`.</span>

<span class="sd">    This function validates that `obj` represents an element of this</span>
<span class="sd">    graph, and gives an informative error message if it is not.</span>

<span class="sd">    This function is the canonical way to get/validate an object of</span>
<span class="sd">    one of the allowed types from an external argument reference in the</span>
<span class="sd">    Session API.</span>

<span class="sd">    This method may be called concurrently from multiple threads.</span>

<span class="sd">    Args:</span>
<span class="sd">      obj: A `Tensor`, an `Operation`, or the name of a tensor or operation.</span>
<span class="sd">        Can also be any object with an `_as_graph_element()` method that returns</span>
<span class="sd">        a value of one of these types.</span>
<span class="sd">      allow_tensor: If true, `obj` may refer to a `Tensor`.</span>
<span class="sd">      allow_operation: If true, `obj` may refer to an `Operation`.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The `Tensor` or `Operation` in the Graph corresponding to `obj`.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: If `obj` is not a type we support attempting to convert</span>
<span class="sd">        to types.</span>
<span class="sd">      ValueError: If `obj` is of an appropriate type but invalid. For</span>
<span class="sd">        example, an invalid string.</span>
<span class="sd">      KeyError: If `obj` is not an object in the graph.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_finalized</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_as_graph_element_locked</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">allow_tensor</span><span class="p">,</span> <span class="n">allow_operation</span><span class="p">)</span>

    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_as_graph_element_locked</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">allow_tensor</span><span class="p">,</span> <span class="n">allow_operation</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_as_graph_element_locked</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obj</span><span class="p">,</span> <span class="n">allow_tensor</span><span class="p">,</span> <span class="n">allow_operation</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;See `Graph.as_graph_element()` for details.&quot;&quot;&quot;</span>
    <span class="c1"># The vast majority of this function is figuring</span>
    <span class="c1"># out what an API user might be doing wrong, so</span>
    <span class="c1"># that we can give helpful error messages.</span>
    <span class="c1">#</span>
    <span class="c1"># Ideally, it would be nice to split it up, but we</span>
    <span class="c1"># need context to generate nice error messages.</span>

    <span class="k">if</span> <span class="n">allow_tensor</span> <span class="ow">and</span> <span class="n">allow_operation</span><span class="p">:</span>
      <span class="n">types_str</span> <span class="o">=</span> <span class="s2">&quot;Tensor or Operation&quot;</span>
    <span class="k">elif</span> <span class="n">allow_tensor</span><span class="p">:</span>
      <span class="n">types_str</span> <span class="o">=</span> <span class="s2">&quot;Tensor&quot;</span>
    <span class="k">elif</span> <span class="n">allow_operation</span><span class="p">:</span>
      <span class="n">types_str</span> <span class="o">=</span> <span class="s2">&quot;Operation&quot;</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;allow_tensor and allow_operation can&#39;t both be False.&quot;</span><span class="p">)</span>

    <span class="n">temp_obj</span> <span class="o">=</span> <span class="n">_as_graph_element</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">temp_obj</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">obj</span> <span class="o">=</span> <span class="n">temp_obj</span>

    <span class="c1"># If obj appears to be a name...</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">compat</span><span class="o">.</span><span class="n">bytes_or_text_types</span><span class="p">):</span>
      <span class="n">name</span> <span class="o">=</span> <span class="n">compat</span><span class="o">.</span><span class="n">as_str</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span>

      <span class="k">if</span> <span class="s2">&quot;:&quot;</span> <span class="ow">in</span> <span class="n">name</span> <span class="ow">and</span> <span class="n">allow_tensor</span><span class="p">:</span>
        <span class="c1"># Looks like a Tensor name and can be a Tensor.</span>
        <span class="k">try</span><span class="p">:</span>
          <span class="n">op_name</span><span class="p">,</span> <span class="n">out_n</span> <span class="o">=</span> <span class="n">name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;:&quot;</span><span class="p">)</span>
          <span class="n">out_n</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">out_n</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
          <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The name </span><span class="si">%s</span><span class="s2"> looks a like a Tensor name, but is &quot;</span>
                           <span class="s2">&quot;not a valid one. Tensor names must be of the &quot;</span>
                           <span class="s2">&quot;form </span><span class="se">\&quot;</span><span class="s2">&lt;op_name&gt;:&lt;output_index&gt;</span><span class="se">\&quot;</span><span class="s2">.&quot;</span> <span class="o">%</span> <span class="nb">repr</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">op_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_nodes_by_name</span><span class="p">:</span>
          <span class="n">op</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_nodes_by_name</span><span class="p">[</span><span class="n">op_name</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;The name </span><span class="si">%s</span><span class="s2"> refers to a Tensor which does not &quot;</span>
                         <span class="s2">&quot;exist. The operation, </span><span class="si">%s</span><span class="s2">, does not exist in the &quot;</span>
                         <span class="s2">&quot;graph.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">repr</span><span class="p">(</span><span class="n">name</span><span class="p">),</span> <span class="nb">repr</span><span class="p">(</span><span class="n">op_name</span><span class="p">)))</span>
        <span class="k">try</span><span class="p">:</span>
          <span class="k">return</span> <span class="n">op</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="n">out_n</span><span class="p">]</span>
        <span class="k">except</span><span class="p">:</span>
          <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;The name </span><span class="si">%s</span><span class="s2"> refers to a Tensor which does not &quot;</span>
                         <span class="s2">&quot;exist. The operation, </span><span class="si">%s</span><span class="s2">, exists but only has &quot;</span>
                         <span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> outputs.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">repr</span><span class="p">(</span><span class="n">name</span><span class="p">),</span> <span class="nb">repr</span><span class="p">(</span><span class="n">op_name</span><span class="p">),</span>
                                          <span class="nb">len</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">outputs</span><span class="p">)))</span>

      <span class="k">elif</span> <span class="s2">&quot;:&quot;</span> <span class="ow">in</span> <span class="n">name</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">allow_tensor</span><span class="p">:</span>
        <span class="c1"># Looks like a Tensor name but can&#39;t be a Tensor.</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Name </span><span class="si">%s</span><span class="s2"> appears to refer to a Tensor, not a </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span>
                         <span class="p">(</span><span class="nb">repr</span><span class="p">(</span><span class="n">name</span><span class="p">),</span> <span class="n">types_str</span><span class="p">))</span>

      <span class="k">elif</span> <span class="s2">&quot;:&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">name</span> <span class="ow">and</span> <span class="n">allow_operation</span><span class="p">:</span>
        <span class="c1"># Looks like an Operation name and can be an Operation.</span>
        <span class="k">if</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_nodes_by_name</span><span class="p">:</span>
          <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;The name </span><span class="si">%s</span><span class="s2"> refers to an Operation not in the &quot;</span>
                         <span class="s2">&quot;graph.&quot;</span> <span class="o">%</span> <span class="nb">repr</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_nodes_by_name</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>

      <span class="k">elif</span> <span class="s2">&quot;:&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">name</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">allow_operation</span><span class="p">:</span>
        <span class="c1"># Looks like an Operation name but can&#39;t be an Operation.</span>
        <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_nodes_by_name</span><span class="p">:</span>
          <span class="c1"># Yep, it&#39;s an Operation name</span>
          <span class="n">err_msg</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;The name </span><span class="si">%s</span><span class="s2"> refers to an Operation, not a </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span>
                     <span class="p">(</span><span class="nb">repr</span><span class="p">(</span><span class="n">name</span><span class="p">),</span> <span class="n">types_str</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">err_msg</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;The name </span><span class="si">%s</span><span class="s2"> looks like an (invalid) Operation name, &quot;</span>
                     <span class="s2">&quot;not a </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">repr</span><span class="p">(</span><span class="n">name</span><span class="p">),</span> <span class="n">types_str</span><span class="p">))</span>
        <span class="n">err_msg</span> <span class="o">+=</span> <span class="p">(</span><span class="s2">&quot; Tensor names must be of the form &quot;</span>
                    <span class="s2">&quot;</span><span class="se">\&quot;</span><span class="s2">&lt;op_name&gt;:&lt;output_index&gt;</span><span class="se">\&quot;</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">err_msg</span><span class="p">)</span>

    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">allow_tensor</span><span class="p">:</span>
      <span class="c1"># Actually obj is just the object it&#39;s referring to.</span>
      <span class="k">if</span> <span class="n">obj</span><span class="o">.</span><span class="n">graph</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">self</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Tensor </span><span class="si">%s</span><span class="s2"> is not an element of this graph.&quot;</span> <span class="o">%</span> <span class="n">obj</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">obj</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">Operation</span><span class="p">)</span> <span class="ow">and</span> <span class="n">allow_operation</span><span class="p">:</span>
      <span class="c1"># Actually obj is just the object it&#39;s referring to.</span>
      <span class="k">if</span> <span class="n">obj</span><span class="o">.</span><span class="n">graph</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">self</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Operation </span><span class="si">%s</span><span class="s2"> is not an element of this graph.&quot;</span> <span class="o">%</span> <span class="n">obj</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">obj</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># We give up!</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Can not convert a </span><span class="si">%s</span><span class="s2"> into a </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span>
                                                           <span class="n">types_str</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">get_operations</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Return the list of operations in the graph.</span>

<span class="sd">    You can modify the operations in place, but modifications</span>
<span class="sd">    to the list such as inserts/delete have no effect on the</span>
<span class="sd">    list of operations known to the graph.</span>

<span class="sd">    This method may be called concurrently from multiple threads.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A list of Operations.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_finalized</span><span class="p">:</span>
      <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_nodes_by_id</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>

    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span><span class="p">:</span>
      <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_nodes_by_id</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>

  <span class="k">def</span> <span class="nf">get_operation_by_name</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the `Operation` with the given `name`.</span>

<span class="sd">    This method may be called concurrently from multiple threads.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: The name of the `Operation` to return.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The `Operation` with the given `name`.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: If `name` is not a string.</span>
<span class="sd">      KeyError: If `name` does not correspond to an operation in this graph.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Operation names are strings (or similar), not </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span>
                      <span class="nb">type</span><span class="p">(</span><span class="n">name</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">as_graph_element</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_tensor</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">allow_operation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_get_operation_by_name_unsafe</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the `Operation` with the given `name`.</span>

<span class="sd">    This is a internal unsafe version of get_operation_by_name. It skips many</span>
<span class="sd">    checks and does not have user friedly error messages but runs considerably</span>
<span class="sd">    faster. This method may be called concurrently from multiple threads.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: The name of the `Operation` to return.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The `Operation` with the given `name`.</span>

<span class="sd">    Raises:</span>
<span class="sd">      KeyError: If `name` does not correspond to an operation in this graph.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_finalized</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_nodes_by_name</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>

    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_nodes_by_name</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>

  <span class="k">def</span> <span class="nf">_get_operation_by_tf_operation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tf_oper</span><span class="p">):</span>
    <span class="n">op_name</span> <span class="o">=</span> <span class="n">c_api</span><span class="o">.</span><span class="n">TF_OperationName</span><span class="p">(</span><span class="n">tf_oper</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_operation_by_name_unsafe</span><span class="p">(</span><span class="n">op_name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">get_tensor_by_name</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the `Tensor` with the given `name`.</span>

<span class="sd">    This method may be called concurrently from multiple threads.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: The name of the `Tensor` to return.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The `Tensor` with the given `name`.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: If `name` is not a string.</span>
<span class="sd">      KeyError: If `name` does not correspond to a tensor in this graph.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Names should be strings.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Tensor names are strings (or similar), not </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span>
                      <span class="nb">type</span><span class="p">(</span><span class="n">name</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">as_graph_element</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_tensor</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">allow_operation</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_get_tensor_by_tf_output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tf_output</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the `Tensor` representing `tf_output`.</span>

<span class="sd">    Note that there is only one such `Tensor`, i.e. multiple calls to this</span>
<span class="sd">    function with the same TF_Output value will always return the same `Tensor`</span>
<span class="sd">    object.</span>

<span class="sd">    Args:</span>
<span class="sd">      tf_output: A wrapped `TF_Output` (the C API equivalent of `Tensor`).</span>

<span class="sd">    Returns:</span>
<span class="sd">      The `Tensor` that represents `tf_output`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">op</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_operation_by_tf_operation</span><span class="p">(</span><span class="n">tf_output</span><span class="o">.</span><span class="n">oper</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">op</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="n">tf_output</span><span class="o">.</span><span class="n">index</span><span class="p">]</span>

  <span class="k">def</span> <span class="nf">_next_id</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Id for next Operation instance. Also increments the internal id.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_check_not_finalized</span><span class="p">()</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_next_id_counter</span> <span class="o">+=</span> <span class="mi">1</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_next_id_counter</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_last_id</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_next_id_counter</span>

  <span class="k">def</span> <span class="nf">_get_op_def</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">type</span><span class="p">):</span>  <span class="c1"># pylint: disable=redefined-builtin</span>
    <span class="sd">&quot;&quot;&quot;Returns the `OpDef` proto for `type`. `type` is a string.&quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">c_api_util</span><span class="o">.</span><span class="n">tf_buffer</span><span class="p">()</span> <span class="k">as</span> <span class="n">buf</span><span class="p">:</span>
      <span class="c1"># pylint: disable=protected-access</span>
      <span class="n">c_api</span><span class="o">.</span><span class="n">TF_GraphGetOpDef</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_c_graph</span><span class="p">,</span> <span class="n">compat</span><span class="o">.</span><span class="n">as_bytes</span><span class="p">(</span><span class="nb">type</span><span class="p">),</span> <span class="n">buf</span><span class="p">)</span>
      <span class="c1"># pylint: enable=protected-access</span>
      <span class="n">data</span> <span class="o">=</span> <span class="n">c_api</span><span class="o">.</span><span class="n">TF_GetBuffer</span><span class="p">(</span><span class="n">buf</span><span class="p">)</span>
    <span class="n">op_def</span> <span class="o">=</span> <span class="n">op_def_pb2</span><span class="o">.</span><span class="n">OpDef</span><span class="p">()</span>
    <span class="n">op_def</span><span class="o">.</span><span class="n">ParseFromString</span><span class="p">(</span><span class="n">compat</span><span class="o">.</span><span class="n">as_bytes</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">op_def</span>

  <span class="k">def</span> <span class="nf">as_default</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a context manager that makes this `Graph` the default graph.</span>

<span class="sd">    This method should be used if you want to create multiple graphs</span>
<span class="sd">    in the same process. For convenience, a global default graph is</span>
<span class="sd">    provided, and all ops will be added to this graph if you do not</span>
<span class="sd">    create a new graph explicitly.</span>

<span class="sd">    Use this method with the `with` keyword to specify that ops created within</span>
<span class="sd">    the scope of a block should be added to this graph. In this case, once</span>
<span class="sd">    the scope of the `with` is exited, the previous default graph is set again</span>
<span class="sd">    as default. There is a stack, so it&#39;s ok to have multiple nested levels</span>
<span class="sd">    of `as_default` calls.</span>

<span class="sd">    The default graph is a property of the current thread. If you</span>
<span class="sd">    create a new thread, and wish to use the default graph in that</span>
<span class="sd">    thread, you must explicitly add a `with g.as_default():` in that</span>
<span class="sd">    thread&#39;s function.</span>

<span class="sd">    The following code examples are equivalent:</span>

<span class="sd">    ```python</span>
<span class="sd">    # 1. Using Graph.as_default():</span>
<span class="sd">    g = tf.Graph()</span>
<span class="sd">    with g.as_default():</span>
<span class="sd">      c = tf.constant(5.0)</span>
<span class="sd">      assert c.graph is g</span>

<span class="sd">    # 2. Constructing and making default:</span>
<span class="sd">    with tf.Graph().as_default() as g:</span>
<span class="sd">      c = tf.constant(5.0)</span>
<span class="sd">      assert c.graph is g</span>
<span class="sd">    ```</span>

<span class="sd">    If eager execution is enabled ops created under this context manager will be</span>
<span class="sd">    added to the graph instead of executed eagerly.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A context manager for using this graph as the default graph.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_default_graph_stack</span><span class="o">.</span><span class="n">get_controller</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">collections</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the names of the collections known to this graph.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_collections</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">add_to_collection</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Stores `value` in the collection with the given `name`.</span>

<span class="sd">    Note that collections are not sets, so it is possible to add a value to</span>
<span class="sd">    a collection several times.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: The key for the collection. The `GraphKeys` class</span>
<span class="sd">        contains many standard names for collections.</span>
<span class="sd">      value: The value to add to the collection.</span>
<span class="sd">    &quot;&quot;&quot;</span>  <span class="c1"># pylint: disable=g-doc-exception</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_check_not_finalized</span><span class="p">()</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_collections</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_collections</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">value</span><span class="p">]</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_collections</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">add_to_collections</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">names</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Stores `value` in the collections given by `names`.</span>

<span class="sd">    Note that collections are not sets, so it is possible to add a value to</span>
<span class="sd">    a collection several times. This function makes sure that duplicates in</span>
<span class="sd">    `names` are ignored, but it will not check for pre-existing membership of</span>
<span class="sd">    `value` in any of the collections in `names`.</span>

<span class="sd">    `names` can be any iterable, but if `names` is a string, it is treated as a</span>
<span class="sd">    single collection name.</span>

<span class="sd">    Args:</span>
<span class="sd">      names: The keys for the collections to add to. The `GraphKeys` class</span>
<span class="sd">        contains many standard names for collections.</span>
<span class="sd">      value: The value to add to the collections.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Make sure names are unique, but treat strings as a single collection name</span>
    <span class="n">names</span> <span class="o">=</span> <span class="p">(</span><span class="n">names</span><span class="p">,)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">)</span> <span class="k">else</span> <span class="nb">set</span><span class="p">(</span><span class="n">names</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">names</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">get_collection_ref</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a list of values in the collection with the given `name`.</span>

<span class="sd">    If the collection exists, this returns the list itself, which can</span>
<span class="sd">    be modified in place to change the collection.  If the collection does</span>
<span class="sd">    not exist, it is created as an empty list and the list is returned.</span>

<span class="sd">    This is different from `get_collection()` which always returns a copy of</span>
<span class="sd">    the collection list if it exists and never creates an empty collection.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: The key for the collection. For example, the `GraphKeys` class</span>
<span class="sd">        contains many standard names for collections.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The list of values in the collection with the given `name`, or an empty</span>
<span class="sd">      list if no value has been added to that collection.</span>
<span class="sd">    &quot;&quot;&quot;</span>  <span class="c1"># pylint: disable=g-doc-exception</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span><span class="p">:</span>
      <span class="n">coll_list</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_collections</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">coll_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">coll_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_collections</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">coll_list</span>
      <span class="k">return</span> <span class="n">coll_list</span>

  <span class="k">def</span> <span class="nf">get_collection</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a list of values in the collection with the given `name`.</span>

<span class="sd">    This is different from `get_collection_ref()` which always returns the</span>
<span class="sd">    actual collection list if it exists in that it returns a new list each time</span>
<span class="sd">    it is called.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: The key for the collection. For example, the `GraphKeys` class</span>
<span class="sd">        contains many standard names for collections.</span>
<span class="sd">      scope: (Optional.) A string. If supplied, the resulting list is filtered</span>
<span class="sd">        to include only items whose `name` attribute matches `scope` using</span>
<span class="sd">        `re.match`. Items without a `name` attribute are never returned if a</span>
<span class="sd">        scope is supplied. The choice of `re.match` means that a `scope` without</span>
<span class="sd">        special tokens filters by prefix.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The list of values in the collection with the given `name`, or</span>
<span class="sd">      an empty list if no value has been added to that collection. The</span>
<span class="sd">      list contains the values in the order under which they were</span>
<span class="sd">      collected.</span>
<span class="sd">    &quot;&quot;&quot;</span>  <span class="c1"># pylint: disable=g-doc-exception</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span><span class="p">:</span>
      <span class="n">collection</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_collections</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">collection</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">[]</span>
      <span class="k">if</span> <span class="n">scope</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">collection</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">c</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">regex</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">scope</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">collection</span><span class="p">:</span>
          <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="s2">&quot;name&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">regex</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">item</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
            <span class="n">c</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">c</span>

  <span class="k">def</span> <span class="nf">get_all_collection_keys</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a list of collections used in this graph.&quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span><span class="p">:</span>
      <span class="k">return</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_collections</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">)]</span>

  <span class="k">def</span> <span class="nf">clear_collection</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Clears all values in a collection.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: The key for the collection. The `GraphKeys` class contains many</span>
<span class="sd">        standard names for collections.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_check_not_finalized</span><span class="p">()</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_collections</span><span class="p">:</span>
        <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_collections</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>

  <span class="nd">@tf_contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
  <span class="k">def</span> <span class="nf">_original_op</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Python &#39;with&#39; handler to help annotate ops with their originator.</span>

<span class="sd">    An op may have an &#39;original_op&#39; property that indicates the op on which</span>
<span class="sd">    it was based. For example a replica op is based on the op that was</span>
<span class="sd">    replicated and a gradient op is based on the op that was differentiated.</span>

<span class="sd">    All ops created in the scope of this &#39;with&#39; handler will have</span>
<span class="sd">    the given &#39;op&#39; as their original op.</span>

<span class="sd">    Args:</span>
<span class="sd">      op: The Operation that all ops created in this scope will have as their</span>
<span class="sd">        original op.</span>

<span class="sd">    Yields:</span>
<span class="sd">      Nothing.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">old_original_op</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_default_original_op</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_default_original_op</span> <span class="o">=</span> <span class="n">op</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">yield</span>
    <span class="k">finally</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_default_original_op</span> <span class="o">=</span> <span class="n">old_original_op</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_name_stack</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># This may be called from a thread where name_stack doesn&#39;t yet exist.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="p">,</span> <span class="s2">&quot;_name_stack&quot;</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_name_stack</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_name_stack</span>

  <span class="nd">@_name_stack</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">_name_stack</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name_stack</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_name_stack</span> <span class="o">=</span> <span class="n">name_stack</span>

  <span class="c1"># pylint: disable=g-doc-return-or-yield,line-too-long</span>
  <span class="nd">@tf_contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
  <span class="k">def</span> <span class="nf">name_scope</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns a context manager that creates hierarchical names for operations.</span>

<span class="sd">    A graph maintains a stack of name scopes. A `with name_scope(...):`</span>
<span class="sd">    statement pushes a new name onto the stack for the lifetime of the context.</span>

<span class="sd">    The `name` argument will be interpreted as follows:</span>

<span class="sd">    * A string (not ending with &#39;/&#39;) will create a new name scope, in which</span>
<span class="sd">      `name` is appended to the prefix of all operations created in the</span>
<span class="sd">      context. If `name` has been used before, it will be made unique by</span>
<span class="sd">      calling `self.unique_name(name)`.</span>
<span class="sd">    * A scope previously captured from a `with g.name_scope(...) as</span>
<span class="sd">      scope:` statement will be treated as an &quot;absolute&quot; name scope, which</span>
<span class="sd">      makes it possible to re-enter existing scopes.</span>
<span class="sd">    * A value of `None` or the empty string will reset the current name scope</span>
<span class="sd">      to the top-level (empty) name scope.</span>

<span class="sd">    For example:</span>

<span class="sd">    ```python</span>
<span class="sd">    with tf.Graph().as_default() as g:</span>
<span class="sd">      c = tf.constant(5.0, name=&quot;c&quot;)</span>
<span class="sd">      assert c.op.name == &quot;c&quot;</span>
<span class="sd">      c_1 = tf.constant(6.0, name=&quot;c&quot;)</span>
<span class="sd">      assert c_1.op.name == &quot;c_1&quot;</span>

<span class="sd">      # Creates a scope called &quot;nested&quot;</span>
<span class="sd">      with g.name_scope(&quot;nested&quot;) as scope:</span>
<span class="sd">        nested_c = tf.constant(10.0, name=&quot;c&quot;)</span>
<span class="sd">        assert nested_c.op.name == &quot;nested/c&quot;</span>

<span class="sd">        # Creates a nested scope called &quot;inner&quot;.</span>
<span class="sd">        with g.name_scope(&quot;inner&quot;):</span>
<span class="sd">          nested_inner_c = tf.constant(20.0, name=&quot;c&quot;)</span>
<span class="sd">          assert nested_inner_c.op.name == &quot;nested/inner/c&quot;</span>

<span class="sd">        # Create a nested scope called &quot;inner_1&quot;.</span>
<span class="sd">        with g.name_scope(&quot;inner&quot;):</span>
<span class="sd">          nested_inner_1_c = tf.constant(30.0, name=&quot;c&quot;)</span>
<span class="sd">          assert nested_inner_1_c.op.name == &quot;nested/inner_1/c&quot;</span>

<span class="sd">          # Treats `scope` as an absolute name scope, and</span>
<span class="sd">          # switches to the &quot;nested/&quot; scope.</span>
<span class="sd">          with g.name_scope(scope):</span>
<span class="sd">            nested_d = tf.constant(40.0, name=&quot;d&quot;)</span>
<span class="sd">            assert nested_d.op.name == &quot;nested/d&quot;</span>

<span class="sd">            with g.name_scope(&quot;&quot;):</span>
<span class="sd">              e = tf.constant(50.0, name=&quot;e&quot;)</span>
<span class="sd">              assert e.op.name == &quot;e&quot;</span>
<span class="sd">    ```</span>

<span class="sd">    The name of the scope itself can be captured by `with</span>
<span class="sd">    g.name_scope(...) as scope:`, which stores the name of the scope</span>
<span class="sd">    in the variable `scope`. This value can be used to name an</span>
<span class="sd">    operation that represents the overall result of executing the ops</span>
<span class="sd">    in a scope. For example:</span>

<span class="sd">    ```python</span>
<span class="sd">    inputs = tf.constant(...)</span>
<span class="sd">    with g.name_scope(&#39;my_layer&#39;) as scope:</span>
<span class="sd">      weights = tf.Variable(..., name=&quot;weights&quot;)</span>
<span class="sd">      biases = tf.Variable(..., name=&quot;biases&quot;)</span>
<span class="sd">      affine = tf.matmul(inputs, weights) + biases</span>
<span class="sd">      output = tf.nn.relu(affine, name=scope)</span>
<span class="sd">    ```</span>

<span class="sd">    NOTE: This constructor validates the given `name`. Valid scope</span>
<span class="sd">    names match one of the following regular expressions:</span>

<span class="sd">        [A-Za-z0-9.][A-Za-z0-9_.\\-/]* (for scopes at the root)</span>
<span class="sd">        [A-Za-z0-9_.\\-/]* (for other scopes)</span>

<span class="sd">    Args:</span>
<span class="sd">      name: A name for the scope.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A context manager that installs `name` as a new name scope.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If `name` is not a valid scope name, according to the rules</span>
<span class="sd">        above.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">name</span><span class="p">:</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">compat</span><span class="o">.</span><span class="n">bytes_or_text_types</span><span class="p">):</span>
        <span class="n">name</span> <span class="o">=</span> <span class="n">compat</span><span class="o">.</span><span class="n">as_str</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>

      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name_stack</span><span class="p">:</span>
        <span class="c1"># Scopes created in a nested scope may have initial characters</span>
        <span class="c1"># that are illegal as the initial character of an op name</span>
        <span class="c1"># (viz. &#39;-&#39;, &#39;\&#39;, &#39;/&#39;, and &#39;_&#39;).</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">_VALID_SCOPE_NAME_REGEX</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
          <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;</span><span class="si">%s</span><span class="s2">&#39; is not a valid scope name&quot;</span> <span class="o">%</span> <span class="n">name</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Scopes created in the root must match the more restrictive</span>
        <span class="c1"># op name regex, which constrains the initial character.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">_VALID_OP_NAME_REGEX</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
          <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;</span><span class="si">%s</span><span class="s2">&#39; is not a valid scope name&quot;</span> <span class="o">%</span> <span class="n">name</span><span class="p">)</span>
    <span class="n">old_stack</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name_stack</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">name</span><span class="p">:</span>  <span class="c1"># Both for name=None and name=&quot;&quot; we re-set to empty scope.</span>
      <span class="n">new_stack</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">elif</span> <span class="n">name</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;/&quot;</span><span class="p">:</span>
      <span class="n">new_stack</span> <span class="o">=</span> <span class="n">_name_from_scope_name</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">new_stack</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">unique_name</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_name_stack</span> <span class="o">=</span> <span class="n">new_stack</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">yield</span> <span class="s2">&quot;&quot;</span> <span class="k">if</span> <span class="n">new_stack</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">new_stack</span> <span class="o">+</span> <span class="s2">&quot;/&quot;</span>
    <span class="k">finally</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_name_stack</span> <span class="o">=</span> <span class="n">old_stack</span>

  <span class="c1"># pylint: enable=g-doc-return-or-yield,line-too-long</span>

  <span class="k">def</span> <span class="nf">unique_name</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">mark_as_used</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Return a unique operation name for `name`.</span>

<span class="sd">    Note: You rarely need to call `unique_name()` directly.  Most of</span>
<span class="sd">    the time you just need to create `with g.name_scope()` blocks to</span>
<span class="sd">    generate structured names.</span>

<span class="sd">    `unique_name` is used to generate structured names, separated by</span>
<span class="sd">    `&quot;/&quot;`, to help identify operations when debugging a graph.</span>
<span class="sd">    Operation names are displayed in error messages reported by the</span>
<span class="sd">    TensorFlow runtime, and in various visualization tools such as</span>
<span class="sd">    TensorBoard.</span>

<span class="sd">    If `mark_as_used` is set to `True`, which is the default, a new</span>
<span class="sd">    unique name is created and marked as in use. If it&#39;s set to `False`,</span>
<span class="sd">    the unique name is returned without actually being marked as used.</span>
<span class="sd">    This is useful when the caller simply wants to know what the name</span>
<span class="sd">    to be created will be.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: The name for an operation.</span>
<span class="sd">      mark_as_used: Whether to mark this name as being used.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A string to be passed to `create_op()` that will be used</span>
<span class="sd">      to name the operation being created.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name_stack</span><span class="p">:</span>
      <span class="n">name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name_stack</span> <span class="o">+</span> <span class="s2">&quot;/&quot;</span> <span class="o">+</span> <span class="n">name</span>

    <span class="c1"># For the sake of checking for names in use, we treat names as case</span>
    <span class="c1"># insensitive (e.g. foo = Foo).</span>
    <span class="n">name_key</span> <span class="o">=</span> <span class="n">name</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="n">i</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_names_in_use</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">name_key</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="c1"># Increment the number for &quot;name_key&quot;.</span>
    <span class="k">if</span> <span class="n">mark_as_used</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_names_in_use</span><span class="p">[</span><span class="n">name_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
      <span class="n">base_name_key</span> <span class="o">=</span> <span class="n">name_key</span>
      <span class="c1"># Make sure the composed name key is not already used.</span>
      <span class="k">while</span> <span class="n">name_key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_names_in_use</span><span class="p">:</span>
        <span class="n">name_key</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">_</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">base_name_key</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
      <span class="c1"># Mark the composed name_key as used in case someone wants</span>
      <span class="c1"># to call unique_name(&quot;name_1&quot;).</span>
      <span class="k">if</span> <span class="n">mark_as_used</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_names_in_use</span><span class="p">[</span><span class="n">name_key</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

      <span class="c1"># Return the new name with the original capitalization of the given name.</span>
      <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">_</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">name</span>

  <span class="k">def</span> <span class="nf">get_name_scope</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the current name scope.</span>

<span class="sd">    For example:</span>

<span class="sd">    ```python</span>
<span class="sd">    with tf.name_scope(&#39;scope1&#39;):</span>
<span class="sd">      with tf.name_scope(&#39;scope2&#39;):</span>
<span class="sd">        print(tf.get_default_graph().get_name_scope())</span>
<span class="sd">    ```</span>
<span class="sd">    would print the string `scope1/scope2`.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A string representing the current name scope.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name_stack</span>

  <span class="nd">@tf_contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
  <span class="k">def</span> <span class="nf">_colocate_with_for_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="n">gradient_uid</span><span class="p">,</span>
                                  <span class="n">ignore_existing</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">colocate_with</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">ignore_existing</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">gradient_uid</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_control_flow_context</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_control_flow_context</span><span class="o">.</span><span class="n">EnterGradientColocation</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">gradient_uid</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
          <span class="k">yield</span>
        <span class="k">finally</span><span class="p">:</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_control_flow_context</span><span class="o">.</span><span class="n">ExitGradientColocation</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">gradient_uid</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">yield</span>

  <span class="nd">@tf_contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
  <span class="k">def</span> <span class="nf">colocate_with</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="n">ignore_existing</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a context manager that specifies an op to colocate with.</span>

<span class="sd">    Note: this function is not for public use, only for internal libraries.</span>

<span class="sd">    For example:</span>

<span class="sd">    ```python</span>
<span class="sd">    a = tf.Variable([1.0])</span>
<span class="sd">    with g.colocate_with(a):</span>
<span class="sd">      b = tf.constant(1.0)</span>
<span class="sd">      c = tf.add(a, b)</span>
<span class="sd">    ```</span>

<span class="sd">    `b` and `c` will always be colocated with `a`, no matter where `a`</span>
<span class="sd">    is eventually placed.</span>

<span class="sd">    **NOTE** Using a colocation scope resets any existing device constraints.</span>

<span class="sd">    If `op` is `None` then `ignore_existing` must be `True` and the new</span>
<span class="sd">    scope resets all colocation and device constraints.</span>

<span class="sd">    Args:</span>
<span class="sd">      op: The op to colocate all created ops with, or `None`.</span>
<span class="sd">      ignore_existing: If true, only applies colocation of this op within</span>
<span class="sd">        the context, rather than applying all colocation properties</span>
<span class="sd">        on the stack.  If `op` is `None`, this value must be `True`.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: if op is None but ignore_existing is False.</span>

<span class="sd">    Yields:</span>
<span class="sd">      A context manager that specifies the op with which to colocate</span>
<span class="sd">      newly created ops.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">op</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">ignore_existing</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Trying to reset colocation (op is None) but &quot;</span>
                       <span class="s2">&quot;ignore_existing is not True&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">op</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">Operation</span><span class="p">):</span>
      <span class="c1"># We always want to colocate with the reference op.</span>
      <span class="n">op</span> <span class="o">=</span> <span class="n">internal_convert_to_tensor_or_indexed_slices</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">as_ref</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">op</span>

    <span class="c1"># By default, colocate_with resets the device function stack,</span>
    <span class="c1"># since colocate_with is typically used in specific internal</span>
    <span class="c1"># library functions where colocation is intended to be &quot;stronger&quot;</span>
    <span class="c1"># than device functions.</span>
    <span class="c1">#</span>
    <span class="c1"># In the future, a caller may specify that device_functions win</span>
    <span class="c1"># over colocation, in which case we can add support.</span>
    <span class="n">device_fn_tmp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device_function_stack</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_device_function_stack</span> <span class="o">=</span> <span class="n">traceable_stack</span><span class="o">.</span><span class="n">TraceableStack</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">ignore_existing</span><span class="p">:</span>
      <span class="n">current_stack</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_colocation_stack</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_colocation_stack</span> <span class="o">=</span> <span class="n">traceable_stack</span><span class="o">.</span><span class="n">TraceableStack</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">op</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="c1"># offset refers to the stack frame used for storing code location.</span>
      <span class="c1"># We use 4, the sum of 1 to use our caller&#39;s stack frame and 3</span>
      <span class="c1"># to jump over layers of context managers above us.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_colocation_stack</span><span class="o">.</span><span class="n">push_obj</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

    <span class="k">try</span><span class="p">:</span>
      <span class="k">yield</span>
    <span class="k">finally</span><span class="p">:</span>
      <span class="c1"># Restore device function stack</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_device_function_stack</span> <span class="o">=</span> <span class="n">device_fn_tmp</span>
      <span class="k">if</span> <span class="n">op</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_colocation_stack</span><span class="o">.</span><span class="n">pop_obj</span><span class="p">()</span>

      <span class="c1"># Reset the colocation stack if requested.</span>
      <span class="k">if</span> <span class="n">ignore_existing</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_colocation_stack</span> <span class="o">=</span> <span class="n">current_stack</span>

  <span class="k">def</span> <span class="nf">_add_device_to_stack</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device_name_or_function</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Add device to stack manually, separate from a context manager.&quot;&quot;&quot;</span>
    <span class="n">total_offset</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">offset</span>
    <span class="n">spec</span> <span class="o">=</span> <span class="n">_UserDeviceSpec</span><span class="p">(</span><span class="n">device_name_or_function</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_device_function_stack</span><span class="o">.</span><span class="n">push_obj</span><span class="p">(</span><span class="n">spec</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="n">total_offset</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">spec</span>

  <span class="nd">@tf_contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
  <span class="k">def</span> <span class="nf">device</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device_name_or_function</span><span class="p">):</span>
    <span class="c1"># pylint: disable=line-too-long</span>
    <span class="sd">&quot;&quot;&quot;Returns a context manager that specifies the default device to use.</span>

<span class="sd">    The `device_name_or_function` argument may either be a device name</span>
<span class="sd">    string, a device function, or None:</span>

<span class="sd">    * If it is a device name string, all operations constructed in</span>
<span class="sd">      this context will be assigned to the device with that name, unless</span>
<span class="sd">      overridden by a nested `device()` context.</span>
<span class="sd">    * If it is a function, it will be treated as a function from</span>
<span class="sd">      Operation objects to device name strings, and invoked each time</span>
<span class="sd">      a new Operation is created. The Operation will be assigned to</span>
<span class="sd">      the device with the returned name.</span>
<span class="sd">    * If it is None, all `device()` invocations from the enclosing context</span>
<span class="sd">      will be ignored.</span>

<span class="sd">    For information about the valid syntax of device name strings, see</span>
<span class="sd">    the documentation in</span>
<span class="sd">    [`DeviceNameUtils`](https://www.tensorflow.org/code/tensorflow/core/util/device_name_utils.h).</span>

<span class="sd">    For example:</span>

<span class="sd">    ```python</span>
<span class="sd">    with g.device(&#39;/device:GPU:0&#39;):</span>
<span class="sd">      # All operations constructed in this context will be placed</span>
<span class="sd">      # on GPU 0.</span>
<span class="sd">      with g.device(None):</span>
<span class="sd">        # All operations constructed in this context will have no</span>
<span class="sd">        # assigned device.</span>

<span class="sd">    # Defines a function from `Operation` to device string.</span>
<span class="sd">    def matmul_on_gpu(n):</span>
<span class="sd">      if n.type == &quot;MatMul&quot;:</span>
<span class="sd">        return &quot;/device:GPU:0&quot;</span>
<span class="sd">      else:</span>
<span class="sd">        return &quot;/cpu:0&quot;</span>

<span class="sd">    with g.device(matmul_on_gpu):</span>
<span class="sd">      # All operations of type &quot;MatMul&quot; constructed in this context</span>
<span class="sd">      # will be placed on GPU 0; all other operations will be placed</span>
<span class="sd">      # on CPU 0.</span>
<span class="sd">    ```</span>

<span class="sd">    **N.B.** The device scope may be overridden by op wrappers or</span>
<span class="sd">    other library code. For example, a variable assignment op</span>
<span class="sd">    `v.assign()` must be colocated with the `tf.Variable` `v`, and</span>
<span class="sd">    incompatible device scopes will be ignored.</span>

<span class="sd">    Args:</span>
<span class="sd">      device_name_or_function: The device name or function to use in</span>
<span class="sd">        the context.</span>

<span class="sd">    Yields:</span>
<span class="sd">      A context manager that specifies the default device to use for newly</span>
<span class="sd">      created ops.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_add_device_to_stack</span><span class="p">(</span><span class="n">device_name_or_function</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">yield</span>
    <span class="k">finally</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_device_function_stack</span><span class="o">.</span><span class="n">pop_obj</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">_apply_device_functions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Applies the current device function stack to the given operation.&quot;&quot;&quot;</span>
    <span class="c1"># Apply any device functions in LIFO order, so that the most recently</span>
    <span class="c1"># pushed function has the first chance to apply a device to the op.</span>
    <span class="c1"># We apply here because the result can depend on the Operation&#39;s</span>
    <span class="c1"># signature, which is computed in the Operation constructor.</span>
    <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">for</span> <span class="n">device_spec</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device_function_stack</span><span class="o">.</span><span class="n">peek_objs</span><span class="p">():</span>
      <span class="k">if</span> <span class="n">device_spec</span><span class="o">.</span><span class="n">function</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">break</span>
      <span class="n">op</span><span class="o">.</span><span class="n">_set_device</span><span class="p">(</span><span class="n">device_spec</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">op</span><span class="p">))</span>
    <span class="n">op</span><span class="o">.</span><span class="n">_device_code_locations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_snapshot_device_function_stack_metadata</span><span class="p">()</span>
    <span class="c1"># pylint: enable=protected-access</span>

  <span class="c1"># pylint: disable=g-doc-return-or-yield</span>
  <span class="nd">@tf_contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
  <span class="k">def</span> <span class="nf">container</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">container_name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a context manager that specifies the resource container to use.</span>

<span class="sd">    Stateful operations, such as variables and queues, can maintain their</span>
<span class="sd">    states on devices so that they can be shared by multiple processes.</span>
<span class="sd">    A resource container is a string name under which these stateful</span>
<span class="sd">    operations are tracked. These resources can be released or cleared</span>
<span class="sd">    with `tf.Session.reset()`.</span>

<span class="sd">    For example:</span>

<span class="sd">    ```python</span>
<span class="sd">    with g.container(&#39;experiment0&#39;):</span>
<span class="sd">      # All stateful Operations constructed in this context will be placed</span>
<span class="sd">      # in resource container &quot;experiment0&quot;.</span>
<span class="sd">      v1 = tf.Variable([1.0])</span>
<span class="sd">      v2 = tf.Variable([2.0])</span>
<span class="sd">      with g.container(&quot;experiment1&quot;):</span>
<span class="sd">        # All stateful Operations constructed in this context will be</span>
<span class="sd">        # placed in resource container &quot;experiment1&quot;.</span>
<span class="sd">        v3 = tf.Variable([3.0])</span>
<span class="sd">        q1 = tf.FIFOQueue(10, tf.float32)</span>
<span class="sd">      # All stateful Operations constructed in this context will be</span>
<span class="sd">      # be created in the &quot;experiment0&quot;.</span>
<span class="sd">      v4 = tf.Variable([4.0])</span>
<span class="sd">      q1 = tf.FIFOQueue(20, tf.float32)</span>
<span class="sd">      with g.container(&quot;&quot;):</span>
<span class="sd">        # All stateful Operations constructed in this context will be</span>
<span class="sd">        # be placed in the default resource container.</span>
<span class="sd">        v5 = tf.Variable([5.0])</span>
<span class="sd">        q3 = tf.FIFOQueue(30, tf.float32)</span>

<span class="sd">    # Resets container &quot;experiment0&quot;, after which the state of v1, v2, v4, q1</span>
<span class="sd">    # will become undefined (such as uninitialized).</span>
<span class="sd">    tf.Session.reset(target, [&quot;experiment0&quot;])</span>
<span class="sd">    ```</span>

<span class="sd">    Args:</span>
<span class="sd">      container_name: container name string.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A context manager for defining resource containers for stateful ops,</span>
<span class="sd">        yields the container name.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">original_container</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_container</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_container</span> <span class="o">=</span> <span class="n">container_name</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">yield</span> <span class="bp">self</span><span class="o">.</span><span class="n">_container</span>
    <span class="k">finally</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_container</span> <span class="o">=</span> <span class="n">original_container</span>

  <span class="c1"># pylint: enable=g-doc-return-or-yield</span>

  <span class="k">class</span> <span class="nc">_ControlDependenciesController</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Context manager for `control_dependencies()`.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">graph</span><span class="p">,</span> <span class="n">control_inputs</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Create a new `_ControlDependenciesController`.</span>

<span class="sd">      A `_ControlDependenciesController` is the context manager for</span>
<span class="sd">      `with tf.control_dependencies()` blocks.  These normally nest,</span>
<span class="sd">      as described in the documentation for `control_dependencies()`.</span>

<span class="sd">      The `control_inputs` argument list control dependencies that must be</span>
<span class="sd">      added to the current set of control dependencies.  Because of</span>
<span class="sd">      uniquification the set can be empty even if the caller passed a list of</span>
<span class="sd">      ops.  The special value `None` indicates that we want to start a new</span>
<span class="sd">      empty set of control dependencies instead of extending the current set.</span>

<span class="sd">      In that case we also clear the current control flow context, which is an</span>
<span class="sd">      additional mechanism to add control dependencies.</span>

<span class="sd">      Args:</span>
<span class="sd">        graph: The graph that this controller is managing.</span>
<span class="sd">        control_inputs: List of ops to use as control inputs in addition</span>
<span class="sd">          to the current control dependencies.  None to indicate that</span>
<span class="sd">          the dependencies should be cleared.</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span> <span class="o">=</span> <span class="n">graph</span>
      <span class="k">if</span> <span class="n">control_inputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_control_inputs_val</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_new_stack</span> <span class="o">=</span> <span class="kc">True</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_control_inputs_val</span> <span class="o">=</span> <span class="n">control_inputs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_new_stack</span> <span class="o">=</span> <span class="kc">False</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_seen_nodes</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_old_stack</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_old_control_flow_context</span> <span class="o">=</span> <span class="kc">None</span>

<span class="c1"># pylint: disable=protected-access</span>

    <span class="k">def</span> <span class="nf">__enter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_new_stack</span><span class="p">:</span>
        <span class="c1"># Clear the control_dependencies graph.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_old_stack</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_control_dependencies_stack</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_control_dependencies_stack</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># Clear the control_flow_context too.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_old_control_flow_context</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_get_control_flow_context</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_set_control_flow_context</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_push_control_dependencies_controller</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">unused_type</span><span class="p">,</span> <span class="n">unused_value</span><span class="p">,</span> <span class="n">unused_traceback</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_pop_control_dependencies_controller</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_new_stack</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_control_dependencies_stack</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_old_stack</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_set_control_flow_context</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_old_control_flow_context</span><span class="p">)</span>

<span class="c1"># pylint: enable=protected-access</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">control_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_control_inputs_val</span>

    <span class="k">def</span> <span class="nf">add_op</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_seen_nodes</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">op_in_group</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">op</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_seen_nodes</span>

  <span class="k">def</span> <span class="nf">_push_control_dependencies_controller</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">controller</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_control_dependencies_stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">controller</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_pop_control_dependencies_controller</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">controller</span><span class="p">):</span>
    <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_control_dependencies_stack</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="n">controller</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_control_dependencies_stack</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">_current_control_dependencies</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">controller</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_control_dependencies_stack</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">op</span> <span class="ow">in</span> <span class="n">controller</span><span class="o">.</span><span class="n">control_inputs</span><span class="p">:</span>
        <span class="n">ret</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span>

  <span class="k">def</span> <span class="nf">_control_dependencies_for_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ops</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;For an op that takes `input_ops` as inputs, compute control inputs.</span>

<span class="sd">    The returned control dependencies should yield an execution that</span>
<span class="sd">    is equivalent to adding all control inputs in</span>
<span class="sd">    self._control_dependencies_stack to a newly created op. However,</span>
<span class="sd">    this function attempts to prune the returned control dependencies</span>
<span class="sd">    by observing that nodes created within the same `with</span>
<span class="sd">    control_dependencies(...):` block may have data dependencies that make</span>
<span class="sd">    the explicit approach redundant.</span>

<span class="sd">    Args:</span>
<span class="sd">      input_ops: The data input ops for an op to be created.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A list of control inputs for the op to be created.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">controller</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_control_dependencies_stack</span><span class="p">:</span>
      <span class="c1"># If any of the input_ops already depends on the inputs from controller,</span>
      <span class="c1"># we say that the new op is dominated (by that input), and we therefore</span>
      <span class="c1"># do not need to add control dependencies for this controller&#39;s inputs.</span>
      <span class="n">dominated</span> <span class="o">=</span> <span class="kc">False</span>
      <span class="k">for</span> <span class="n">op</span> <span class="ow">in</span> <span class="n">input_ops</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">controller</span><span class="o">.</span><span class="n">op_in_group</span><span class="p">(</span><span class="n">op</span><span class="p">):</span>
          <span class="n">dominated</span> <span class="o">=</span> <span class="kc">True</span>
          <span class="k">break</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">dominated</span><span class="p">:</span>
        <span class="c1"># Don&#39;t add a control input if we already have a data dependency on i.</span>
        <span class="c1"># NOTE(mrry): We do not currently track transitive data dependencies,</span>
        <span class="c1">#   so we may add redundant control inputs.</span>
        <span class="n">ret</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">controller</span><span class="o">.</span><span class="n">control_inputs</span> <span class="k">if</span> <span class="n">c</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">input_ops</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">ret</span>

  <span class="k">def</span> <span class="nf">_record_op_seen_by_control_dependencies</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Record that the given op depends on all registered control dependencies.</span>

<span class="sd">    Args:</span>
<span class="sd">      op: An Operation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">controller</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_control_dependencies_stack</span><span class="p">:</span>
      <span class="n">controller</span><span class="o">.</span><span class="n">add_op</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">control_dependencies</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">control_inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a context manager that specifies control dependencies.</span>

<span class="sd">    Use with the `with` keyword to specify that all operations constructed</span>
<span class="sd">    within the context should have control dependencies on</span>
<span class="sd">    `control_inputs`. For example:</span>

<span class="sd">    ```python</span>
<span class="sd">    with g.control_dependencies([a, b, c]):</span>
<span class="sd">      # `d` and `e` will only run after `a`, `b`, and `c` have executed.</span>
<span class="sd">      d = ...</span>
<span class="sd">      e = ...</span>
<span class="sd">    ```</span>

<span class="sd">    Multiple calls to `control_dependencies()` can be nested, and in</span>
<span class="sd">    that case a new `Operation` will have control dependencies on the union</span>
<span class="sd">    of `control_inputs` from all active contexts.</span>

<span class="sd">    ```python</span>
<span class="sd">    with g.control_dependencies([a, b]):</span>
<span class="sd">      # Ops constructed here run after `a` and `b`.</span>
<span class="sd">      with g.control_dependencies([c, d]):</span>
<span class="sd">        # Ops constructed here run after `a`, `b`, `c`, and `d`.</span>
<span class="sd">    ```</span>

<span class="sd">    You can pass None to clear the control dependencies:</span>

<span class="sd">    ```python</span>
<span class="sd">    with g.control_dependencies([a, b]):</span>
<span class="sd">      # Ops constructed here run after `a` and `b`.</span>
<span class="sd">      with g.control_dependencies(None):</span>
<span class="sd">        # Ops constructed here run normally, not waiting for either `a` or `b`.</span>
<span class="sd">        with g.control_dependencies([c, d]):</span>
<span class="sd">          # Ops constructed here run after `c` and `d`, also not waiting</span>
<span class="sd">          # for either `a` or `b`.</span>
<span class="sd">    ```</span>

<span class="sd">    *N.B.* The control dependencies context applies *only* to ops that</span>
<span class="sd">    are constructed within the context. Merely using an op or tensor</span>
<span class="sd">    in the context does not add a control dependency. The following</span>
<span class="sd">    example illustrates this point:</span>

<span class="sd">    ```python</span>
<span class="sd">    # WRONG</span>
<span class="sd">    def my_func(pred, tensor):</span>
<span class="sd">      t = tf.matmul(tensor, tensor)</span>
<span class="sd">      with tf.control_dependencies([pred]):</span>
<span class="sd">        # The matmul op is created outside the context, so no control</span>
<span class="sd">        # dependency will be added.</span>
<span class="sd">        return t</span>

<span class="sd">    # RIGHT</span>
<span class="sd">    def my_func(pred, tensor):</span>
<span class="sd">      with tf.control_dependencies([pred]):</span>
<span class="sd">        # The matmul op is created in the context, so a control dependency</span>
<span class="sd">        # will be added.</span>
<span class="sd">        return tf.matmul(tensor, tensor)</span>
<span class="sd">    ```</span>

<span class="sd">    Also note that though execution of ops created under this scope will trigger</span>
<span class="sd">    execution of the dependencies, the ops created under this scope might still</span>
<span class="sd">    be pruned from a normal tensorflow graph. For example, in the following</span>
<span class="sd">    snippet of code the dependencies are never executed:</span>

<span class="sd">    ```python</span>
<span class="sd">      loss = model.loss()</span>
<span class="sd">      with tf.control_dependencies(dependencies):</span>
<span class="sd">        loss = loss + tf.constant(1)  # note: dependencies ignored in the</span>
<span class="sd">                                      # backward pass</span>
<span class="sd">      return tf.gradients(loss, model.variables)</span>
<span class="sd">    ```</span>

<span class="sd">    This is because evaluating the gradient graph does not require evaluating</span>
<span class="sd">    the constant(1) op created in the forward pass.</span>

<span class="sd">    Args:</span>
<span class="sd">      control_inputs: A list of `Operation` or `Tensor` objects which</span>
<span class="sd">        must be executed or computed before running the operations</span>
<span class="sd">        defined in the context.  Can also be `None` to clear the control</span>
<span class="sd">        dependencies.</span>

<span class="sd">    Returns:</span>
<span class="sd">     A context manager that specifies control dependencies for all</span>
<span class="sd">     operations constructed within the context.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: If `control_inputs` is not a list of `Operation` or</span>
<span class="sd">        `Tensor` objects.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">control_inputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ControlDependenciesController</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="c1"># First convert the inputs to ops, and deduplicate them.</span>
    <span class="c1"># NOTE(mrry): Other than deduplication, we do not currently track direct</span>
    <span class="c1">#   or indirect dependencies between control_inputs, which may result in</span>
    <span class="c1">#   redundant control inputs.</span>
    <span class="n">control_ops</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">current</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_current_control_dependencies</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">control_inputs</span><span class="p">:</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">IndexedSlices</span><span class="p">):</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">c</span><span class="o">.</span><span class="n">op</span>
      <span class="n">c</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">as_graph_element</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">c</span><span class="o">.</span><span class="n">op</span>
      <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">Operation</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Control input must be Operation or Tensor: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">c</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">c</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">current</span><span class="p">:</span>
        <span class="n">control_ops</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
        <span class="n">current</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ControlDependenciesController</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">control_ops</span><span class="p">)</span>

  <span class="c1"># pylint: disable=g-doc-return-or-yield</span>
  <span class="nd">@tf_contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
  <span class="k">def</span> <span class="nf">_attr_scope</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr_map</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;EXPERIMENTAL: A context manager for setting attributes on operators.</span>

<span class="sd">    This context manager can be used to add additional</span>
<span class="sd">    attributes to operators within the scope of the context.</span>

<span class="sd">    For example:</span>

<span class="sd">       with ops.Graph().as_default() as g:</span>
<span class="sd">         f_1 = Foo()  # No extra attributes</span>
<span class="sd">         with g._attr_scope({&quot;_a&quot;: tf.attr_value_pb2.AttrValue(b=False)}):</span>
<span class="sd">           f_2 = Foo()  # Additional attribute _a=False</span>
<span class="sd">           with g._attr_scope({&quot;_a&quot;: tf.attr_value_pb2.AttrValue(b=True)}):</span>
<span class="sd">             f_3 = Foo()  # Additional attribute _a=False</span>
<span class="sd">             with g._attr_scope({&quot;_a&quot;: None}):</span>
<span class="sd">               f_4 = Foo()  # No additional attributes.</span>

<span class="sd">    Args:</span>
<span class="sd">      attr_map: A dictionary mapping attr name strings to</span>
<span class="sd">        AttrValue protocol buffers or None.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A context manager that sets the kernel label to be used for one or more</span>
<span class="sd">      ops created in that context.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: If attr_map is not a dictionary mapping</span>
<span class="sd">        strings to AttrValue protobufs.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">attr_map</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;attr_map must be a dictionary mapping &quot;</span>
                      <span class="s2">&quot;strings to AttrValue protocol buffers&quot;</span><span class="p">)</span>
    <span class="c1"># The saved_attrs dictionary stores any currently-set labels that</span>
    <span class="c1"># will be overridden by this context manager.</span>
    <span class="n">saved_attrs</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="c1"># Install the given attribute</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">attr_map</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">)</span> <span class="ow">and</span>
              <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">attr</span><span class="p">,</span> <span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="kc">None</span><span class="p">),</span> <span class="n">attr_value_pb2</span><span class="o">.</span><span class="n">AttrValue</span><span class="p">))</span> <span class="ow">or</span>
               <span class="n">callable</span><span class="p">(</span><span class="n">attr</span><span class="p">))):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;attr_map must be a dictionary mapping &quot;</span>
                        <span class="s2">&quot;strings to AttrValue protocol buffers or &quot;</span>
                        <span class="s2">&quot;callables that emit AttrValue protocol buffers&quot;</span><span class="p">)</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="n">saved_attrs</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attr_scope_map</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
      <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
        <span class="k">pass</span>
      <span class="k">if</span> <span class="n">attr</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attr_scope_map</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_attr_scope_map</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">attr</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">yield</span>  <span class="c1"># The code within the context runs here.</span>
    <span class="k">finally</span><span class="p">:</span>
      <span class="c1"># Remove the attributes set for this context, and restore any saved</span>
      <span class="c1"># attributes.</span>
      <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">attr_map</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">try</span><span class="p">:</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_attr_scope_map</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">saved_attrs</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
        <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
          <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attr_scope_map</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>

  <span class="c1"># pylint: enable=g-doc-return-or-yield</span>

  <span class="c1"># pylint: disable=g-doc-return-or-yield</span>
  <span class="nd">@tf_contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
  <span class="k">def</span> <span class="nf">_kernel_label_map</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op_to_kernel_label_map</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;EXPERIMENTAL: A context manager for setting kernel labels.</span>

<span class="sd">    This context manager can be used to select particular</span>
<span class="sd">    implementations of kernels within the scope of the context.</span>

<span class="sd">    For example:</span>

<span class="sd">        with ops.Graph().as_default() as g:</span>
<span class="sd">          f_1 = Foo()  # Uses the default registered kernel for the Foo op.</span>
<span class="sd">          with g.kernel_label_map({&quot;Foo&quot;: &quot;v_2&quot;}):</span>
<span class="sd">            f_2 = Foo()  # Uses the registered kernel with label &quot;v_2&quot;</span>
<span class="sd">                         # for the Foo op.</span>
<span class="sd">            with g.kernel_label_map({&quot;Foo&quot;: &quot;v_3&quot;}):</span>
<span class="sd">              f_3 = Foo()  # Uses the registered kernel with label &quot;v_3&quot;</span>
<span class="sd">                           # for the Foo op.</span>
<span class="sd">              with g.kernel_label_map({&quot;Foo&quot;: &quot;&quot;}):</span>
<span class="sd">                f_4 = Foo()  # Uses the default registered kernel</span>
<span class="sd">                             # for the Foo op.</span>

<span class="sd">    Args:</span>
<span class="sd">      op_to_kernel_label_map: A dictionary mapping op type strings to</span>
<span class="sd">        kernel label strings.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A context manager that sets the kernel label to be used for one or more</span>
<span class="sd">      ops created in that context.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: If op_to_kernel_label_map is not a dictionary mapping</span>
<span class="sd">        strings to strings.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op_to_kernel_label_map</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;op_to_kernel_label_map must be a dictionary mapping &quot;</span>
                      <span class="s2">&quot;strings to strings&quot;</span><span class="p">)</span>
    <span class="c1"># The saved_labels dictionary stores any currently-set labels that</span>
    <span class="c1"># will be overridden by this context manager.</span>
    <span class="n">saved_labels</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="c1"># Install the given label</span>
    <span class="k">for</span> <span class="n">op_type</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">op_to_kernel_label_map</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">op_type</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">)</span> <span class="ow">and</span>
              <span class="nb">isinstance</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;op_to_kernel_label_map must be a dictionary mapping &quot;</span>
                        <span class="s2">&quot;strings to strings&quot;</span><span class="p">)</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="n">saved_labels</span><span class="p">[</span><span class="n">op_type</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op_to_kernel_label_map</span><span class="p">[</span><span class="n">op_type</span><span class="p">]</span>
      <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
        <span class="k">pass</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_op_to_kernel_label_map</span><span class="p">[</span><span class="n">op_type</span><span class="p">]</span> <span class="o">=</span> <span class="n">label</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">yield</span>  <span class="c1"># The code within the context runs here.</span>
    <span class="k">finally</span><span class="p">:</span>
      <span class="c1"># Remove the labels set for this context, and restore any saved labels.</span>
      <span class="k">for</span> <span class="n">op_type</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">op_to_kernel_label_map</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">try</span><span class="p">:</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_op_to_kernel_label_map</span><span class="p">[</span><span class="n">op_type</span><span class="p">]</span> <span class="o">=</span> <span class="n">saved_labels</span><span class="p">[</span><span class="n">op_type</span><span class="p">]</span>
        <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
          <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op_to_kernel_label_map</span><span class="p">[</span><span class="n">op_type</span><span class="p">]</span>

  <span class="c1"># pylint: enable=g-doc-return-or-yield</span>

  <span class="c1"># pylint: disable=g-doc-return-or-yield</span>
  <span class="nd">@tf_contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
  <span class="k">def</span> <span class="nf">gradient_override_map</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op_type_map</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;EXPERIMENTAL: A context manager for overriding gradient functions.</span>

<span class="sd">    This context manager can be used to override the gradient function</span>
<span class="sd">    that will be used for ops within the scope of the context.</span>

<span class="sd">    For example:</span>

<span class="sd">    ```python</span>
<span class="sd">    @tf.RegisterGradient(&quot;CustomSquare&quot;)</span>
<span class="sd">    def _custom_square_grad(op, grad):</span>
<span class="sd">      # ...</span>

<span class="sd">    with tf.Graph().as_default() as g:</span>
<span class="sd">      c = tf.constant(5.0)</span>
<span class="sd">      s_1 = tf.square(c)  # Uses the default gradient for tf.square.</span>
<span class="sd">      with g.gradient_override_map({&quot;Square&quot;: &quot;CustomSquare&quot;}):</span>
<span class="sd">        s_2 = tf.square(s_2)  # Uses _custom_square_grad to compute the</span>
<span class="sd">                              # gradient of s_2.</span>
<span class="sd">    ```</span>

<span class="sd">    Args:</span>
<span class="sd">      op_type_map: A dictionary mapping op type strings to alternative op</span>
<span class="sd">        type strings.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A context manager that sets the alternative op type to be used for one</span>
<span class="sd">      or more ops created in that context.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: If `op_type_map` is not a dictionary mapping strings to</span>
<span class="sd">        strings.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op_type_map</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;op_type_map must be a dictionary mapping &quot;</span>
                      <span class="s2">&quot;strings to strings&quot;</span><span class="p">)</span>
    <span class="c1"># The saved_mappings dictionary stores any currently-set mappings that</span>
    <span class="c1"># will be overridden by this context manager.</span>
    <span class="n">saved_mappings</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="c1"># Install the given label</span>
    <span class="k">for</span> <span class="n">op_type</span><span class="p">,</span> <span class="n">mapped_op_type</span> <span class="ow">in</span> <span class="n">op_type_map</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">op_type</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">)</span> <span class="ow">and</span>
              <span class="nb">isinstance</span><span class="p">(</span><span class="n">mapped_op_type</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;op_type_map must be a dictionary mapping &quot;</span>
                        <span class="s2">&quot;strings to strings&quot;</span><span class="p">)</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="n">saved_mappings</span><span class="p">[</span><span class="n">op_type</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gradient_override_map</span><span class="p">[</span><span class="n">op_type</span><span class="p">]</span>
      <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
        <span class="k">pass</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_gradient_override_map</span><span class="p">[</span><span class="n">op_type</span><span class="p">]</span> <span class="o">=</span> <span class="n">mapped_op_type</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">yield</span>  <span class="c1"># The code within the context runs here.</span>
    <span class="k">finally</span><span class="p">:</span>
      <span class="c1"># Remove the labels set for this context, and restore any saved labels.</span>
      <span class="k">for</span> <span class="n">op_type</span><span class="p">,</span> <span class="n">mapped_op_type</span> <span class="ow">in</span> <span class="n">op_type_map</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">try</span><span class="p">:</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_gradient_override_map</span><span class="p">[</span><span class="n">op_type</span><span class="p">]</span> <span class="o">=</span> <span class="n">saved_mappings</span><span class="p">[</span><span class="n">op_type</span><span class="p">]</span>
        <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
          <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gradient_override_map</span><span class="p">[</span><span class="n">op_type</span><span class="p">]</span>

  <span class="c1"># pylint: enable=g-doc-return-or-yield</span>

  <span class="k">def</span> <span class="nf">prevent_feeding</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Marks the given `tensor` as unfeedable in this graph.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_unfeedable_tensors</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">is_feedable</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns `True` if and only if `tensor` is feedable.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unfeedable_tensors</span>

  <span class="k">def</span> <span class="nf">prevent_fetching</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Marks the given `op` as unfetchable in this graph.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_unfetchable_ops</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">is_fetchable</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor_or_op</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns `True` if and only if `tensor_or_op` is fetchable.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor_or_op</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">tensor_or_op</span><span class="o">.</span><span class="n">op</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unfetchable_ops</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">tensor_or_op</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unfetchable_ops</span>

  <span class="k">def</span> <span class="nf">switch_to_thread_local</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Make device, colocation and dependencies stacks thread-local.</span>

<span class="sd">    Device, colocation and dependencies stacks are not thread-local be default.</span>
<span class="sd">    If multiple threads access them, then the state is shared.  This means that</span>
<span class="sd">    one thread may affect the behavior of another thread.</span>

<span class="sd">    After this method is called, the stacks become thread-local.  If multiple</span>
<span class="sd">    threads access them, then the state is not shared.  Each thread uses its own</span>
<span class="sd">    value; a thread doesn&#39;t affect other threads by mutating such a stack.</span>

<span class="sd">    The initial value for every thread&#39;s stack is set to the current value</span>
<span class="sd">    of the stack when `switch_to_thread_local()` was first called.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stack_state_is_thread_local</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_stack_state_is_thread_local</span> <span class="o">=</span> <span class="kc">True</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_device_function_stack</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stack_state_is_thread_local</span><span class="p">:</span>
      <span class="c1"># This may be called from a thread where device_function_stack doesn&#39;t yet</span>
      <span class="c1"># exist.</span>
      <span class="c1"># pylint: disable=protected-access</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="p">,</span> <span class="s2">&quot;_device_function_stack&quot;</span><span class="p">):</span>
        <span class="n">stack_copy_for_this_thread</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph_device_function_stack</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_device_function_stack</span> <span class="o">=</span> <span class="n">stack_copy_for_this_thread</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_device_function_stack</span>
      <span class="c1"># pylint: enable=protected-access</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph_device_function_stack</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_device_functions_outer_to_inner</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">user_device_specs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device_function_stack</span><span class="o">.</span><span class="n">peek_objs</span><span class="p">()</span>
    <span class="n">device_functions</span> <span class="o">=</span> <span class="p">[</span><span class="n">spec</span><span class="o">.</span><span class="n">function</span> <span class="k">for</span> <span class="n">spec</span> <span class="ow">in</span> <span class="n">user_device_specs</span><span class="p">]</span>
    <span class="n">device_functions_outer_to_inner</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">device_functions</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">device_functions_outer_to_inner</span>

  <span class="k">def</span> <span class="nf">_snapshot_device_function_stack_metadata</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Return device function stack as a list of TraceableObjects.</span>

<span class="sd">    Returns:</span>
<span class="sd">      [traceable_stack.TraceableObject, ...] where each TraceableObject&#39;s .obj</span>
<span class="sd">      member is a displayable name for the user&#39;s argument to Graph.device, and</span>
<span class="sd">      the filename and lineno members point to the code location where</span>
<span class="sd">      Graph.device was called directly or indirectly by the user.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">traceable_objects</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device_function_stack</span><span class="o">.</span><span class="n">peek_traceable_objs</span><span class="p">()</span>
    <span class="n">snapshot</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">obj</span> <span class="ow">in</span> <span class="n">traceable_objects</span><span class="p">:</span>
      <span class="n">obj_copy</span> <span class="o">=</span> <span class="n">obj</span><span class="o">.</span><span class="n">copy_metadata</span><span class="p">()</span>
      <span class="n">obj_copy</span><span class="o">.</span><span class="n">obj</span> <span class="o">=</span> <span class="n">obj</span><span class="o">.</span><span class="n">obj</span><span class="o">.</span><span class="n">display_name</span>
      <span class="n">snapshot</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">obj_copy</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">snapshot</span>

  <span class="nd">@_device_function_stack</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">_device_function_stack</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device_function_stack</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stack_state_is_thread_local</span><span class="p">:</span>
      <span class="c1"># pylint: disable=protected-access</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_device_function_stack</span> <span class="o">=</span> <span class="n">device_function_stack</span>
      <span class="c1"># pylint: enable=protected-access</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_graph_device_function_stack</span> <span class="o">=</span> <span class="n">device_function_stack</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_colocation_stack</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Return thread-local copy of colocation stack.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stack_state_is_thread_local</span><span class="p">:</span>
      <span class="c1"># This may be called from a thread where colocation_stack doesn&#39;t yet</span>
      <span class="c1"># exist.</span>
      <span class="c1"># pylint: disable=protected-access</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="p">,</span> <span class="s2">&quot;_colocation_stack&quot;</span><span class="p">):</span>
        <span class="n">stack_copy_for_this_thread</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph_colocation_stack</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_colocation_stack</span> <span class="o">=</span> <span class="n">stack_copy_for_this_thread</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_colocation_stack</span>
      <span class="c1"># pylint: enable=protected-access</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph_colocation_stack</span>

  <span class="k">def</span> <span class="nf">_snapshot_colocation_stack_metadata</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Return colocation stack metadata as a dictionary.&quot;&quot;&quot;</span>
    <span class="n">traceable_objects</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_colocation_stack</span><span class="o">.</span><span class="n">peek_traceable_objs</span><span class="p">()</span>
    <span class="k">return</span> <span class="p">{</span><span class="n">obj</span><span class="o">.</span><span class="n">obj</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="n">obj</span><span class="o">.</span><span class="n">copy_metadata</span><span class="p">()</span> <span class="k">for</span> <span class="n">obj</span> <span class="ow">in</span> <span class="n">traceable_objects</span><span class="p">}</span>

  <span class="nd">@_colocation_stack</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">_colocation_stack</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">colocation_stack</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stack_state_is_thread_local</span><span class="p">:</span>
      <span class="c1"># pylint: disable=protected-access</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_colocation_stack</span> <span class="o">=</span> <span class="n">colocation_stack</span>
      <span class="c1"># pylint: enable=protected-access</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_graph_colocation_stack</span> <span class="o">=</span> <span class="n">colocation_stack</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_control_dependencies_stack</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stack_state_is_thread_local</span><span class="p">:</span>
      <span class="c1"># This may be called from a thread where control_dependencies_stack</span>
      <span class="c1"># doesn&#39;t yet exist.</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="p">,</span> <span class="s2">&quot;_control_dependencies_stack&quot;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_control_dependencies_stack</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_graph_control_dependencies_stack</span><span class="p">[:])</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_control_dependencies_stack</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph_control_dependencies_stack</span>

  <span class="nd">@_control_dependencies_stack</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">_control_dependencies_stack</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">control_dependencies</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stack_state_is_thread_local</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_control_dependencies_stack</span> <span class="o">=</span> <span class="n">control_dependencies</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_graph_control_dependencies_stack</span> <span class="o">=</span> <span class="n">control_dependencies</span>

  <span class="k">def</span> <span class="nf">_mutation_lock</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a lock to guard code that creates &amp; mutates ops.</span>

<span class="sd">    See the comment for self._group_lock for more info.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_group_lock</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="n">_MUTATION_LOCK_GROUP</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_session_run_lock</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a lock to guard code for Session.run.</span>

<span class="sd">    See the comment for self._group_lock for more info.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_group_lock</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="n">_SESSION_RUN_LOCK_GROUP</span><span class="p">)</span>


<span class="c1"># TODO(agarwal): currently device directives in an outer eager scope will not</span>
<span class="c1"># apply to inner graph mode code. Fix that.</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;device&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">device</span><span class="p">(</span><span class="n">device_name_or_function</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Wrapper for `Graph.device()` using the default graph.</span>

<span class="sd">  See</span>
<span class="sd">  @{tf.Graph.device}</span>
<span class="sd">  for more details.</span>

<span class="sd">  Args:</span>
<span class="sd">    device_name_or_function: The device name or function to use in</span>
<span class="sd">      the context.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A context manager that specifies the default device to use for newly</span>
<span class="sd">    created ops.</span>

<span class="sd">  Raises:</span>
<span class="sd">    RuntimeError: If eager execution is enabled and a function is passed in.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
    <span class="c1"># TODO(agarwal): support device functions in EAGER mode.</span>
    <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="n">device_name_or_function</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
          <span class="s2">&quot;tf.device does not support functions when eager execution &quot;</span>
          <span class="s2">&quot;is enabled.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">context</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device_name_or_function</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device_name_or_function</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;container&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">container</span><span class="p">(</span><span class="n">container_name</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Wrapper for `Graph.container()` using the default graph.</span>

<span class="sd">  Args:</span>
<span class="sd">    container_name: The container string to use in the context.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A context manager that specifies the default container to use for newly</span>
<span class="sd">    created stateful ops.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">container</span><span class="p">(</span><span class="n">container_name</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_colocate_with_for_gradient</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">gradient_uid</span><span class="p">,</span> <span class="n">ignore_existing</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">op</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">device</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">_NullContextmanager</span><span class="p">()</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">default_graph</span> <span class="o">=</span> <span class="n">get_default_graph</span><span class="p">()</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">EagerTensor</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">default_graph</span><span class="o">.</span><span class="n">building_function</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">default_graph</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Encountered an Eager-defined Tensor during graph &quot;</span>
                         <span class="s2">&quot;construction, but a function was not being built.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">default_graph</span><span class="o">.</span><span class="n">_colocate_with_for_gradient</span><span class="p">(</span>
        <span class="n">op</span><span class="p">,</span> <span class="n">gradient_uid</span><span class="o">=</span><span class="n">gradient_uid</span><span class="p">,</span> <span class="n">ignore_existing</span><span class="o">=</span><span class="n">ignore_existing</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;colocate_with&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">colocate_with</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">ignore_existing</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">_colocate_with_for_gradient</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">ignore_existing</span><span class="o">=</span><span class="n">ignore_existing</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;control_dependencies&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">control_dependencies</span><span class="p">(</span><span class="n">control_inputs</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Wrapper for `Graph.control_dependencies()` using the default graph.</span>

<span class="sd">  See @{tf.Graph.control_dependencies}</span>
<span class="sd">  for more details.</span>

<span class="sd">  When eager execution is enabled, any callable object in the `control_inputs`</span>
<span class="sd">  list will be called.</span>

<span class="sd">  Args:</span>
<span class="sd">    control_inputs: A list of `Operation` or `Tensor` objects which</span>
<span class="sd">      must be executed or computed before running the operations</span>
<span class="sd">      defined in the context.  Can also be `None` to clear the control</span>
<span class="sd">      dependencies. If eager execution is enabled, any callable object in the</span>
<span class="sd">      `control_inputs` list will be called.</span>

<span class="sd">  Returns:</span>
<span class="sd">   A context manager that specifies control dependencies for all</span>
<span class="sd">   operations constructed within the context.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">control_inputs</span><span class="p">:</span>
      <span class="c1"># Excute any pending callables.</span>
      <span class="k">for</span> <span class="n">control</span> <span class="ow">in</span> <span class="n">control_inputs</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="n">control</span><span class="p">):</span>
          <span class="n">control</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">_NullContextmanager</span><span class="p">()</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">(</span><span class="n">control_inputs</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_DefaultStack</span><span class="p">(</span><span class="n">threading</span><span class="o">.</span><span class="n">local</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A thread-local stack of objects for providing implicit defaults.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">_DefaultStack</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_enforce_nesting</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">stack</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="k">def</span> <span class="nf">get_default</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">stack</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stack</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="k">else</span> <span class="kc">None</span>

  <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">stack</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="k">def</span> <span class="nf">is_cleared</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">stack</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">enforce_nesting</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enforce_nesting</span>

  <span class="nd">@enforce_nesting</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">enforce_nesting</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_enforce_nesting</span> <span class="o">=</span> <span class="n">value</span>

  <span class="nd">@tf_contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
  <span class="k">def</span> <span class="nf">get_controller</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">default</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A context manager for manipulating a default stack.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">default</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">yield</span> <span class="n">default</span>
    <span class="k">finally</span><span class="p">:</span>
      <span class="c1"># stack may be empty if reset() was called</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stack</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enforce_nesting</span><span class="p">:</span>
          <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stack</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">default</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span>
                <span class="s2">&quot;Nesting violated for default stack of </span><span class="si">%s</span><span class="s2"> objects&quot;</span> <span class="o">%</span>
                <span class="nb">type</span><span class="p">(</span><span class="n">default</span><span class="p">))</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">stack</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">stack</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">default</span><span class="p">)</span>


<span class="n">_default_session_stack</span> <span class="o">=</span> <span class="n">_DefaultStack</span><span class="p">()</span>  <span class="c1"># pylint: disable=protected-access</span>


<span class="k">def</span> <span class="nf">default_session</span><span class="p">(</span><span class="n">session</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Python &quot;with&quot; handler for defining a default session.</span>

<span class="sd">  This function provides a means of registering a session for handling</span>
<span class="sd">  Tensor.eval() and Operation.run() calls. It is primarily intended for use</span>
<span class="sd">  by session.Session, but can be used with any object that implements</span>
<span class="sd">  the Session.run() interface.</span>

<span class="sd">  Use with the &quot;with&quot; keyword to specify that Tensor.eval() and Operation.run()</span>
<span class="sd">  invocations within the scope of a block should be executed by a particular</span>
<span class="sd">  session.</span>

<span class="sd">  The default session applies to the current thread only, so it is always</span>
<span class="sd">  possible to inspect the call stack and determine the scope of a default</span>
<span class="sd">  session. If you create a new thread, and wish to use the default session</span>
<span class="sd">  in that thread, you must explicitly add a &quot;with ops.default_session(sess):&quot;</span>
<span class="sd">  block in that thread&#39;s function.</span>

<span class="sd">  Example:</span>
<span class="sd">    The following code examples are equivalent:</span>

<span class="sd">    # 1. Using the Session object directly:</span>
<span class="sd">    sess = ...</span>
<span class="sd">    c = tf.constant(5.0)</span>
<span class="sd">    sess.run(c)</span>

<span class="sd">    # 2. Using default_session():</span>
<span class="sd">    sess = ...</span>
<span class="sd">    with ops.default_session(sess):</span>
<span class="sd">      c = tf.constant(5.0)</span>
<span class="sd">      result = c.eval()</span>

<span class="sd">    # 3. Overriding default_session():</span>
<span class="sd">    sess = ...</span>
<span class="sd">    with ops.default_session(sess):</span>
<span class="sd">      c = tf.constant(5.0)</span>
<span class="sd">      with ops.default_session(...):</span>
<span class="sd">        c.eval(session=sess)</span>

<span class="sd">  Args:</span>
<span class="sd">    session: The session to be installed as the default session.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A context manager for the default session.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">_default_session_stack</span><span class="o">.</span><span class="n">get_controller</span><span class="p">(</span><span class="n">session</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;get_default_session&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">get_default_session</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Returns the default session for the current thread.</span>

<span class="sd">  The returned `Session` will be the innermost session on which a</span>
<span class="sd">  `Session` or `Session.as_default()` context has been entered.</span>

<span class="sd">  NOTE: The default session is a property of the current thread. If you</span>
<span class="sd">  create a new thread, and wish to use the default session in that</span>
<span class="sd">  thread, you must explicitly add a `with sess.as_default():` in that</span>
<span class="sd">  thread&#39;s function.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The default `Session` being used in the current thread.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">_default_session_stack</span><span class="o">.</span><span class="n">get_default</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">_eval_using_default_session</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">,</span> <span class="n">graph</span><span class="p">,</span> <span class="n">session</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Uses the default session to evaluate one or more tensors.</span>

<span class="sd">  Args:</span>
<span class="sd">    tensors: A single Tensor, or a list of Tensor objects.</span>
<span class="sd">    feed_dict: A dictionary that maps Tensor objects (or tensor names) to lists,</span>
<span class="sd">      numpy ndarrays, TensorProtos, or strings.</span>
<span class="sd">    graph: The graph in which the tensors are defined.</span>
<span class="sd">    session: (Optional) A different session to use to evaluate &quot;tensors&quot;.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Either a single numpy ndarray if &quot;tensors&quot; is a single tensor; or a list</span>
<span class="sd">    of numpy ndarrays that each correspond to the respective element in</span>
<span class="sd">    &quot;tensors&quot;.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If no default session is available; the default session</span>
<span class="sd">      does not have &quot;graph&quot; as its graph; or if &quot;session&quot; is specified,</span>
<span class="sd">      and it does not have &quot;graph&quot; as its graph.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">session</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">session</span> <span class="o">=</span> <span class="n">get_default_session</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">session</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot evaluate tensor using `eval()`: No default &quot;</span>
                       <span class="s2">&quot;session is registered. Use `with &quot;</span>
                       <span class="s2">&quot;sess.as_default()` or pass an explicit session to &quot;</span>
                       <span class="s2">&quot;`eval(session=sess)`&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">session</span><span class="o">.</span><span class="n">graph</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">graph</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot use the default session to evaluate tensor: &quot;</span>
                       <span class="s2">&quot;the tensor&#39;s graph is different from the session&#39;s &quot;</span>
                       <span class="s2">&quot;graph. Pass an explicit session to &quot;</span>
                       <span class="s2">&quot;`eval(session=sess)`.&quot;</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">session</span><span class="o">.</span><span class="n">graph</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">graph</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot use the given session to evaluate tensor: &quot;</span>
                       <span class="s2">&quot;the tensor&#39;s graph is different from the session&#39;s &quot;</span>
                       <span class="s2">&quot;graph.&quot;</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_run_using_default_session</span><span class="p">(</span><span class="n">operation</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">,</span> <span class="n">graph</span><span class="p">,</span> <span class="n">session</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Uses the default session to run &quot;operation&quot;.</span>

<span class="sd">  Args:</span>
<span class="sd">    operation: The Operation to be run.</span>
<span class="sd">    feed_dict: A dictionary that maps Tensor objects (or tensor names) to lists,</span>
<span class="sd">      numpy ndarrays, TensorProtos, or strings.</span>
<span class="sd">    graph: The graph in which &quot;operation&quot; is defined.</span>
<span class="sd">    session: (Optional) A different session to use to run &quot;operation&quot;.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If no default session is available; the default session</span>
<span class="sd">      does not have &quot;graph&quot; as its graph; or if &quot;session&quot; is specified,</span>
<span class="sd">      and it does not have &quot;graph&quot; as its graph.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">session</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">session</span> <span class="o">=</span> <span class="n">get_default_session</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">session</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot execute operation using `run()`: No default &quot;</span>
                       <span class="s2">&quot;session is registered. Use `with &quot;</span>
                       <span class="s2">&quot;sess.as_default():` or pass an explicit session to &quot;</span>
                       <span class="s2">&quot;`run(session=sess)`&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">session</span><span class="o">.</span><span class="n">graph</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">graph</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot use the default session to execute operation: &quot;</span>
                       <span class="s2">&quot;the operation&#39;s graph is different from the &quot;</span>
                       <span class="s2">&quot;session&#39;s graph. Pass an explicit session to &quot;</span>
                       <span class="s2">&quot;run(session=sess).&quot;</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">session</span><span class="o">.</span><span class="n">graph</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">graph</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot use the given session to execute operation: &quot;</span>
                       <span class="s2">&quot;the operation&#39;s graph is different from the session&#39;s &quot;</span>
                       <span class="s2">&quot;graph.&quot;</span><span class="p">)</span>
  <span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">operation</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_DefaultGraphStack</span><span class="p">(</span><span class="n">_DefaultStack</span><span class="p">):</span>  <span class="c1"># pylint: disable=protected-access</span>
  <span class="sd">&quot;&quot;&quot;A thread-local stack of objects for providing an implicit default graph.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">_DefaultGraphStack</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_global_default_graph</span> <span class="o">=</span> <span class="kc">None</span>

  <span class="k">def</span> <span class="nf">get_default</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Override that returns a global default if the stack is empty.&quot;&quot;&quot;</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">_DefaultGraphStack</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">get_default</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">ret</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">ret</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_GetGlobalDefaultGraph</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">ret</span>

  <span class="k">def</span> <span class="nf">_GetGlobalDefaultGraph</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_default_graph</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="c1"># TODO(mrry): Perhaps log that the default graph is being used, or set</span>
      <span class="c1">#   provide some other feedback to prevent confusion when a mixture of</span>
      <span class="c1">#   the global default graph and an explicit graph are combined in the</span>
      <span class="c1">#   same process.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_global_default_graph</span> <span class="o">=</span> <span class="n">Graph</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_default_graph</span>

  <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">_DefaultGraphStack</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_global_default_graph</span> <span class="o">=</span> <span class="kc">None</span>

  <span class="nd">@tf_contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
  <span class="k">def</span> <span class="nf">get_controller</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">default</span><span class="p">):</span>
    <span class="n">context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">context_switches</span><span class="o">.</span><span class="n">push</span><span class="p">(</span>
        <span class="n">default</span><span class="o">.</span><span class="n">building_function</span><span class="p">,</span> <span class="n">default</span><span class="o">.</span><span class="n">as_default</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">with</span> <span class="nb">super</span><span class="p">(</span><span class="n">_DefaultGraphStack</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">get_controller</span><span class="p">(</span>
          <span class="n">default</span><span class="p">)</span> <span class="k">as</span> <span class="n">g</span><span class="p">,</span> <span class="n">context</span><span class="o">.</span><span class="n">graph_mode</span><span class="p">():</span>
        <span class="k">yield</span> <span class="n">g</span>
    <span class="k">finally</span><span class="p">:</span>
      <span class="c1"># If an exception is raised here it may be hiding a related exception in</span>
      <span class="c1"># the try-block (just above).</span>
      <span class="n">context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">context_switches</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>


<span class="n">_default_graph_stack</span> <span class="o">=</span> <span class="n">_DefaultGraphStack</span><span class="p">()</span>


<span class="c1"># pylint: disable=g-doc-return-or-yield,line-too-long</span>
<span class="nd">@tf_contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">init_scope</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;A context manager that lifts ops out of control-flow scopes and function-building graphs.</span>

<span class="sd">  There is often a need to lift variable initialization ops out of control-flow</span>
<span class="sd">  scopes, function-building graphs, and gradient tapes. Entering an</span>
<span class="sd">  `init_scope` is a mechanism for satisfying these desiderata. In particular,</span>
<span class="sd">  entering an `init_scope` has three effects:</span>

<span class="sd">    (1) All control dependencies are cleared the moment the scope is entered;</span>
<span class="sd">        this is equivalent to entering the context manager returned from</span>
<span class="sd">        `control_dependencies(None)`, which has the side-effect of exiting</span>
<span class="sd">        control-flow scopes like `tf.cond` and `tf.while_loop`.</span>

<span class="sd">    (2) All operations that are created while the scope is active are lifted</span>
<span class="sd">        into the lowest context on the `context_stack` that is not building a</span>
<span class="sd">        graph function. Here, a context is defined as either a graph or an eager</span>
<span class="sd">        context. Every context switch, i.e., every installation of a graph as</span>
<span class="sd">        the default graph and every switch into eager mode, is logged in a</span>
<span class="sd">        thread-local stack called `context_switches`; the log entry for a</span>
<span class="sd">        context switch is popped from the stack when the context is exited.</span>
<span class="sd">        Entering an `init_scope` is equivalent to crawling up</span>
<span class="sd">        `context_switches`, finding the first context that is not building a</span>
<span class="sd">        graph function, and entering it. A caveat is that if graph mode is</span>
<span class="sd">        enabled but the default graph stack is empty, then entering an</span>
<span class="sd">        `init_scope` will simply install a fresh graph as the default one.</span>

<span class="sd">    (3) The gradient tape is paused while the scope is active.</span>

<span class="sd">  Raises:</span>
<span class="sd">    RuntimeError: if graph state is incompatible with this initialization.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># pylint: enable=g-doc-return-or-yield,line-too-long</span>

  <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
    <span class="c1"># Fastpath.</span>
    <span class="k">with</span> <span class="n">tape</span><span class="o">.</span><span class="n">stop_recording</span><span class="p">():</span>
      <span class="k">yield</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="c1"># Retrieve the active name scope: entering an `init_scope` preserves</span>
    <span class="c1"># the name scope of the current context.</span>
    <span class="n">default_graph</span> <span class="o">=</span> <span class="n">get_default_graph</span><span class="p">()</span>
    <span class="n">scope</span> <span class="o">=</span> <span class="n">default_graph</span><span class="o">.</span><span class="n">get_name_scope</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">scope</span> <span class="ow">and</span> <span class="n">scope</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="s2">&quot;/&quot;</span><span class="p">:</span>
      <span class="c1"># Names that end with trailing slashes are treated by `name_scope` as</span>
      <span class="c1"># absolute.</span>
      <span class="n">scope</span> <span class="o">=</span> <span class="n">scope</span> <span class="o">+</span> <span class="s2">&quot;/&quot;</span>
    <span class="n">inner_device_stack</span> <span class="o">=</span> <span class="n">default_graph</span><span class="o">.</span><span class="n">_device_function_stack</span>  <span class="c1"># pylint: disable=protected-access</span>

    <span class="n">outer_context</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_default_graph_stack</span><span class="o">.</span><span class="n">stack</span><span class="p">:</span>
      <span class="c1"># If the default graph stack is empty, then we cannot be building a</span>
      <span class="c1"># function. Install the global graph (which, in this case, is also the</span>
      <span class="c1"># default graph) as the outer context.</span>
      <span class="k">if</span> <span class="n">default_graph</span><span class="o">.</span><span class="n">building_function</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;The global graph is building a function.&quot;</span><span class="p">)</span>
      <span class="n">outer_context</span> <span class="o">=</span> <span class="n">default_graph</span><span class="o">.</span><span class="n">as_default</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># Find a context that is not building a function.</span>
      <span class="k">for</span> <span class="n">stack_entry</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">context_switches</span><span class="o">.</span><span class="n">stack</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">stack_entry</span><span class="o">.</span><span class="n">is_building_function</span><span class="p">:</span>
          <span class="n">outer_context</span> <span class="o">=</span> <span class="n">stack_entry</span><span class="o">.</span><span class="n">enter_context_fn</span>
          <span class="k">break</span>

      <span class="k">if</span> <span class="n">outer_context</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># As a last resort, obtain the global default graph; this graph doesn&#39;t</span>
        <span class="c1"># necessarily live on the graph stack (and hence it doesn&#39;t necessarily</span>
        <span class="c1"># live on the context stack), but it is stored in the graph stack&#39;s</span>
        <span class="c1"># encapsulating object.</span>
        <span class="n">outer_context</span> <span class="o">=</span> <span class="n">_default_graph_stack</span><span class="o">.</span><span class="n">_GetGlobalDefaultGraph</span><span class="p">()</span><span class="o">.</span><span class="n">as_default</span>  <span class="c1"># pylint: disable=protected-access</span>

    <span class="k">if</span> <span class="n">outer_context</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="c1"># Sanity check; this shouldn&#39;t be triggered.</span>
      <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;All graphs are building functions, and no &quot;</span>
                         <span class="s2">&quot;eager context was previously active.&quot;</span><span class="p">)</span>

    <span class="n">outer_graph</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">outer_device_stack</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">with</span> <span class="n">outer_context</span><span class="p">(),</span> <span class="n">name_scope</span><span class="p">(</span><span class="n">scope</span><span class="p">),</span> <span class="n">control_dependencies</span><span class="p">(</span>
          <span class="kc">None</span><span class="p">),</span> <span class="n">tape</span><span class="o">.</span><span class="n">stop_recording</span><span class="p">():</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
          <span class="c1"># The device stack is preserved when lifting into a graph. Eager</span>
          <span class="c1"># execution doesn&#39;t implement device stacks and in particular it</span>
          <span class="c1"># doesn&#39;t support device functions, so in general it&#39;s not possible</span>
          <span class="c1"># to do the same when lifting into the eager context.</span>
          <span class="n">outer_graph</span> <span class="o">=</span> <span class="n">get_default_graph</span><span class="p">()</span>
          <span class="n">outer_device_stack</span> <span class="o">=</span> <span class="n">outer_graph</span><span class="o">.</span><span class="n">_device_function_stack</span>  <span class="c1"># pylint: disable=protected-access</span>
          <span class="n">outer_graph</span><span class="o">.</span><span class="n">_device_function_stack</span> <span class="o">=</span> <span class="n">inner_device_stack</span>  <span class="c1"># pylint: disable=protected-access</span>
        <span class="k">yield</span>
    <span class="k">finally</span><span class="p">:</span>
      <span class="c1"># If an exception is raised here it may be hiding a related exception in</span>
      <span class="c1"># try-block (just above).</span>
      <span class="k">if</span> <span class="n">outer_graph</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">outer_graph</span><span class="o">.</span><span class="n">_device_function_stack</span> <span class="o">=</span> <span class="n">outer_device_stack</span>  <span class="c1"># pylint: disable=protected-access</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;enable_eager_execution&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">enable_eager_execution</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                           <span class="n">device_policy</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                           <span class="n">execution_mode</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Enables eager execution for the lifetime of this program.</span>

<span class="sd">  Eager execution provides an imperative interface to TensorFlow. With eager</span>
<span class="sd">  execution enabled, TensorFlow functions execute operations immediately (as</span>
<span class="sd">  opposed to adding to a graph to be executed later in a @{tf.Session}) and</span>
<span class="sd">  return concrete values (as opposed to symbolic references to a node in a</span>
<span class="sd">  computational graph).</span>

<span class="sd">  For example:</span>
<span class="sd">  ```python</span>
<span class="sd">  tf.enable_eager_execution()</span>

<span class="sd">  # After eager execution is enabled, operations are executed as they are</span>
<span class="sd">  # defined and Tensor objects hold concrete values, which can be accessed as</span>
<span class="sd">  # numpy.ndarray`s through the numpy() method.</span>
<span class="sd">  assert tf.multiply(6, 7).numpy() == 42</span>
<span class="sd">  ```</span>

<span class="sd">  Eager execution cannot be enabled after TensorFlow APIs have been used to</span>
<span class="sd">  create or execute graphs. It is typically recommended to invoke this function</span>
<span class="sd">  at program startup and not in a library (as most libraries should be usable</span>
<span class="sd">  both with and without eager execution).</span>

<span class="sd">  Args:</span>
<span class="sd">    config: (Optional.) A @{tf.ConfigProto} to use to configure the environment</span>
<span class="sd">      in which operations are executed. Note that @{tf.ConfigProto} is also</span>
<span class="sd">      used to configure graph execution (via @{tf.Session}) and many options</span>
<span class="sd">      within `tf.ConfigProto` are not implemented (or are irrelevant) when</span>
<span class="sd">      eager execution is enabled.</span>
<span class="sd">    device_policy: (Optional.) Policy controlling how operations requiring</span>
<span class="sd">      inputs on a specific device (e.g., a GPU 0) handle inputs on a different</span>
<span class="sd">      device  (e.g. GPU 1 or CPU). When set to None, an appropriate value will be</span>
<span class="sd">      picked automatically. The value picked may change between TensorFlow</span>
<span class="sd">      releases.</span>
<span class="sd">      Valid values:</span>
<span class="sd">      - tf.contrib.eager.DEVICE_PLACEMENT_EXPLICIT: raises an error if the</span>
<span class="sd">        placement is not correct.</span>
<span class="sd">      - tf.contrib.eager.DEVICE_PLACEMENT_WARN: copies the tensors which are not</span>
<span class="sd">        on the right device but logs a warning.</span>
<span class="sd">      - tf.contrib.eager.DEVICE_PLACEMENT_SILENT: silently copies the tensors.</span>
<span class="sd">        Note that this may hide performance problems as there is no notification</span>
<span class="sd">        provided when operations are blocked on the tensor being copied between</span>
<span class="sd">        devices.</span>
<span class="sd">      - tf.contrib.eager.DEVICE_PLACEMENT_SILENT_FOR_INT32: silently copies</span>
<span class="sd">        int32 tensors, raising errors on the other ones.</span>
<span class="sd">    execution_mode: (Optional.) Policy controlling how operations dispatched are</span>
<span class="sd">      actually executed. When set to None, an appropriate value will be picked</span>
<span class="sd">      automatically. The value picked may change between TensorFlow releases.</span>
<span class="sd">      Valid values:</span>
<span class="sd">      - tf.contrib.eager.SYNC: executes each operation synchronously.</span>
<span class="sd">      - tf.contrib.eager.ASYNC: executes each operation asynchronously. These</span>
<span class="sd">        operations may return &quot;non-ready&quot; handles.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If eager execution is enabled after creating/executing a</span>
<span class="sd">     TensorFlow graph, or if options provided conflict with a previous call</span>
<span class="sd">     to this function.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">enable_eager_execution_internal</span><span class="p">(</span>
      <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
      <span class="n">device_policy</span><span class="o">=</span><span class="n">device_policy</span><span class="p">,</span>
      <span class="n">execution_mode</span><span class="o">=</span><span class="n">execution_mode</span><span class="p">,</span>
      <span class="n">server_def</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">enable_eager_execution_internal</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                    <span class="n">device_policy</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                    <span class="n">execution_mode</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                    <span class="n">server_def</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Enables eager execution for the lifetime of this program.</span>

<span class="sd">  Most of the doc string for enable_eager_execution is relevant here as well.</span>
<span class="sd">  Args:</span>
<span class="sd">    config: See enable_eager_execution doc string</span>
<span class="sd">    device_policy: See enable_eager_execution doc string</span>
<span class="sd">    execution_mode: See enable_eager_execution doc string</span>
<span class="sd">    server_def: (Optional.) A tensorflow::ServerDef proto.</span>
<span class="sd">      Enables execution on remote devices. GrpcServers need to be started by</span>
<span class="sd">      creating an identical server_def to this, and setting the appropriate</span>
<span class="sd">      task_indexes, so that the servers can communicate. It will then be</span>
<span class="sd">      possible to execute operations on remote devices.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError</span>

<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">config_pb2</span><span class="o">.</span><span class="n">ConfigProto</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
        <span class="s2">&quot;config must be a tf.ConfigProto, but got </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="n">config</span><span class="p">))</span>
  <span class="k">if</span> <span class="n">device_policy</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">context</span><span class="o">.</span><span class="n">DEVICE_PLACEMENT_EXPLICIT</span><span class="p">,</span>
                           <span class="n">context</span><span class="o">.</span><span class="n">DEVICE_PLACEMENT_WARN</span><span class="p">,</span>
                           <span class="n">context</span><span class="o">.</span><span class="n">DEVICE_PLACEMENT_SILENT</span><span class="p">,</span>
                           <span class="n">context</span><span class="o">.</span><span class="n">DEVICE_PLACEMENT_SILENT_FOR_INT32</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
        <span class="s2">&quot;device_policy must be one of None, tf.contrib.eager.DEVICE_PLACEMENT_*&quot;</span>
    <span class="p">)</span>
  <span class="k">if</span> <span class="n">execution_mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">context</span><span class="o">.</span><span class="n">SYNC</span><span class="p">,</span> <span class="n">context</span><span class="o">.</span><span class="n">ASYNC</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
        <span class="s2">&quot;execution_mode must be one of None, tf.contrib.eager.SYNC, &quot;</span>
        <span class="s2">&quot;tf.contrib.eager.ASYNC&quot;</span><span class="p">)</span>
  <span class="c1"># pylint: disable=protected-access</span>
  <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">_default_mode</span> <span class="o">==</span> <span class="n">context</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">:</span>
    <span class="n">graph_mode_has_been_used</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">_default_session_stack</span><span class="o">.</span><span class="n">stack</span>
        <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_operations</span><span class="p">())</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># pylint: disable=g-explicit-length-test</span>
    <span class="k">if</span> <span class="n">graph_mode_has_been_used</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;tf.enable_eager_execution must be called at program startup.&quot;</span><span class="p">)</span>
  <span class="n">context</span><span class="o">.</span><span class="n">_default_mode</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">EAGER_MODE</span>
  <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">_context</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">context</span><span class="o">.</span><span class="n">_context</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">Context</span><span class="p">(</span>
        <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
        <span class="n">device_policy</span><span class="o">=</span><span class="n">device_policy</span><span class="p">,</span>
        <span class="n">execution_mode</span><span class="o">=</span><span class="n">execution_mode</span><span class="p">,</span>
        <span class="n">server_def</span><span class="o">=</span><span class="n">server_def</span><span class="p">)</span>
  <span class="k">elif</span> <span class="p">((</span><span class="n">config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">_context</span><span class="o">.</span><span class="n">_config</span><span class="p">)</span> <span class="ow">or</span>
        <span class="p">(</span><span class="n">device_policy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span>
         <span class="n">device_policy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">_context</span><span class="o">.</span><span class="n">_device_policy</span><span class="p">)</span> <span class="ow">or</span>
        <span class="p">(</span><span class="n">execution_mode</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span>
         <span class="n">execution_mode</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">_context</span><span class="o">.</span><span class="n">_execution_mode</span><span class="p">)):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Trying to change the options of an active eager&quot;</span>
                     <span class="s2">&quot; execution. Context config: </span><span class="si">%s</span><span class="s2">, specified config:&quot;</span>
                     <span class="s2">&quot; </span><span class="si">%s</span><span class="s2">. Context device policy: </span><span class="si">%s</span><span class="s2">, specified device&quot;</span>
                     <span class="s2">&quot; policy: </span><span class="si">%s</span><span class="s2">. Context execution mode: </span><span class="si">%s</span><span class="s2">, &quot;</span>
                     <span class="s2">&quot; specified execution mode </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span>
                     <span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">_context</span><span class="o">.</span><span class="n">_config</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span>
                      <span class="n">context</span><span class="o">.</span><span class="n">_context</span><span class="o">.</span><span class="n">_device_policy</span><span class="p">,</span> <span class="n">device_policy</span><span class="p">,</span>
                      <span class="n">context</span><span class="o">.</span><span class="n">_context</span><span class="o">.</span><span class="n">_execution_mode</span><span class="p">,</span> <span class="n">execution_mode</span><span class="p">))</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
        <span class="s2">&quot;tf.enable_eager_execution must be called at program startup.&quot;</span><span class="p">)</span>

  <span class="c1"># Monkey patch to get rid of an unnecessary conditional since the context is</span>
  <span class="c1"># now initialized.</span>
  <span class="n">context</span><span class="o">.</span><span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">context_safe</span>


<span class="k">def</span> <span class="nf">eager_run</span><span class="p">(</span><span class="n">main</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">argv</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Runs the program with an optional main function and argv list.</span>

<span class="sd">  The program will run with eager execution enabled.</span>

<span class="sd">  Example:</span>
<span class="sd">  ```python</span>
<span class="sd">  import tensorflow as tf</span>
<span class="sd">  # Import subject to future changes:</span>
<span class="sd">  from tensorflow.contrib.eager.python import tfe</span>

<span class="sd">  def main(_):</span>
<span class="sd">    u = tf.constant(6.0)</span>
<span class="sd">    v = tf.constant(7.0)</span>
<span class="sd">    print(u * v)</span>

<span class="sd">  if __name__ == &quot;__main__&quot;:</span>
<span class="sd">    tfe.run()</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    main: the main function to run.</span>
<span class="sd">    argv: the arguments to pass to it.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">enable_eager_execution</span><span class="p">()</span>
  <span class="n">app</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">main</span><span class="p">,</span> <span class="n">argv</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;reset_default_graph&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">reset_default_graph</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Clears the default graph stack and resets the global default graph.</span>

<span class="sd">  NOTE: The default graph is a property of the current thread. This</span>
<span class="sd">  function applies only to the current thread.  Calling this function while</span>
<span class="sd">  a `tf.Session` or `tf.InteractiveSession` is active will result in undefined</span>
<span class="sd">  behavior. Using any previously created `tf.Operation` or `tf.Tensor` objects</span>
<span class="sd">  after calling this function will result in undefined behavior.</span>
<span class="sd">  Raises:</span>
<span class="sd">    AssertionError: If this function is called within a nested graph.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">_default_graph_stack</span><span class="o">.</span><span class="n">is_cleared</span><span class="p">():</span>
    <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;Do not use tf.reset_default_graph() to clear &quot;</span>
                         <span class="s2">&quot;nested graphs. If you need a cleared graph, &quot;</span>
                         <span class="s2">&quot;exit the nesting and create a new graph.&quot;</span><span class="p">)</span>
  <span class="n">_default_graph_stack</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;get_default_graph&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">get_default_graph</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Returns the default graph for the current thread.</span>

<span class="sd">  The returned graph will be the innermost graph on which a</span>
<span class="sd">  `Graph.as_default()` context has been entered, or a global default</span>
<span class="sd">  graph if none has been explicitly created.</span>

<span class="sd">  NOTE: The default graph is a property of the current thread. If you</span>
<span class="sd">  create a new thread, and wish to use the default graph in that</span>
<span class="sd">  thread, you must explicitly add a `with g.as_default():` in that</span>
<span class="sd">  thread&#39;s function.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The default `Graph` being used in the current thread.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">_default_graph_stack</span><span class="o">.</span><span class="n">get_default</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">has_default_graph</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Returns True if there is a default graph.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">_default_graph_stack</span><span class="o">.</span><span class="n">stack</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">1</span>


<span class="k">def</span> <span class="nf">get_name_scope</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Returns the current name scope in the default_graph.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  with tf.name_scope(&#39;scope1&#39;):</span>
<span class="sd">    with tf.name_scope(&#39;scope2&#39;):</span>
<span class="sd">      print(tf.get_name_scope())</span>
<span class="sd">  ```</span>
<span class="sd">  would print the string `scope1/scope2`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A string representing the current name scope.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">scope_name</span><span class="o">.</span><span class="n">rstrip</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_name_scope</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">_assert_same_graph</span><span class="p">(</span><span class="n">original_item</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Fail if the 2 items are from different graphs.</span>

<span class="sd">  Args:</span>
<span class="sd">    original_item: Original item to check against.</span>
<span class="sd">    item: Item to check.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: if graphs do not match.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">original_item</span><span class="o">.</span><span class="n">graph</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">item</span><span class="o">.</span><span class="n">graph</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> must be from the same graph as </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">item</span><span class="p">,</span>
                                                                <span class="n">original_item</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">_get_graph_from_inputs</span><span class="p">(</span><span class="n">op_input_list</span><span class="p">,</span> <span class="n">graph</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns the appropriate graph to use for the given inputs.</span>

<span class="sd">  This library method provides a consistent algorithm for choosing the graph</span>
<span class="sd">  in which an Operation should be constructed:</span>

<span class="sd">  1. If the default graph is being used to construct a function, we</span>
<span class="sd">     use the default graph.</span>
<span class="sd">  2. If the &quot;graph&quot; is specified explicitly, we validate that all of the inputs</span>
<span class="sd">     in &quot;op_input_list&quot; are compatible with that graph.</span>
<span class="sd">  3. Otherwise, we attempt to select a graph from the first Operation-</span>
<span class="sd">     or Tensor-valued input in &quot;op_input_list&quot;, and validate that all other</span>
<span class="sd">     such inputs are in the same graph.</span>
<span class="sd">  4. If the graph was not specified and it could not be inferred from</span>
<span class="sd">     &quot;op_input_list&quot;, we attempt to use the default graph.</span>

<span class="sd">  Args:</span>
<span class="sd">    op_input_list: A list of inputs to an operation, which may include `Tensor`,</span>
<span class="sd">      `Operation`, and other objects that may be converted to a graph element.</span>
<span class="sd">    graph: (Optional) The explicit graph to use.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If op_input_list is not a list or tuple, or if graph is not a</span>
<span class="sd">      Graph.</span>
<span class="sd">    ValueError: If a graph is explicitly passed and not all inputs are from it,</span>
<span class="sd">      or if the inputs are from multiple graphs, or we could not find a graph</span>
<span class="sd">      and there was no default graph.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The appropriate graph to use for the given inputs.</span>

<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">building_function</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">get_default_graph</span><span class="p">()</span>

  <span class="n">op_input_list</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">op_input_list</span><span class="p">)</span>  <span class="c1"># Handle generators correctly</span>
  <span class="k">if</span> <span class="n">graph</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">Graph</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Input graph needs to be a Graph: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">graph</span><span class="p">)</span>

  <span class="c1"># 1. We validate that all of the inputs are from the same graph. This is</span>
  <span class="c1">#    either the supplied graph parameter, or the first one selected from one</span>
  <span class="c1">#    the graph-element-valued inputs. In the latter case, we hold onto</span>
  <span class="c1">#    that input in original_graph_element so we can provide a more</span>
  <span class="c1">#    informative error if a mismatch is found.</span>
  <span class="n">original_graph_element</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="k">for</span> <span class="n">op_input</span> <span class="ow">in</span> <span class="n">op_input_list</span><span class="p">:</span>
    <span class="c1"># Determine if this is a valid graph_element.</span>
    <span class="c1"># TODO(josh11b): Note that we exclude subclasses of Tensor. Need to clean this</span>
    <span class="c1"># up.</span>
    <span class="n">graph_element</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">op_input</span><span class="p">,</span> <span class="p">(</span><span class="n">Operation</span><span class="p">,</span> <span class="n">_TensorLike</span><span class="p">))</span> <span class="ow">and</span>
        <span class="p">((</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op_input</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">))</span> <span class="ow">or</span> <span class="nb">type</span><span class="p">(</span><span class="n">op_input</span><span class="p">)</span> <span class="o">==</span> <span class="n">Tensor</span><span class="p">)):</span>  <span class="c1"># pylint: disable=unidiomatic-typecheck</span>
      <span class="n">graph_element</span> <span class="o">=</span> <span class="n">op_input</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">graph_element</span> <span class="o">=</span> <span class="n">_as_graph_element</span><span class="p">(</span><span class="n">op_input</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">graph_element</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">graph</span><span class="p">:</span>
        <span class="n">original_graph_element</span> <span class="o">=</span> <span class="n">graph_element</span>
        <span class="n">graph</span> <span class="o">=</span> <span class="n">graph_element</span><span class="o">.</span><span class="n">graph</span>
      <span class="k">elif</span> <span class="n">original_graph_element</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_assert_same_graph</span><span class="p">(</span><span class="n">original_graph_element</span><span class="p">,</span> <span class="n">graph_element</span><span class="p">)</span>
      <span class="k">elif</span> <span class="n">graph_element</span><span class="o">.</span><span class="n">graph</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">graph</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> is not from the passed-in graph.&quot;</span> <span class="o">%</span> <span class="n">graph_element</span><span class="p">)</span>

  <span class="c1"># 2. If all else fails, we use the default graph, which is always there.</span>
  <span class="k">return</span> <span class="n">graph</span> <span class="ow">or</span> <span class="n">get_default_graph</span><span class="p">()</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;GraphKeys&quot;</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">GraphKeys</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Standard names to use for graph collections.</span>

<span class="sd">  The standard library uses various well-known names to collect and</span>
<span class="sd">  retrieve values associated with a graph. For example, the</span>
<span class="sd">  `tf.Optimizer` subclasses default to optimizing the variables</span>
<span class="sd">  collected under `tf.GraphKeys.TRAINABLE_VARIABLES` if none is</span>
<span class="sd">  specified, but it is also possible to pass an explicit list of</span>
<span class="sd">  variables.</span>

<span class="sd">  The following standard keys are defined:</span>

<span class="sd">  * `GLOBAL_VARIABLES`: the default collection of `Variable` objects, shared</span>
<span class="sd">    across distributed environment (model variables are subset of these). See</span>
<span class="sd">    @{tf.global_variables}</span>
<span class="sd">    for more details.</span>
<span class="sd">    Commonly, all `TRAINABLE_VARIABLES` variables will be in `MODEL_VARIABLES`,</span>
<span class="sd">    and all `MODEL_VARIABLES` variables will be in `GLOBAL_VARIABLES`.</span>
<span class="sd">  * `LOCAL_VARIABLES`: the subset of `Variable` objects that are local to each</span>
<span class="sd">    machine. Usually used for temporarily variables, like counters.</span>
<span class="sd">    Note: use `tf.contrib.framework.local_variable` to add to this collection.</span>
<span class="sd">  * `MODEL_VARIABLES`: the subset of `Variable` objects that are used in the</span>
<span class="sd">    model for inference (feed forward). Note: use</span>
<span class="sd">    `tf.contrib.framework.model_variable` to add to this collection.</span>
<span class="sd">  * `TRAINABLE_VARIABLES`: the subset of `Variable` objects that will</span>
<span class="sd">    be trained by an optimizer. See</span>
<span class="sd">    @{tf.trainable_variables}</span>
<span class="sd">    for more details.</span>
<span class="sd">  * `SUMMARIES`: the summary `Tensor` objects that have been created in the</span>
<span class="sd">    graph. See</span>
<span class="sd">    @{tf.summary.merge_all}</span>
<span class="sd">    for more details.</span>
<span class="sd">  * `QUEUE_RUNNERS`: the `QueueRunner` objects that are used to</span>
<span class="sd">    produce input for a computation. See</span>
<span class="sd">    @{tf.train.start_queue_runners}</span>
<span class="sd">    for more details.</span>
<span class="sd">  * `MOVING_AVERAGE_VARIABLES`: the subset of `Variable` objects that will also</span>
<span class="sd">    keep moving averages.  See</span>
<span class="sd">    @{tf.moving_average_variables}</span>
<span class="sd">    for more details.</span>
<span class="sd">  * `REGULARIZATION_LOSSES`: regularization losses collected during graph</span>
<span class="sd">    construction.</span>

<span class="sd">  The following standard keys are _defined_, but their collections are **not**</span>
<span class="sd">  automatically populated as many of the others are:</span>

<span class="sd">  * `WEIGHTS`</span>
<span class="sd">  * `BIASES`</span>
<span class="sd">  * `ACTIVATIONS`</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="c1"># Key to collect Variable objects that are global (shared across machines).</span>
  <span class="c1"># Default collection for all variables, except local ones.</span>
  <span class="n">GLOBAL_VARIABLES</span> <span class="o">=</span> <span class="s2">&quot;variables&quot;</span>
  <span class="c1"># Key to collect local variables that are local to the machine and are not</span>
  <span class="c1"># saved/restored.</span>
  <span class="n">LOCAL_VARIABLES</span> <span class="o">=</span> <span class="s2">&quot;local_variables&quot;</span>
  <span class="c1"># Key to collect local variables which are used to accumulate interal state</span>
  <span class="c1"># to be used in tf.metrics.*.</span>
  <span class="n">METRIC_VARIABLES</span> <span class="o">=</span> <span class="s2">&quot;metric_variables&quot;</span>
  <span class="c1"># Key to collect model variables defined by layers.</span>
  <span class="n">MODEL_VARIABLES</span> <span class="o">=</span> <span class="s2">&quot;model_variables&quot;</span>
  <span class="c1"># Key to collect Variable objects that will be trained by the</span>
  <span class="c1"># optimizers.</span>
  <span class="n">TRAINABLE_VARIABLES</span> <span class="o">=</span> <span class="s2">&quot;trainable_variables&quot;</span>
  <span class="c1"># Key to collect summaries.</span>
  <span class="n">SUMMARIES</span> <span class="o">=</span> <span class="s2">&quot;summaries&quot;</span>
  <span class="c1"># Key to collect QueueRunners.</span>
  <span class="n">QUEUE_RUNNERS</span> <span class="o">=</span> <span class="s2">&quot;queue_runners&quot;</span>
  <span class="c1"># Key to collect table initializers.</span>
  <span class="n">TABLE_INITIALIZERS</span> <span class="o">=</span> <span class="s2">&quot;table_initializer&quot;</span>
  <span class="c1"># Key to collect asset filepaths. An asset represents an external resource</span>
  <span class="c1"># like a vocabulary file.</span>
  <span class="n">ASSET_FILEPATHS</span> <span class="o">=</span> <span class="s2">&quot;asset_filepaths&quot;</span>
  <span class="c1"># Key to collect Variable objects that keep moving averages.</span>
  <span class="n">MOVING_AVERAGE_VARIABLES</span> <span class="o">=</span> <span class="s2">&quot;moving_average_variables&quot;</span>
  <span class="c1"># Key to collect regularization losses at graph construction.</span>
  <span class="n">REGULARIZATION_LOSSES</span> <span class="o">=</span> <span class="s2">&quot;regularization_losses&quot;</span>
  <span class="c1"># Key to collect concatenated sharded variables.</span>
  <span class="n">CONCATENATED_VARIABLES</span> <span class="o">=</span> <span class="s2">&quot;concatenated_variables&quot;</span>
  <span class="c1"># Key to collect savers.</span>
  <span class="n">SAVERS</span> <span class="o">=</span> <span class="s2">&quot;savers&quot;</span>
  <span class="c1"># Key to collect weights</span>
  <span class="n">WEIGHTS</span> <span class="o">=</span> <span class="s2">&quot;weights&quot;</span>
  <span class="c1"># Key to collect biases</span>
  <span class="n">BIASES</span> <span class="o">=</span> <span class="s2">&quot;biases&quot;</span>
  <span class="c1"># Key to collect activations</span>
  <span class="n">ACTIVATIONS</span> <span class="o">=</span> <span class="s2">&quot;activations&quot;</span>
  <span class="c1"># Key to collect update_ops</span>
  <span class="n">UPDATE_OPS</span> <span class="o">=</span> <span class="s2">&quot;update_ops&quot;</span>
  <span class="c1"># Key to collect losses</span>
  <span class="n">LOSSES</span> <span class="o">=</span> <span class="s2">&quot;losses&quot;</span>
  <span class="c1"># Key to collect BaseSaverBuilder.SaveableObject instances for checkpointing.</span>
  <span class="n">SAVEABLE_OBJECTS</span> <span class="o">=</span> <span class="s2">&quot;saveable_objects&quot;</span>
  <span class="c1"># Key to collect all shared resources used by the graph which need to be</span>
  <span class="c1"># initialized once per cluster.</span>
  <span class="n">RESOURCES</span> <span class="o">=</span> <span class="s2">&quot;resources&quot;</span>
  <span class="c1"># Key to collect all shared resources used in this graph which need to be</span>
  <span class="c1"># initialized once per session.</span>
  <span class="n">LOCAL_RESOURCES</span> <span class="o">=</span> <span class="s2">&quot;local_resources&quot;</span>
  <span class="c1"># Trainable resource-style variables.</span>
  <span class="n">TRAINABLE_RESOURCE_VARIABLES</span> <span class="o">=</span> <span class="s2">&quot;trainable_resource_variables&quot;</span>

  <span class="c1"># Key to indicate various ops.</span>
  <span class="n">INIT_OP</span> <span class="o">=</span> <span class="s2">&quot;init_op&quot;</span>
  <span class="n">LOCAL_INIT_OP</span> <span class="o">=</span> <span class="s2">&quot;local_init_op&quot;</span>
  <span class="n">READY_OP</span> <span class="o">=</span> <span class="s2">&quot;ready_op&quot;</span>
  <span class="n">READY_FOR_LOCAL_INIT_OP</span> <span class="o">=</span> <span class="s2">&quot;ready_for_local_init_op&quot;</span>
  <span class="n">SUMMARY_OP</span> <span class="o">=</span> <span class="s2">&quot;summary_op&quot;</span>
  <span class="n">GLOBAL_STEP</span> <span class="o">=</span> <span class="s2">&quot;global_step&quot;</span>

  <span class="c1"># Used to count the number of evaluations performed during a single evaluation</span>
  <span class="c1"># run.</span>
  <span class="n">EVAL_STEP</span> <span class="o">=</span> <span class="s2">&quot;eval_step&quot;</span>
  <span class="n">TRAIN_OP</span> <span class="o">=</span> <span class="s2">&quot;train_op&quot;</span>

  <span class="c1"># Key for control flow context.</span>
  <span class="n">COND_CONTEXT</span> <span class="o">=</span> <span class="s2">&quot;cond_context&quot;</span>
  <span class="n">WHILE_CONTEXT</span> <span class="o">=</span> <span class="s2">&quot;while_context&quot;</span>

  <span class="c1"># Used to store v2 summary names.</span>
  <span class="n">_SUMMARY_COLLECTION</span> <span class="o">=</span> <span class="s2">&quot;_SUMMARY_V2&quot;</span>

  <span class="c1"># List of all collections that keep track of variables.</span>
  <span class="n">_VARIABLE_COLLECTIONS</span> <span class="o">=</span> <span class="p">[</span>
      <span class="n">GLOBAL_VARIABLES</span><span class="p">,</span>
      <span class="n">LOCAL_VARIABLES</span><span class="p">,</span>
      <span class="n">METRIC_VARIABLES</span><span class="p">,</span>
      <span class="n">MODEL_VARIABLES</span><span class="p">,</span>
      <span class="n">TRAINABLE_VARIABLES</span><span class="p">,</span>
      <span class="n">MOVING_AVERAGE_VARIABLES</span><span class="p">,</span>
      <span class="n">CONCATENATED_VARIABLES</span><span class="p">,</span>
      <span class="n">TRAINABLE_RESOURCE_VARIABLES</span><span class="p">,</span>
  <span class="p">]</span>

  <span class="c1"># Key for streaming model ports.</span>
  <span class="c1"># NOTE(yuanbyu): internal and experimental.</span>
  <span class="n">_STREAMING_MODEL_PORTS</span> <span class="o">=</span> <span class="s2">&quot;streaming_model_ports&quot;</span>

  <span class="nd">@decorator_utils</span><span class="o">.</span><span class="n">classproperty</span>
  <span class="k">def</span> <span class="nf">VARIABLES</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>  <span class="c1"># pylint: disable=no-self-argument</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">log_first_n</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">WARN</span><span class="p">,</span>
                        <span class="s2">&quot;VARIABLES collection name is deprecated, please use &quot;</span>
                        <span class="s2">&quot;GLOBAL_VARIABLES instead; VARIABLES will be removed &quot;</span>
                        <span class="s2">&quot;after 2017-03-02.&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">GLOBAL_VARIABLES</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;add_to_collection&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">add_to_collection</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Wrapper for `Graph.add_to_collection()` using the default graph.</span>

<span class="sd">  See @{tf.Graph.add_to_collection}</span>
<span class="sd">  for more details.</span>

<span class="sd">  Args:</span>
<span class="sd">    name: The key for the collection. For example, the `GraphKeys` class</span>
<span class="sd">      contains many standard names for collections.</span>
<span class="sd">    value: The value to add to the collection.</span>

<span class="sd">  @compatibility(eager)</span>
<span class="sd">  Collections are only supported in eager when variables are created inside an</span>
<span class="sd">  EagerVariableStore (e.g. as part of a layer or template).</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;add_to_collections&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">add_to_collections</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Wrapper for `Graph.add_to_collections()` using the default graph.</span>

<span class="sd">  See @{tf.Graph.add_to_collections}</span>
<span class="sd">  for more details.</span>

<span class="sd">  Args:</span>
<span class="sd">    names: The key for the collections. The `GraphKeys` class</span>
<span class="sd">      contains many standard names for collections.</span>
<span class="sd">    value: The value to add to the collections.</span>

<span class="sd">  @compatibility(eager)</span>
<span class="sd">  Collections are only supported in eager when variables are created inside an</span>
<span class="sd">  EagerVariableStore (e.g. as part of a layer or template).</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">add_to_collections</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;get_collection_ref&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">get_collection_ref</span><span class="p">(</span><span class="n">key</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Wrapper for `Graph.get_collection_ref()` using the default graph.</span>

<span class="sd">  See @{tf.Graph.get_collection_ref}</span>
<span class="sd">  for more details.</span>

<span class="sd">  Args:</span>
<span class="sd">    key: The key for the collection. For example, the `GraphKeys` class</span>
<span class="sd">      contains many standard names for collections.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The list of values in the collection with the given `name`, or an empty</span>
<span class="sd">    list if no value has been added to that collection.  Note that this returns</span>
<span class="sd">    the collection list itself, which can be modified in place to change the</span>
<span class="sd">    collection.</span>

<span class="sd">  @compatibility(eager)</span>
<span class="sd">  Collections are not supported when eager execution is enabled.</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_collection_ref</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;get_collection&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">get_collection</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Wrapper for `Graph.get_collection()` using the default graph.</span>

<span class="sd">  See @{tf.Graph.get_collection}</span>
<span class="sd">  for more details.</span>

<span class="sd">  Args:</span>
<span class="sd">    key: The key for the collection. For example, the `GraphKeys` class</span>
<span class="sd">      contains many standard names for collections.</span>
<span class="sd">    scope: (Optional.) If supplied, the resulting list is filtered to include</span>
<span class="sd">      only items whose `name` attribute matches using `re.match`. Items</span>
<span class="sd">      without a `name` attribute are never returned if a scope is supplied and</span>
<span class="sd">      the choice or `re.match` means that a `scope` without special tokens</span>
<span class="sd">      filters by prefix.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The list of values in the collection with the given `name`, or</span>
<span class="sd">    an empty list if no value has been added to that collection. The</span>
<span class="sd">    list contains the values in the order under which they were</span>
<span class="sd">    collected.</span>

<span class="sd">  @compatibility(eager)</span>
<span class="sd">  Collections are not supported when eager execution is enabled.</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">scope</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">get_all_collection_keys</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Returns a list of collections used in the default graph.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_all_collection_keys</span><span class="p">()</span>


<span class="n">name_scope_cache</span> <span class="o">=</span> <span class="p">{}</span>


<span class="c1"># Named like a function for backwards compatibility with the</span>
<span class="c1"># @tf_contextlib.contextmanager version, which was switched to a class to avoid</span>
<span class="c1"># some object creation overhead.</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;name_scope&quot;</span><span class="p">,</span> <span class="s2">&quot;keras.backend.name_scope&quot;</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">name_scope</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="sd">&quot;&quot;&quot;A context manager for use when defining a Python op.</span>

<span class="sd">  This context manager validates that the given `values` are from the</span>
<span class="sd">  same graph, makes that graph the default graph, and pushes a</span>
<span class="sd">  name scope in that graph (see</span>
<span class="sd">  @{tf.Graph.name_scope}</span>
<span class="sd">  for more details on that).</span>

<span class="sd">  For example, to define a new Python op called `my_op`:</span>

<span class="sd">  ```python</span>
<span class="sd">  def my_op(a, b, c, name=None):</span>
<span class="sd">    with tf.name_scope(name, &quot;MyOp&quot;, [a, b, c]) as scope:</span>
<span class="sd">      a = tf.convert_to_tensor(a, name=&quot;a&quot;)</span>
<span class="sd">      b = tf.convert_to_tensor(b, name=&quot;b&quot;)</span>
<span class="sd">      c = tf.convert_to_tensor(c, name=&quot;c&quot;)</span>
<span class="sd">      # Define some computation that uses `a`, `b`, and `c`.</span>
<span class="sd">      return foo_op(..., name=scope)</span>
<span class="sd">  ```</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">default_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initialize the context manager.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: The name argument that is passed to the op function.</span>
<span class="sd">      default_name: The default name to use if the `name` argument is `None`.</span>
<span class="sd">      values: The list of `Tensor` arguments that are passed to the op function.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="n">default_name</span> <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">name</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_default_name</span> <span class="o">=</span> <span class="n">default_name</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_values</span> <span class="o">=</span> <span class="n">values</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ctx</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_in_eager_mode</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ctx</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">__enter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Start the scope block.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The scope name.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: if neither `name` nor `default_name` is provided</span>
<span class="sd">        but `values` are.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_in_eager_mode</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_old_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ctx</span><span class="o">.</span><span class="n">scope_name</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="p">:</span>
        <span class="n">scope_name</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">cache_key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_old_name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_default_name</span>
        <span class="k">if</span> <span class="n">cache_key</span> <span class="ow">in</span> <span class="n">name_scope_cache</span><span class="p">:</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_ctx</span><span class="o">.</span><span class="n">scope_name</span> <span class="o">=</span> <span class="n">name_scope_cache</span><span class="p">[</span><span class="n">cache_key</span><span class="p">]</span>
          <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ctx</span><span class="o">.</span><span class="n">scope_name</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;/&quot;</span><span class="p">:</span>
          <span class="c1"># A trailing slash breaks out of nested name scopes, indicating a</span>
          <span class="c1"># fully specified scope name, for compatibility with Graph.name_scope.</span>
          <span class="n">scope_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">name_with_trailing_slash</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">+</span> <span class="s2">&quot;/&quot;</span>
          <span class="n">scope_name</span> <span class="o">=</span> <span class="p">(</span>
              <span class="bp">self</span><span class="o">.</span><span class="n">_old_name</span> <span class="o">+</span> <span class="n">name_with_trailing_slash</span>
              <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_old_name</span> <span class="k">else</span> <span class="n">name_with_trailing_slash</span><span class="p">)</span>
        <span class="n">name_scope_cache</span><span class="p">[</span><span class="n">cache_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">scope_name</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_ctx</span><span class="o">.</span><span class="n">scope_name</span> <span class="o">=</span> <span class="n">scope_name</span>
      <span class="k">return</span> <span class="n">scope_name</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># We only raise an error if values is not None (provided) because</span>
        <span class="c1"># currently tf.name_scope(None) (values=None then) is sometimes used as</span>
        <span class="c1"># an idiom to reset to top scope.</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;At least one of name (</span><span class="si">%s</span><span class="s2">) and default_name (</span><span class="si">%s</span><span class="s2">) must be provided.&quot;</span>
            <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_default_name</span><span class="p">))</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_values</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_values</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="n">g</span> <span class="o">=</span> <span class="n">_get_graph_from_inputs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_values</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_g_manager</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">as_default</span><span class="p">()</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_g_manager</span><span class="o">.</span><span class="fm">__enter__</span><span class="p">()</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_name_scope</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name_scope</span><span class="o">.</span><span class="fm">__enter__</span><span class="p">()</span>
      <span class="k">except</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_g_manager</span><span class="o">.</span><span class="fm">__exit__</span><span class="p">(</span><span class="o">*</span><span class="n">sys</span><span class="o">.</span><span class="n">exc_info</span><span class="p">())</span>
        <span class="k">raise</span>

  <span class="k">def</span> <span class="nf">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">type_arg</span><span class="p">,</span> <span class="n">value_arg</span><span class="p">,</span> <span class="n">traceback_arg</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_in_eager_mode</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_ctx</span><span class="o">.</span><span class="n">scope_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_old_name</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_name_scope</span><span class="o">.</span><span class="fm">__exit__</span><span class="p">(</span><span class="n">type_arg</span><span class="p">,</span> <span class="n">value_arg</span><span class="p">,</span> <span class="n">traceback_arg</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_g_manager</span><span class="o">.</span><span class="fm">__exit__</span><span class="p">(</span><span class="n">type_arg</span><span class="p">,</span> <span class="n">value_arg</span><span class="p">,</span> <span class="n">traceback_arg</span><span class="p">)</span>
    <span class="k">return</span> <span class="kc">False</span>  <span class="c1"># False values do not suppress exceptions</span>


<span class="k">def</span> <span class="nf">strip_name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">export_scope</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Removes name scope from a name.</span>

<span class="sd">  Args:</span>
<span class="sd">    name: A `string` name.</span>
<span class="sd">    export_scope: Optional `string`. Name scope to remove.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Name with name scope removed, or the original name if export_scope</span>
<span class="sd">    is None.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">export_scope</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">export_scope</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;/&quot;</span><span class="p">:</span>
      <span class="n">export_scope</span> <span class="o">=</span> <span class="n">export_scope</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">try</span><span class="p">:</span>
      <span class="c1"># Strips export_scope/, export_scope///,</span>
      <span class="c1"># ^export_scope/, loc:@export_scope/.</span>
      <span class="n">str_to_replace</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;([\^]|loc:@|^)&quot;</span> <span class="o">+</span> <span class="n">export_scope</span> <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;[\/]+(.*)&quot;</span>
      <span class="k">return</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">str_to_replace</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;\1\2&quot;</span><span class="p">,</span> <span class="n">compat</span><span class="o">.</span><span class="n">as_str</span><span class="p">(</span><span class="n">name</span><span class="p">),</span> <span class="n">count</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">TypeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="c1"># If the name is not of a type we can process, simply return it.</span>
      <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">name</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">name</span>


<span class="k">def</span> <span class="nf">prepend_name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">import_scope</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Prepends name scope to a name.</span>

<span class="sd">  Args:</span>
<span class="sd">    name: A `string` name.</span>
<span class="sd">    import_scope: Optional `string`. Name scope to add.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Name with name scope added, or the original name if import_scope</span>
<span class="sd">    is None.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">import_scope</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">import_scope</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;/&quot;</span><span class="p">:</span>
      <span class="n">import_scope</span> <span class="o">=</span> <span class="n">import_scope</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">try</span><span class="p">:</span>
      <span class="n">str_to_replace</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;([\^]|loc:@|^)(.*)&quot;</span>
      <span class="k">return</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">str_to_replace</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;\1&quot;</span> <span class="o">+</span> <span class="n">import_scope</span> <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;/\2&quot;</span><span class="p">,</span>
                    <span class="n">compat</span><span class="o">.</span><span class="n">as_str</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="k">except</span> <span class="ne">TypeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="c1"># If the name is not of a type we can process, simply return it.</span>
      <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">name</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">name</span>


<span class="c1"># pylint: disable=g-doc-return-or-yield</span>
<span class="c1"># pylint: disable=not-context-manager</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;op_scope&quot;</span><span class="p">)</span>
<span class="nd">@tf_contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">op_scope</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">default_name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;DEPRECATED. Same as name_scope above, just different argument order.&quot;&quot;&quot;</span>
  <span class="n">logging</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;tf.op_scope(values, name, default_name) is deprecated,&quot;</span>
               <span class="s2">&quot; use tf.name_scope(name, default_name, values)&quot;</span><span class="p">)</span>
  <span class="k">with</span> <span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">default_name</span><span class="o">=</span><span class="n">default_name</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">values</span><span class="p">)</span> <span class="k">as</span> <span class="n">scope</span><span class="p">:</span>
    <span class="k">yield</span> <span class="n">scope</span>


<span class="n">_proto_function_registry</span> <span class="o">=</span> <span class="n">registry</span><span class="o">.</span><span class="n">Registry</span><span class="p">(</span><span class="s2">&quot;proto functions&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">register_proto_function</span><span class="p">(</span><span class="n">collection_name</span><span class="p">,</span>
                            <span class="n">proto_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                            <span class="n">to_proto</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                            <span class="n">from_proto</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Registers `to_proto` and `from_proto` functions for collection_name.</span>

<span class="sd">  `to_proto` function converts a Python object to the corresponding protocol</span>
<span class="sd">  buffer, and returns the protocol buffer.</span>

<span class="sd">  `from_proto` function converts protocol buffer into a Python object, and</span>
<span class="sd">  returns the object..</span>

<span class="sd">  Args:</span>
<span class="sd">    collection_name: Name of the collection.</span>
<span class="sd">    proto_type: Protobuf type, such as `saver_pb2.SaverDef`,</span>
<span class="sd">      `variable_pb2.VariableDef`, `queue_runner_pb2.QueueRunnerDef`..</span>
<span class="sd">    to_proto: Function that implements Python object to protobuf conversion.</span>
<span class="sd">    from_proto: Function that implements protobuf to Python object conversion.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">to_proto</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="n">to_proto</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;to_proto must be callable.&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">from_proto</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="n">from_proto</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;from_proto must be callable.&quot;</span><span class="p">)</span>

  <span class="n">_proto_function_registry</span><span class="o">.</span><span class="n">register</span><span class="p">((</span><span class="n">proto_type</span><span class="p">,</span> <span class="n">to_proto</span><span class="p">,</span> <span class="n">from_proto</span><span class="p">),</span>
                                    <span class="n">collection_name</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">get_collection_proto_type</span><span class="p">(</span><span class="n">collection_name</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns the proto_type for collection_name.&quot;&quot;&quot;</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">_proto_function_registry</span><span class="o">.</span><span class="n">lookup</span><span class="p">(</span><span class="n">collection_name</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
  <span class="k">except</span> <span class="ne">LookupError</span><span class="p">:</span>
    <span class="k">return</span> <span class="kc">None</span>


<span class="k">def</span> <span class="nf">get_to_proto_function</span><span class="p">(</span><span class="n">collection_name</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns the to_proto function for collection_name.&quot;&quot;&quot;</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">_proto_function_registry</span><span class="o">.</span><span class="n">lookup</span><span class="p">(</span><span class="n">collection_name</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
  <span class="k">except</span> <span class="ne">LookupError</span><span class="p">:</span>
    <span class="k">return</span> <span class="kc">None</span>


<span class="k">def</span> <span class="nf">get_from_proto_function</span><span class="p">(</span><span class="n">collection_name</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns the from_proto function for collection_name.&quot;&quot;&quot;</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">_proto_function_registry</span><span class="o">.</span><span class="n">lookup</span><span class="p">(</span><span class="n">collection_name</span><span class="p">)[</span><span class="mi">2</span><span class="p">]</span>
  <span class="k">except</span> <span class="ne">LookupError</span><span class="p">:</span>
    <span class="k">return</span> <span class="kc">None</span>


<span class="k">def</span> <span class="nf">_operation_conversion_error</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">as_ref</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Produce a nice error if someone converts an Operation to a Tensor.&quot;&quot;&quot;</span>
  <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">((</span><span class="s2">&quot;Can&#39;t convert Operation &#39;</span><span class="si">%s</span><span class="s2">&#39; to Tensor &quot;</span>
                   <span class="s2">&quot;(target dtype=</span><span class="si">%r</span><span class="s2">, name=</span><span class="si">%r</span><span class="s2">, as_ref=</span><span class="si">%r</span><span class="s2">)&quot;</span><span class="p">)</span> <span class="o">%</span> <span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span>
                                                               <span class="n">name</span><span class="p">,</span> <span class="n">as_ref</span><span class="p">))</span>


<span class="n">register_tensor_conversion_function</span><span class="p">(</span><span class="n">Operation</span><span class="p">,</span> <span class="n">_operation_conversion_error</span><span class="p">)</span>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../../../',
            VERSION:'',
            LANGUAGE:'en',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../../../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>