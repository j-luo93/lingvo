

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>lingvo.core.attention_test &mdash; lingvo  documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 

  
  <script src="../../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../../index.html" class="icon icon-home"> lingvo
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../lingvo.html">lingvo package</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">lingvo</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../index.html">Module code</a> &raquo;</li>
        
      <li>lingvo.core.attention_test</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for lingvo.core.attention_test</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2018 The TensorFlow Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ==============================================================================</span>
<span class="sd">&quot;&quot;&quot;Tests for attention.&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">math</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">six.moves</span> <span class="k">import</span> <span class="nb">range</span>
<span class="kn">from</span> <span class="nn">six.moves</span> <span class="k">import</span> <span class="nb">zip</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="k">import</span> <span class="n">attention</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="k">import</span> <span class="n">py_utils</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="k">import</span> <span class="n">test_utils</span>


<div class="viewcode-block" id="AttentionTest"><a class="viewcode-back" href="../../../lingvo.core.attention_test.html#lingvo.core.attention_test.AttentionTest">[docs]</a><span class="k">class</span> <span class="nc">AttentionTest</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">TestCase</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Test attention models.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">_AdditiveAttentionInputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">packed_inputs</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">tgt_bs</span><span class="o">=</span><span class="mi">6</span><span class="p">):</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">12345</span><span class="p">)</span>
    <span class="n">source_vecs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">source_contexts</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span>
            <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
    <span class="n">source_segment_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span>
            <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
    <span class="n">query_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">tgt_bs</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">qsi</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
    <span class="n">query_segment_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">qsi</span><span class="p">[:</span><span class="n">tgt_bs</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="n">params</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">AdditiveAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">params</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;atten&#39;</span>
    <span class="n">params</span><span class="o">.</span><span class="n">params_init</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Gaussian</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">12345</span><span class="p">)</span>
    <span class="n">params</span><span class="o">.</span><span class="n">source_dim</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="n">params</span><span class="o">.</span><span class="n">query_dim</span> <span class="o">=</span> <span class="mi">7</span>
    <span class="n">params</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">7</span>
    <span class="n">params</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">global_vn</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">params</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">per_step_vn</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">params</span><span class="o">.</span><span class="n">packed_input</span> <span class="o">=</span> <span class="n">packed_inputs</span>
    <span class="n">tensors</span> <span class="o">=</span> <span class="p">(</span><span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span> <span class="n">source_segment_id</span><span class="p">,</span>
               <span class="n">query_vec</span><span class="p">,</span> <span class="n">query_segment_id</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">params</span><span class="p">,</span> <span class="n">tensors</span>

<div class="viewcode-block" id="AttentionTest.testAdditiveAttention"><a class="viewcode-back" href="../../../lingvo.core.attention_test.html#lingvo.core.attention_test.AttentionTest.testAdditiveAttention">[docs]</a>  <span class="k">def</span> <span class="nf">testAdditiveAttention</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="p">(</span><span class="n">use_gpu</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
      <span class="n">params</span><span class="p">,</span> <span class="n">tensors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AdditiveAttentionInputs</span><span class="p">()</span>
      <span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tensors</span>
      <span class="n">atten</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">AdditiveAttention</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">atten</span><span class="o">.</span><span class="n">vars</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()),</span> <span class="mi">3</span><span class="p">)</span>
      <span class="n">atten</span><span class="o">.</span><span class="n">InitForSourcePacked</span><span class="p">(</span><span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span>
                                <span class="n">source_padding</span><span class="p">)</span>
      <span class="n">atten_vec</span><span class="p">,</span> <span class="n">atten_prob</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">ComputeContextVector</span><span class="p">(</span>
          <span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">)</span>

      <span class="c1"># TODO(yonghui): add atten.vars for the variables attention model</span>
      <span class="c1"># declares.</span>
      <span class="n">atten_vars</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="s1">&#39;AdditiveAttention_vars&#39;</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">atten_vars</span><span class="p">))</span>

      <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>

      <span class="n">all_vars</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">all_variables</span><span class="p">()</span>
      <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">all_vars</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">eval</span><span class="p">())</span>

      <span class="n">atten_vec_out</span><span class="p">,</span> <span class="n">prob_out</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">atten_vec</span><span class="p">,</span> <span class="n">atten_prob</span><span class="p">])</span>

      <span class="nb">print</span><span class="p">([</span><span class="s1">&#39;additive attention prob_out&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array_repr</span><span class="p">(</span><span class="n">prob_out</span><span class="p">)])</span>
      <span class="nb">print</span><span class="p">([</span><span class="s1">&#39;additive attention atten_vec_out&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array_repr</span><span class="p">(</span><span class="n">atten_vec_out</span><span class="p">)])</span>

      <span class="c1"># pyformat: disable</span>
      <span class="c1"># pylint: disable=bad-whitespace</span>
      <span class="n">expected_prob_out</span> <span class="o">=</span> <span class="p">[</span>
          <span class="p">[</span><span class="mf">0.2555742</span> <span class="p">,</span>  <span class="mf">0.24073002</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>  <span class="mf">0.25412574</span><span class="p">,</span>
           <span class="mf">0.24957004</span><span class="p">],</span>
          <span class="p">[</span><span class="mf">0.</span>        <span class="p">,</span>  <span class="mf">0.25394136</span><span class="p">,</span>  <span class="mf">0.24764746</span><span class="p">,</span>  <span class="mf">0.25480017</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>
           <span class="mf">0.24361098</span><span class="p">],</span>
          <span class="p">[</span><span class="mf">0.25094295</span><span class="p">,</span>  <span class="mf">0.2499937</span> <span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>  <span class="mf">0.24308342</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>
           <span class="mf">0.25597993</span><span class="p">],</span>
          <span class="p">[</span><span class="mf">0.25559244</span><span class="p">,</span>  <span class="mf">0.24070661</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>  <span class="mf">0.25412717</span><span class="p">,</span>
           <span class="mf">0.24957375</span><span class="p">],</span>
          <span class="p">[</span><span class="mf">0.</span>        <span class="p">,</span>  <span class="mf">0.25393167</span><span class="p">,</span>  <span class="mf">0.24765188</span><span class="p">,</span>  <span class="mf">0.25481117</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>
           <span class="mf">0.24360526</span><span class="p">],</span>
          <span class="p">[</span><span class="mf">0.25113183</span><span class="p">,</span>  <span class="mf">0.24990553</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>  <span class="mf">0.24246082</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>
           <span class="mf">0.25650182</span><span class="p">]]</span>

      <span class="n">expected_atten_vec_out</span> <span class="o">=</span> <span class="p">[</span>
          <span class="p">[</span><span class="mf">0.49745506</span><span class="p">,</span>  <span class="mf">0.63471669</span><span class="p">,</span>  <span class="mf">0.49220526</span><span class="p">,</span>  <span class="mf">0.5683012</span> <span class="p">,</span>  <span class="mf">0.42753702</span><span class="p">],</span>
          <span class="p">[</span><span class="mf">0.51502365</span><span class="p">,</span>  <span class="mf">0.56183743</span><span class="p">,</span>  <span class="mf">0.37644109</span><span class="p">,</span>  <span class="mf">0.87425125</span><span class="p">,</span>  <span class="mf">0.46182787</span><span class="p">],</span>
          <span class="p">[</span><span class="mf">0.57862502</span><span class="p">,</span>  <span class="mf">0.44246522</span><span class="p">,</span>  <span class="mf">0.36931852</span><span class="p">,</span>  <span class="mf">0.41002905</span><span class="p">,</span>  <span class="mf">0.14327194</span><span class="p">],</span>
          <span class="p">[</span><span class="mf">0.49745634</span><span class="p">,</span>  <span class="mf">0.63471717</span><span class="p">,</span>  <span class="mf">0.49220967</span><span class="p">,</span>  <span class="mf">0.56829125</span><span class="p">,</span>  <span class="mf">0.4275257</span> <span class="p">],</span>
          <span class="p">[</span><span class="mf">0.51501834</span><span class="p">,</span>  <span class="mf">0.56183696</span><span class="p">,</span>  <span class="mf">0.37644821</span><span class="p">,</span>  <span class="mf">0.87425053</span><span class="p">,</span>  <span class="mf">0.46182543</span><span class="p">],</span>
          <span class="p">[</span><span class="mf">0.57893348</span><span class="p">,</span>  <span class="mf">0.44248882</span><span class="p">,</span>  <span class="mf">0.36938411</span><span class="p">,</span>  <span class="mf">0.41006744</span><span class="p">,</span>  <span class="mf">0.14328158</span><span class="p">]]</span>
      <span class="c1"># pylint: enable=bad-whitespace</span>
      <span class="c1"># pyformat: enable</span>

      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">expected_prob_out</span><span class="p">,</span> <span class="n">prob_out</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">expected_atten_vec_out</span><span class="p">,</span> <span class="n">atten_vec_out</span><span class="p">)</span></div>

<div class="viewcode-block" id="AttentionTest.testAdditiveAttentionWithPackedInputs"><a class="viewcode-back" href="../../../lingvo.core.attention_test.html#lingvo.core.attention_test.AttentionTest.testAdditiveAttentionWithPackedInputs">[docs]</a>  <span class="k">def</span> <span class="nf">testAdditiveAttentionWithPackedInputs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="p">(</span><span class="n">use_gpu</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
      <span class="n">params</span><span class="p">,</span> <span class="n">tensors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AdditiveAttentionInputs</span><span class="p">(</span><span class="n">packed_inputs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="p">(</span><span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span> <span class="n">source_segment_id</span><span class="p">,</span>
       <span class="n">query_vec</span><span class="p">,</span> <span class="n">query_segment_id</span><span class="p">)</span> <span class="o">=</span> <span class="n">tensors</span>
      <span class="n">atten</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">AdditiveAttention</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">atten</span><span class="o">.</span><span class="n">vars</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()),</span> <span class="mi">3</span><span class="p">)</span>
      <span class="n">atten</span><span class="o">.</span><span class="n">InitForSourcePacked</span><span class="p">(</span><span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span>
                                <span class="n">source_padding</span><span class="p">,</span> <span class="n">source_segment_id</span><span class="p">)</span>
      <span class="n">atten_vec</span><span class="p">,</span> <span class="n">atten_prob</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">ComputeContextVector</span><span class="p">(</span>
          <span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">query_segment_id</span><span class="o">=</span><span class="n">query_segment_id</span><span class="p">)</span>

      <span class="c1"># TODO(yonghui): add atten.vars for the variables attention model</span>
      <span class="c1"># declares.</span>
      <span class="n">atten_vars</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="s1">&#39;AdditiveAttention_vars&#39;</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">atten_vars</span><span class="p">))</span>

      <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>

      <span class="n">all_vars</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">all_variables</span><span class="p">()</span>
      <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">all_vars</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">eval</span><span class="p">())</span>

      <span class="n">atten_vec_out</span><span class="p">,</span> <span class="n">prob_out</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">atten_vec</span><span class="p">,</span> <span class="n">atten_prob</span><span class="p">])</span>

      <span class="nb">print</span><span class="p">([</span><span class="s1">&#39;packed additive attention prob_out&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array_repr</span><span class="p">(</span><span class="n">prob_out</span><span class="p">)])</span>
      <span class="nb">print</span><span class="p">([</span>
          <span class="s1">&#39;packed additive attention atten_vec_out&#39;</span><span class="p">,</span>
          <span class="n">np</span><span class="o">.</span><span class="n">array_repr</span><span class="p">(</span><span class="n">atten_vec_out</span><span class="p">)</span>
      <span class="p">])</span>

      <span class="c1"># pyformat: disable</span>
      <span class="c1"># pylint: disable=bad-whitespace</span>
      <span class="n">expected_prob_out</span> <span class="o">=</span> <span class="p">[</span>
          <span class="p">[</span><span class="mf">0.51495469</span><span class="p">,</span> <span class="mf">0.48504525</span><span class="p">,</span> <span class="mi">0</span>         <span class="p">,</span> <span class="mi">0</span>         <span class="p">,</span> <span class="mi">0</span>         <span class="p">,</span>
           <span class="mi">0</span>        <span class="p">],</span>
          <span class="p">[</span><span class="mi">0</span>         <span class="p">,</span> <span class="mi">0</span>         <span class="p">,</span> <span class="mf">0.49288213</span><span class="p">,</span> <span class="mf">0.50711787</span><span class="p">,</span> <span class="mi">0</span>         <span class="p">,</span>
           <span class="mi">0</span>        <span class="p">],</span>
          <span class="p">[</span><span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.5070073</span> <span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.4929927</span> <span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span>
           <span class="mi">0</span>        <span class="p">],</span>
          <span class="p">[</span><span class="mf">0.</span>        <span class="p">,</span> <span class="mi">0</span>         <span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.50451994</span><span class="p">,</span>
           <span class="mf">0.49548006</span><span class="p">],</span>
          <span class="p">[</span><span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span>
           <span class="mi">1</span>        <span class="p">],</span>
          <span class="p">[</span><span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span>
           <span class="mi">1</span>        <span class="p">]]</span>

      <span class="n">expected_atten_vec_out</span> <span class="o">=</span> <span class="p">[</span>
          <span class="p">[</span><span class="mf">0.35256192</span><span class="p">,</span>  <span class="mf">0.68348885</span><span class="p">,</span>  <span class="mf">0.41128731</span><span class="p">,</span>  <span class="mf">0.48906463</span><span class="p">,</span>  <span class="mf">0.50537711</span><span class="p">],</span>
          <span class="p">[</span><span class="mf">0.45880911</span><span class="p">,</span>  <span class="mf">0.6068666</span> <span class="p">,</span>  <span class="mf">0.59867024</span><span class="p">,</span>  <span class="mf">0.82797134</span><span class="p">,</span>  <span class="mf">0.33504993</span><span class="p">],</span>
          <span class="p">[</span><span class="mf">0.54934788</span><span class="p">,</span>  <span class="mf">0.50335771</span><span class="p">,</span>  <span class="mf">0.26117462</span><span class="p">,</span>  <span class="mf">0.32834488</span><span class="p">,</span>  <span class="mf">0.16398546</span><span class="p">],</span>
          <span class="p">[</span><span class="mf">0.64022166</span><span class="p">,</span>  <span class="mf">0.58665955</span><span class="p">,</span>  <span class="mf">0.571935</span>  <span class="p">,</span>  <span class="mf">0.64637613</span><span class="p">,</span>  <span class="mf">0.35084069</span><span class="p">],</span>
          <span class="p">[</span><span class="mf">0.27927336</span><span class="p">,</span>  <span class="mf">0.06444023</span><span class="p">,</span>  <span class="mf">0.19862361</span><span class="p">,</span>  <span class="mf">0.93168277</span><span class="p">,</span>  <span class="mf">0.85441357</span><span class="p">],</span>
          <span class="p">[</span><span class="mf">0.95473474</span><span class="p">,</span>  <span class="mf">0.05225335</span><span class="p">,</span>  <span class="mf">0.57947171</span><span class="p">,</span>  <span class="mf">0.48049626</span><span class="p">,</span>  <span class="mf">0.02170898</span><span class="p">]]</span>
      <span class="c1"># pylint: enable=bad-whitespace</span>
      <span class="c1"># pyformat: enable</span>

      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">expected_prob_out</span><span class="p">,</span> <span class="n">prob_out</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">expected_atten_vec_out</span><span class="p">,</span> <span class="n">atten_vec_out</span><span class="p">)</span></div>

<div class="viewcode-block" id="AttentionTest.testAdditiveAttentionDeterministicDropout"><a class="viewcode-back" href="../../../lingvo.core.attention_test.html#lingvo.core.attention_test.AttentionTest.testAdditiveAttentionDeterministicDropout">[docs]</a>  <span class="k">def</span> <span class="nf">testAdditiveAttentionDeterministicDropout</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="p">(</span><span class="n">use_gpu</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
      <span class="n">params</span><span class="p">,</span> <span class="n">tensors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AdditiveAttentionInputs</span><span class="p">()</span>
      <span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tensors</span>
      <span class="n">params</span><span class="o">.</span><span class="n">atten_dropout_prob</span> <span class="o">=</span> <span class="mf">0.5</span>
      <span class="n">params</span><span class="o">.</span><span class="n">atten_dropout_deterministic</span> <span class="o">=</span> <span class="kc">True</span>
      <span class="n">params</span><span class="o">.</span><span class="n">random_seed</span> <span class="o">=</span> <span class="mi">78924</span>

      <span class="n">atten</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">AdditiveAttention</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">atten</span><span class="o">.</span><span class="n">vars</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()),</span> <span class="mi">3</span><span class="p">)</span>
      <span class="n">atten</span><span class="o">.</span><span class="n">InitForSourcePacked</span><span class="p">(</span><span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span>
                                <span class="n">source_padding</span><span class="p">)</span>
      <span class="n">step_state</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
          <span class="n">global_step</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">GetOrCreateGlobalStep</span><span class="p">(),</span>
          <span class="n">time_step</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">))</span>
      <span class="n">atten_vec</span><span class="p">,</span> <span class="n">atten_prob</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">ComputeContextVector</span><span class="p">(</span>
          <span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">step_state</span><span class="o">=</span><span class="n">step_state</span><span class="p">)</span>

      <span class="n">atten_vars</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="s1">&#39;AdditiveAttention_vars&#39;</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">atten_vars</span><span class="p">))</span>

      <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>

      <span class="n">all_vars</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">all_variables</span><span class="p">()</span>
      <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">all_vars</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">eval</span><span class="p">())</span>

      <span class="n">atten_vec_out</span><span class="p">,</span> <span class="n">prob_out</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">atten_vec</span><span class="p">,</span> <span class="n">atten_prob</span><span class="p">])</span>

      <span class="nb">print</span><span class="p">([</span><span class="s1">&#39;additive attention prob_out&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array_repr</span><span class="p">(</span><span class="n">prob_out</span><span class="p">)])</span>
      <span class="nb">print</span><span class="p">([</span><span class="s1">&#39;additive attention atten_vec_out&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array_repr</span><span class="p">(</span><span class="n">atten_vec_out</span><span class="p">)])</span>

      <span class="c1"># pyformat: disable</span>
      <span class="c1"># pylint: disable=bad-whitespace</span>
      <span class="n">expected_prob_out</span> <span class="o">=</span> <span class="p">[</span>
          <span class="p">[</span> <span class="mf">0.511148</span><span class="p">,</span> <span class="mf">0.48146</span> <span class="p">,</span> <span class="mf">0.</span>      <span class="p">,</span> <span class="mf">0.</span>      <span class="p">,</span> <span class="mf">0.</span>      <span class="p">,</span> <span class="mf">0.</span>      <span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.</span>      <span class="p">,</span> <span class="mf">0.</span>      <span class="p">,</span> <span class="mf">0.</span>      <span class="p">,</span> <span class="mf">0.5096</span>  <span class="p">,</span> <span class="mf">0.</span>      <span class="p">,</span> <span class="mf">0.</span>      <span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.</span>      <span class="p">,</span> <span class="mf">0.499987</span><span class="p">,</span> <span class="mf">0.</span>      <span class="p">,</span> <span class="mf">0.486167</span><span class="p">,</span> <span class="mf">0.</span>      <span class="p">,</span> <span class="mf">0.</span>      <span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.511185</span><span class="p">,</span> <span class="mf">0.</span>      <span class="p">,</span> <span class="mf">0.</span>      <span class="p">,</span> <span class="mf">0.</span>      <span class="p">,</span> <span class="mf">0.508254</span><span class="p">,</span> <span class="mf">0.</span>      <span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.</span>      <span class="p">,</span> <span class="mf">0.507863</span><span class="p">,</span> <span class="mf">0.</span>      <span class="p">,</span> <span class="mf">0.509622</span><span class="p">,</span> <span class="mf">0.</span>      <span class="p">,</span> <span class="mf">0.487210</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.502264</span><span class="p">,</span> <span class="mf">0.499811</span><span class="p">,</span> <span class="mf">0.</span>      <span class="p">,</span> <span class="mf">0.</span>      <span class="p">,</span> <span class="mf">0.</span>      <span class="p">,</span> <span class="mf">0.513004</span><span class="p">]]</span>

      <span class="n">expected_atten_vec_out</span> <span class="o">=</span> <span class="p">[</span>
          <span class="p">[</span> <span class="mf">0.34995595</span><span class="p">,</span> <span class="mf">0.67843682</span><span class="p">,</span> <span class="mf">0.40824726</span><span class="p">,</span> <span class="mf">0.4854497</span> <span class="p">,</span> <span class="mf">0.50164163</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0367727</span> <span class="p">,</span> <span class="mf">0.28920242</span><span class="p">,</span> <span class="mf">0.31352815</span><span class="p">,</span> <span class="mf">0.47981232</span><span class="p">,</span> <span class="mf">0.2116693</span> <span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.54174173</span><span class="p">,</span> <span class="mf">0.49638829</span><span class="p">,</span> <span class="mf">0.25755844</span><span class="p">,</span> <span class="mf">0.32379869</span><span class="p">,</span> <span class="mf">0.16171494</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.5326106</span> <span class="p">,</span> <span class="mf">0.72949529</span><span class="p">,</span> <span class="mf">0.6285308</span> <span class="p">,</span> <span class="mf">0.32593822</span><span class="p">,</span> <span class="mf">0.13290128</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.60574675</span><span class="p">,</span> <span class="mf">0.80303317</span><span class="p">,</span> <span class="mf">0.46481857</span><span class="p">,</span> <span class="mf">1.39628267</span><span class="p">,</span> <span class="mf">0.79862785</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.02963436</span><span class="p">,</span> <span class="mf">0.8377496</span> <span class="p">,</span> <span class="mf">0.5031718</span> <span class="p">,</span> <span class="mf">0.59480983</span><span class="p">,</span> <span class="mf">0.27213222</span><span class="p">]]</span>
      <span class="c1"># pylint: enable=bad-whitespace</span>
      <span class="c1"># pyformat: enable</span>

      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">expected_prob_out</span><span class="p">,</span> <span class="n">prob_out</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">expected_atten_vec_out</span><span class="p">,</span> <span class="n">atten_vec_out</span><span class="p">)</span></div>

  <span class="k">def</span> <span class="nf">_testSameBatchSize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">same_batch_size</span><span class="p">,</span> <span class="n">packed_inputs</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">g</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">set_random_seed</span><span class="p">(</span><span class="mi">398847392</span><span class="p">)</span>
      <span class="n">params</span><span class="p">,</span> <span class="n">tensors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AdditiveAttentionInputs</span><span class="p">(</span><span class="n">packed_inputs</span><span class="p">,</span> <span class="n">tgt_bs</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
      <span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tensors</span>
      <span class="n">params</span><span class="o">.</span><span class="n">same_batch_size</span> <span class="o">=</span> <span class="n">same_batch_size</span>

      <span class="n">atten</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">AdditiveAttention</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
      <span class="n">atten</span><span class="o">.</span><span class="n">InitForSourcePacked</span><span class="p">(</span><span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span>
                                <span class="n">source_padding</span><span class="p">)</span>
      <span class="n">atten_vec</span><span class="p">,</span> <span class="n">atten_prob</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">ComputeContextVector</span><span class="p">(</span>
          <span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">)</span>

      <span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">atten</span><span class="o">.</span><span class="n">vars</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()))</span>

    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="p">(</span><span class="n">use_gpu</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">graph</span><span class="o">=</span><span class="n">g</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
      <span class="n">atten_vec_out</span><span class="p">,</span> <span class="n">prob_out</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">atten_vec</span><span class="p">,</span> <span class="n">atten_prob</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">atten_vec_out</span><span class="p">,</span> <span class="n">prob_out</span>

<div class="viewcode-block" id="AttentionTest.testAdditiveAttentionSameBatchSize"><a class="viewcode-back" href="../../../lingvo.core.attention_test.html#lingvo.core.attention_test.AttentionTest.testAdditiveAttentionSameBatchSize">[docs]</a>  <span class="k">def</span> <span class="nf">testAdditiveAttentionSameBatchSize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">vec0</span><span class="p">,</span> <span class="n">prob0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_testSameBatchSize</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">vec1</span><span class="p">,</span> <span class="n">prob1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_testSameBatchSize</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">vec0</span><span class="p">,</span> <span class="n">vec1</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">prob0</span><span class="p">,</span> <span class="n">prob1</span><span class="p">)</span></div>

<div class="viewcode-block" id="AttentionTest.testAdditiveAttentionSameBatchSizePackedInputs"><a class="viewcode-back" href="../../../lingvo.core.attention_test.html#lingvo.core.attention_test.AttentionTest.testAdditiveAttentionSameBatchSizePackedInputs">[docs]</a>  <span class="k">def</span> <span class="nf">testAdditiveAttentionSameBatchSizePackedInputs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">vec0</span><span class="p">,</span> <span class="n">prob0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_testSameBatchSize</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="n">vec1</span><span class="p">,</span> <span class="n">prob1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_testSameBatchSize</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">vec0</span><span class="p">,</span> <span class="n">vec1</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">prob0</span><span class="p">,</span> <span class="n">prob1</span><span class="p">)</span></div>

<div class="viewcode-block" id="AttentionTest.testAdditiveAttentionSmallerHiddenLayer"><a class="viewcode-back" href="../../../lingvo.core.attention_test.html#lingvo.core.attention_test.AttentionTest.testAdditiveAttentionSmallerHiddenLayer">[docs]</a>  <span class="k">def</span> <span class="nf">testAdditiveAttentionSmallerHiddenLayer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="p">(</span><span class="n">use_gpu</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
      <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">12345</span><span class="p">)</span>
      <span class="n">source_vecs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
      <span class="n">source_contexts</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
      <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span>
              <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span>
              <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
      <span class="n">query_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

      <span class="n">params</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">AdditiveAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
      <span class="n">params</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;atten&#39;</span>
      <span class="n">params</span><span class="o">.</span><span class="n">params_init</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Gaussian</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">12345</span><span class="p">)</span>
      <span class="n">params</span><span class="o">.</span><span class="n">source_dim</span> <span class="o">=</span> <span class="mi">4</span>
      <span class="n">params</span><span class="o">.</span><span class="n">query_dim</span> <span class="o">=</span> <span class="mi">7</span>
      <span class="n">params</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">5</span>
      <span class="n">params</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">global_vn</span> <span class="o">=</span> <span class="kc">False</span>
      <span class="n">params</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">per_step_vn</span> <span class="o">=</span> <span class="kc">False</span>

      <span class="n">atten</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">AdditiveAttention</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
      <span class="n">atten</span><span class="o">.</span><span class="n">InitForSourcePacked</span><span class="p">(</span><span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span>
                                <span class="n">source_padding</span><span class="p">)</span>
      <span class="n">atten_vec</span><span class="p">,</span> <span class="n">atten_prob</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">ComputeContextVector</span><span class="p">(</span>
          <span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">)</span>

      <span class="n">atten_vars</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="s1">&#39;AdditiveAttention_vars&#39;</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">atten_vars</span><span class="p">))</span>

      <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>

      <span class="n">all_vars</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">all_variables</span><span class="p">()</span>
      <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">all_vars</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">eval</span><span class="p">())</span>

      <span class="n">atten_vec_out</span><span class="p">,</span> <span class="n">prob_out</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">atten_vec</span><span class="p">,</span> <span class="n">atten_prob</span><span class="p">])</span>

      <span class="nb">print</span><span class="p">([</span><span class="s1">&#39;prob_out smaller hidden layer&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array_repr</span><span class="p">(</span><span class="n">prob_out</span><span class="p">)])</span>
      <span class="nb">print</span><span class="p">(</span>
          <span class="p">[</span><span class="s1">&#39;atten_vec_out smaller hidden layer&#39;</span><span class="p">,</span>
           <span class="n">np</span><span class="o">.</span><span class="n">array_repr</span><span class="p">(</span><span class="n">atten_vec_out</span><span class="p">)])</span>

      <span class="c1"># pyformat: disable</span>
      <span class="c1"># pylint: disable=bad-whitespace</span>
      <span class="n">expected_prob_out</span> <span class="o">=</span> <span class="p">[</span>
          <span class="p">[</span><span class="mf">0.25242305</span><span class="p">,</span>  <span class="mf">0.24356601</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>  <span class="mf">0.25346902</span><span class="p">,</span>
           <span class="mf">0.25054196</span><span class="p">],</span>
          <span class="p">[</span><span class="mf">0.</span>        <span class="p">,</span>  <span class="mf">0.25230604</span><span class="p">,</span>  <span class="mf">0.24693871</span><span class="p">,</span>  <span class="mf">0.25406054</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>
           <span class="mf">0.24669473</span><span class="p">],</span>
          <span class="p">[</span><span class="mf">0.2501823</span> <span class="p">,</span>  <span class="mf">0.24922216</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>  <span class="mf">0.24693316</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>
           <span class="mf">0.25366238</span><span class="p">],</span>
          <span class="p">[</span><span class="mf">0.25267059</span><span class="p">,</span>  <span class="mf">0.24300526</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>  <span class="mf">0.25369659</span><span class="p">,</span>
           <span class="mf">0.25062758</span><span class="p">],</span>
          <span class="p">[</span><span class="mf">0.</span>        <span class="p">,</span>  <span class="mf">0.25272119</span><span class="p">,</span>  <span class="mf">0.24642748</span><span class="p">,</span>  <span class="mf">0.25435579</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>
           <span class="mf">0.24649554</span><span class="p">],</span>
          <span class="p">[</span><span class="mf">0.25044653</span><span class="p">,</span>  <span class="mf">0.24924593</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>  <span class="mf">0.24560687</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>
           <span class="mf">0.25470066</span><span class="p">]]</span>

      <span class="n">expected_atten_vec_out</span> <span class="o">=</span> <span class="p">[</span>
          <span class="p">[</span><span class="mf">0.49746257</span><span class="p">,</span>  <span class="mf">0.63428223</span><span class="p">,</span>  <span class="mf">0.4914251</span> <span class="p">,</span>  <span class="mf">0.57035601</span><span class="p">,</span>  <span class="mf">0.42964566</span><span class="p">],</span>
          <span class="p">[</span><span class="mf">0.51383036</span><span class="p">,</span>  <span class="mf">0.55960417</span><span class="p">,</span>  <span class="mf">0.37601081</span><span class="p">,</span>  <span class="mf">0.87443453</span><span class="p">,</span>  <span class="mf">0.46342701</span><span class="p">],</span>
          <span class="p">[</span><span class="mf">0.57660079</span><span class="p">,</span>  <span class="mf">0.44147781</span><span class="p">,</span>  <span class="mf">0.36953348</span><span class="p">,</span>  <span class="mf">0.41017395</span><span class="p">,</span>  <span class="mf">0.14293665</span><span class="p">],</span>
          <span class="p">[</span><span class="mf">0.49755943</span><span class="p">,</span>  <span class="mf">0.63429612</span><span class="p">,</span>  <span class="mf">0.49157569</span><span class="p">,</span>  <span class="mf">0.57015073</span><span class="p">,</span>  <span class="mf">0.42933062</span><span class="p">],</span>
          <span class="p">[</span><span class="mf">0.51371205</span><span class="p">,</span>  <span class="mf">0.55982226</span><span class="p">,</span>  <span class="mf">0.37590009</span><span class="p">,</span>  <span class="mf">0.87454152</span><span class="p">,</span>  <span class="mf">0.4633899</span> <span class="p">],</span>
          <span class="p">[</span><span class="mf">0.57732767</span><span class="p">,</span>  <span class="mf">0.44161472</span><span class="p">,</span>  <span class="mf">0.36958888</span><span class="p">,</span>  <span class="mf">0.41019297</span><span class="p">,</span>  <span class="mf">0.14298658</span><span class="p">]]</span>
      <span class="c1"># pylint: enable=bad-whitespace</span>
      <span class="c1"># pyformat: enable</span>

      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">expected_prob_out</span><span class="p">,</span> <span class="n">prob_out</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">expected_atten_vec_out</span><span class="p">,</span> <span class="n">atten_vec_out</span><span class="p">)</span></div>

<div class="viewcode-block" id="AttentionTest.testAdditiveAttentionFp16NoNaN"><a class="viewcode-back" href="../../../lingvo.core.attention_test.html#lingvo.core.attention_test.AttentionTest.testAdditiveAttentionFp16NoNaN">[docs]</a>  <span class="k">def</span> <span class="nf">testAdditiveAttentionFp16NoNaN</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="p">(</span><span class="n">use_gpu</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
      <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">12345</span><span class="p">)</span>
      <span class="n">source_vecs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
      <span class="n">source_contexts</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
      <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span>
              <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span>
              <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float16</span><span class="p">))</span>
      <span class="n">query_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>

      <span class="n">params</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">AdditiveAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
      <span class="n">params</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float16</span>
      <span class="n">params</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;atten&#39;</span>
      <span class="n">params</span><span class="o">.</span><span class="n">params_init</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Gaussian</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">12345</span><span class="p">)</span>
      <span class="n">params</span><span class="o">.</span><span class="n">source_dim</span> <span class="o">=</span> <span class="mi">4</span>
      <span class="n">params</span><span class="o">.</span><span class="n">query_dim</span> <span class="o">=</span> <span class="mi">7</span>
      <span class="n">params</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">7</span>
      <span class="n">params</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">global_vn</span> <span class="o">=</span> <span class="kc">False</span>
      <span class="n">params</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">per_step_vn</span> <span class="o">=</span> <span class="kc">False</span>

      <span class="n">atten</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">AdditiveAttention</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">atten</span><span class="o">.</span><span class="n">vars</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()),</span> <span class="mi">3</span><span class="p">)</span>
      <span class="n">atten</span><span class="o">.</span><span class="n">InitForSourcePacked</span><span class="p">(</span><span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span>
                                <span class="n">source_padding</span><span class="p">)</span>
      <span class="n">atten_vec</span><span class="p">,</span> <span class="n">atten_prob</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">ComputeContextVector</span><span class="p">(</span>
          <span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">)</span>

      <span class="c1"># TODO(yonghui): add atten.vars for the variables attention model</span>
      <span class="c1"># declares.</span>
      <span class="n">atten_vars</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="s1">&#39;AdditiveAttention_vars&#39;</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">atten_vars</span><span class="p">))</span>

      <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>

      <span class="n">all_vars</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">all_variables</span><span class="p">()</span>
      <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">all_vars</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">eval</span><span class="p">())</span>

      <span class="n">atten_vec_out</span><span class="p">,</span> <span class="n">prob_out</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">atten_vec</span><span class="p">,</span> <span class="n">atten_prob</span><span class="p">])</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">atten_vec_out</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">prob_out</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertTrue</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">atten_vec_out</span><span class="p">)))</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertTrue</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">prob_out</span><span class="p">)))</span></div>

<div class="viewcode-block" id="AttentionTest.testAdditiveAttentionVN64bits"><a class="viewcode-back" href="../../../lingvo.core.attention_test.html#lingvo.core.attention_test.AttentionTest.testAdditiveAttentionVN64bits">[docs]</a>  <span class="k">def</span> <span class="nf">testAdditiveAttentionVN64bits</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="p">(</span><span class="n">use_gpu</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
      <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">12345</span><span class="p">)</span>
      <span class="n">source_vecs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
      <span class="n">source_contexts</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
      <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span>
              <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span>
              <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">))</span>
      <span class="n">query_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

      <span class="n">params</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">AdditiveAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
      <span class="n">params</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;atten&#39;</span>
      <span class="n">params</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float64</span>
      <span class="n">params</span><span class="o">.</span><span class="n">params_init</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Gaussian</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">12345</span><span class="p">)</span>
      <span class="n">params</span><span class="o">.</span><span class="n">source_dim</span> <span class="o">=</span> <span class="mi">4</span>
      <span class="n">params</span><span class="o">.</span><span class="n">query_dim</span> <span class="o">=</span> <span class="mi">7</span>
      <span class="n">params</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">5</span>
      <span class="n">params</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">global_vn</span> <span class="o">=</span> <span class="kc">True</span>
      <span class="n">params</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">per_step_vn</span> <span class="o">=</span> <span class="kc">True</span>
      <span class="n">params</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="mf">1.0</span>
      <span class="n">params</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">seed</span> <span class="o">=</span> <span class="mi">54321</span>

      <span class="n">atten</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">AdditiveAttention</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
      <span class="n">atten</span><span class="o">.</span><span class="n">InitForSourcePacked</span><span class="p">(</span><span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span>
                                <span class="n">source_padding</span><span class="p">)</span>
      <span class="n">atten_vec</span><span class="p">,</span> <span class="n">atten_prob</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">ComputeContextVector</span><span class="p">(</span>
          <span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">)</span>

      <span class="n">atten_vars</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="s1">&#39;AdditiveAttention_vars&#39;</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">atten_vars</span><span class="p">))</span>

      <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>

      <span class="n">all_vars</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">all_variables</span><span class="p">()</span>
      <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">all_vars</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">eval</span><span class="p">())</span>

      <span class="n">atten_vec_out</span><span class="p">,</span> <span class="n">prob_out</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">atten_vec</span><span class="p">,</span> <span class="n">atten_prob</span><span class="p">])</span>

      <span class="nb">print</span><span class="p">([</span><span class="s1">&#39;prob_out with vn:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array_repr</span><span class="p">(</span><span class="n">prob_out</span><span class="p">)])</span>
      <span class="nb">print</span><span class="p">([</span><span class="s1">&#39;atten_vec_out with vn:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array_repr</span><span class="p">(</span><span class="n">atten_vec_out</span><span class="p">)])</span>

      <span class="c1"># pyformat: disable</span>
      <span class="c1"># pylint: disable=bad-whitespace</span>
      <span class="n">expected_prob_out</span> <span class="o">=</span><span class="p">[</span>
          <span class="p">[</span> <span class="mf">0.43249266</span><span class="p">,</span>  <span class="mf">0.18638571</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>  <span class="mf">0.38112162</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.</span>        <span class="p">,</span>  <span class="mf">0.32589137</span><span class="p">,</span>  <span class="mf">0.3505654</span> <span class="p">,</span>  <span class="mf">0.32354323</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.26777833</span><span class="p">,</span>  <span class="mf">0.43991441</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>  <span class="mf">0.29230726</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.34583678</span><span class="p">,</span>  <span class="mf">0.32633085</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>  <span class="mf">0.32783237</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.</span>        <span class="p">,</span>  <span class="mf">0.32734872</span><span class="p">,</span>  <span class="mf">0.34749836</span><span class="p">,</span>  <span class="mf">0.32515292</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.33614176</span><span class="p">,</span>  <span class="mf">0.33607175</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>  <span class="mf">0.32778649</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">]</span>
      <span class="p">]</span>
      <span class="n">expected_atten_vec_out</span> <span class="o">=</span> <span class="p">[</span>
          <span class="p">[</span> <span class="mf">0.56117282</span><span class="p">,</span>  <span class="mf">0.37872234</span><span class="p">,</span>  <span class="mf">0.42109472</span><span class="p">,</span>  <span class="mf">0.38981267</span><span class="p">,</span>  <span class="mf">0.45946841</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.85743407</span><span class="p">,</span>  <span class="mf">0.37325286</span><span class="p">,</span>  <span class="mf">0.66322611</span><span class="p">,</span>  <span class="mf">0.69286686</span><span class="p">,</span>  <span class="mf">0.141359</span>  <span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.7377786</span> <span class="p">,</span>  <span class="mf">0.42298519</span><span class="p">,</span>  <span class="mf">0.39970782</span><span class="p">,</span>  <span class="mf">0.67703222</span><span class="p">,</span>  <span class="mf">0.4157012</span> <span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.51011499</span><span class="p">,</span>  <span class="mf">0.35817489</span><span class="p">,</span>  <span class="mf">0.47894328</span><span class="p">,</span>  <span class="mf">0.41259201</span><span class="p">,</span>  <span class="mf">0.54384056</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.85716326</span><span class="p">,</span>  <span class="mf">0.37340558</span><span class="p">,</span>  <span class="mf">0.66250852</span><span class="p">,</span>  <span class="mf">0.69187486</span><span class="p">,</span>  <span class="mf">0.14179651</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.78078121</span><span class="p">,</span>  <span class="mf">0.45646575</span><span class="p">,</span>  <span class="mf">0.4052385</span> <span class="p">,</span>  <span class="mf">0.68248276</span><span class="p">,</span>  <span class="mf">0.43502425</span><span class="p">]</span>
      <span class="p">]</span>
      <span class="c1"># pylint: enable=bad-whitespace</span>
      <span class="c1"># pyformat: enable</span>

      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">expected_prob_out</span><span class="p">,</span> <span class="n">prob_out</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">expected_atten_vec_out</span><span class="p">,</span> <span class="n">atten_vec_out</span><span class="p">)</span></div>

  <span class="k">def</span> <span class="nf">_DotProductAttention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">packed_inputs</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="p">(</span><span class="n">use_gpu</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
      <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">12345</span><span class="p">)</span>
      <span class="c1"># source_vecs_p, source_contexts_p, source_padding_p, query_vec_p are used</span>
      <span class="c1"># for both TensorFlow and numpy computation.</span>
      <span class="n">source_vecs_p</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">)]</span>
      <span class="n">source_vecs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span>
          <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">source_vecs_p</span><span class="p">])</span>
      <span class="n">source_contexts_p</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">)]</span>
      <span class="n">source_contexts</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span>
          <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">source_contexts_p</span><span class="p">])</span>
      <span class="n">source_padding_p</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                          <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]</span>
      <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">source_padding_p</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
      <span class="n">query_vec_p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
      <span class="n">query_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">query_vec_p</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
      <span class="n">query_segment_id_p</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
      <span class="n">source_segment_id_p</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                             <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]</span>
      <span class="n">source_segment_id</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="n">query_segment_id</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="k">if</span> <span class="n">packed_inputs</span><span class="p">:</span>
        <span class="n">source_segment_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">source_segment_id_p</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
        <span class="n">query_segment_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">query_segment_id_p</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
      <span class="n">params</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">DotProductAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
      <span class="n">params</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;dotproduct_atten&#39;</span>
      <span class="n">params</span><span class="o">.</span><span class="n">source_dim</span> <span class="o">=</span> <span class="mi">4</span>
      <span class="n">params</span><span class="o">.</span><span class="n">query_dim</span> <span class="o">=</span> <span class="mi">4</span>
      <span class="n">params</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">4</span>
      <span class="n">params</span><span class="o">.</span><span class="n">packed_input</span> <span class="o">=</span> <span class="n">packed_inputs</span>
      <span class="n">atten</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">DotProductAttention</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
      <span class="n">atten</span><span class="o">.</span><span class="n">InitForSourcePacked</span><span class="p">(</span><span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span>
                                <span class="n">source_padding</span><span class="p">,</span> <span class="n">source_segment_id</span><span class="p">)</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
      <span class="n">atten_vec</span><span class="p">,</span> <span class="n">atten_prob</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">ComputeContextVector</span><span class="p">(</span>
          <span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">query_segment_id</span><span class="o">=</span><span class="n">query_segment_id</span><span class="p">)</span>
      <span class="n">atten_vec_out</span><span class="p">,</span> <span class="n">prob_out</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">atten_vec</span><span class="p">,</span> <span class="n">atten_prob</span><span class="p">])</span>
      <span class="c1"># Use numpy to perform the same computation to generate expected results.</span>
      <span class="n">source_vecs_p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">source_vecs_p</span><span class="p">)</span>
      <span class="c1"># Dot-product part.</span>
      <span class="n">expected_logit</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
          <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">source_vecs_p</span><span class="p">[:,</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">3</span><span class="p">,</span> <span class="p">:],</span> <span class="n">query_vec_p</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:])</span>
          <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
      <span class="p">])</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
      <span class="n">elexp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">expected_logit</span><span class="p">)</span>
      <span class="n">source_padding_p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">source_padding_p</span><span class="p">)</span>
      <span class="n">elexp</span> <span class="o">*=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">source_padding_p</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
      <span class="k">if</span> <span class="n">packed_inputs</span><span class="p">:</span>
        <span class="c1"># Manually constructed packed input mask.</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                           <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                           <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
        <span class="n">elexp</span> <span class="o">*=</span> <span class="n">mask</span>
      <span class="n">expected_prob_out</span> <span class="o">=</span> <span class="n">elexp</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">elexp</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
      <span class="n">expanded_epo</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">expected_prob_out</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
      <span class="n">source_contexts_p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">source_contexts_p</span><span class="p">)</span>
      <span class="n">expected_atten_vec_out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
          <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
              <span class="n">source_contexts_p</span><span class="p">[:,</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">3</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">expanded_epo</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
          <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
      <span class="p">])</span>

      <span class="nb">print</span><span class="p">([</span><span class="s1">&#39;additive attention prob_out&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array_repr</span><span class="p">(</span><span class="n">prob_out</span><span class="p">)])</span>
      <span class="nb">print</span><span class="p">([</span><span class="s1">&#39;additive attention atten_vec_out&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array_repr</span><span class="p">(</span><span class="n">atten_vec_out</span><span class="p">)])</span>

      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">expected_prob_out</span><span class="p">,</span> <span class="n">prob_out</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">expected_atten_vec_out</span><span class="p">,</span> <span class="n">atten_vec_out</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_MultiHeadedAttentionInputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">6348575</span><span class="p">)</span>
    <span class="c1"># source_vecs_p, source_contexts_p, source_padding_p, query_vec_p are used</span>
    <span class="c1"># for both TensorFlow and numpy computation.</span>
    <span class="n">source_vecs_p</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">source_dim</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">)]</span>
    <span class="n">source_vecs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">source_vecs_p</span><span class="p">])</span>
    <span class="n">source_contexts_p</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">)]</span>
    <span class="n">source_contexts</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span>
        <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">source_contexts_p</span><span class="p">])</span>
    <span class="n">source_padding_p</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                        <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]</span>
    <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">source_padding_p</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">))</span>
    <span class="n">query_vec_p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">query_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">query_vec_p</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">query_segment_id_p</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
    <span class="n">source_segment_id_p</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                           <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]</span>
    <span class="n">source_segment_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">source_segment_id_p</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">))</span>
    <span class="n">query_segment_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">query_segment_id_p</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span> <span class="n">source_padding_p</span><span class="p">,</span>
            <span class="n">query_vec</span><span class="p">,</span> <span class="n">source_segment_id</span><span class="p">,</span> <span class="n">query_segment_id</span><span class="p">)</span>

<div class="viewcode-block" id="AttentionTest.testMultiHeadedAttentionDotProductWithFeedinProbs"><a class="viewcode-back" href="../../../lingvo.core.attention_test.html#lingvo.core.attention_test.AttentionTest.testMultiHeadedAttentionDotProductWithFeedinProbs">[docs]</a>  <span class="k">def</span> <span class="nf">testMultiHeadedAttentionDotProductWithFeedinProbs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="p">(</span><span class="n">use_gpu</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
      <span class="p">(</span><span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span>
       <span class="n">_</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_MultiHeadedAttentionInputs</span><span class="p">()</span>
      <span class="n">iap</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">DotProductAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
      <span class="n">iap</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;dot_atten&#39;</span>
      <span class="n">params</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">MultiHeadedAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
          <span class="n">name</span><span class="o">=</span><span class="s1">&#39;multihead_atten&#39;</span><span class="p">,</span>
          <span class="n">source_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
          <span class="n">query_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
          <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
          <span class="n">inner_atten_params</span><span class="o">=</span><span class="n">iap</span><span class="p">,</span>
          <span class="n">num_attention_heads</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
          <span class="n">use_source_vec_as_attention_value</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
      <span class="n">atten</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">cls</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
      <span class="n">_</span><span class="p">,</span> <span class="n">packed_context</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">InitForSourcePacked</span><span class="p">(</span>
          <span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">)</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
      <span class="n">atten_probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">5</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
      <span class="n">atten_vec_proj</span><span class="p">,</span> <span class="n">atten_vec</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">ComputeContextVectorWithAttenProbs</span><span class="p">(</span>
          <span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">packed_context</span><span class="p">,</span> <span class="n">atten_probs</span><span class="p">)</span>
      <span class="n">atten_vec_proj</span><span class="p">,</span> <span class="n">atten_vec</span><span class="p">,</span> <span class="n">packed_context</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
          <span class="p">[</span><span class="n">atten_vec_proj</span><span class="p">,</span> <span class="n">atten_vec</span><span class="p">,</span> <span class="n">packed_context</span><span class="p">])</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span>
          <span class="n">atten_vec</span><span class="p">,</span>
          <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">packed_context</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
                     <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">])[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">])</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">([</span><span class="mf">2.5694468</span><span class="p">,</span> <span class="mf">4.36386967</span><span class="p">,</span> <span class="mf">3.24537992</span><span class="p">],</span>
                          <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">atten_vec_proj</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span></div>

  <span class="k">def</span> <span class="nf">_testMultiHeadedAttentionExtendCachedSourceVecsHelper</span><span class="p">(</span>
      <span class="bp">self</span><span class="p">,</span> <span class="n">additive_atten</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">fprop_dtype</span><span class="p">):</span>
    <span class="c1"># source_batch:3, target_batch:6. Test n = 2 case.</span>
    <span class="n">use_gpu</span> <span class="o">=</span> <span class="p">(</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span> <span class="ow">and</span> <span class="n">fprop_dtype</span> <span class="o">==</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="p">(</span><span class="n">use_gpu</span><span class="o">=</span><span class="n">use_gpu</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
      <span class="p">(</span><span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span>
       <span class="n">source_seg_id</span><span class="p">,</span>
       <span class="n">query_seg_id</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_MultiHeadedAttentionInputs</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">fprop_dtype</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">additive_atten</span><span class="p">:</span>
        <span class="n">iap</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">AdditiveAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
        <span class="n">iap</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;add_atten&#39;</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">iap</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">DotProductAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
        <span class="n">iap</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;dot_atten&#39;</span>
      <span class="n">params</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">MultiHeadedAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
          <span class="n">name</span><span class="o">=</span><span class="s1">&#39;multihead_atten&#39;</span><span class="p">,</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">fprop_dtype</span><span class="o">=</span><span class="n">fprop_dtype</span><span class="p">,</span>
          <span class="n">source_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
          <span class="n">query_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
          <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
          <span class="n">inner_atten_params</span><span class="o">=</span><span class="n">iap</span><span class="p">,</span>
          <span class="n">num_attention_heads</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
          <span class="n">use_source_vec_as_attention_value</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
          <span class="n">packed_input</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="n">atten</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">cls</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
      <span class="n">theta</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">theta</span>
      <span class="n">source_vec1</span><span class="p">,</span> <span class="n">source_context1</span><span class="p">,</span> <span class="n">source_padding1</span><span class="p">,</span> <span class="n">source_seg_id1</span> <span class="o">=</span> <span class="p">(</span>
          <span class="n">atten</span><span class="o">.</span><span class="n">InitForSourcePacked</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span>
                                    <span class="n">source_padding</span><span class="p">,</span> <span class="n">source_seg_id</span><span class="p">))</span>
      <span class="n">source_vec2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">source_vec1</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="n">source_context2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">source_context1</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="n">source_padding2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">source_padding1</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="n">source_seg_id2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">source_padding1</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
        <span class="p">(</span><span class="n">source_vec2</span><span class="p">,</span> <span class="n">source_context2</span><span class="p">,</span>
         <span class="n">source_padding2</span><span class="p">,</span> <span class="n">source_seg_id2</span><span class="p">)</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">ExtendSourcePacked</span><span class="p">(</span>
             <span class="n">theta</span><span class="p">,</span> <span class="n">source_vecs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:],</span> <span class="n">source_contexts</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:],</span>
             <span class="n">source_padding</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:],</span> <span class="n">source_seg_id</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:],</span> <span class="n">source_vec2</span><span class="p">,</span>
             <span class="n">source_context2</span><span class="p">,</span> <span class="n">source_padding2</span><span class="p">,</span> <span class="n">source_seg_id2</span><span class="p">)</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>

      <span class="n">atten_vec_1</span><span class="p">,</span> <span class="n">prob_1</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">ComputeContextVectorWithSource</span><span class="p">(</span>
          <span class="n">theta</span><span class="p">,</span>
          <span class="n">source_vec1</span><span class="p">,</span>
          <span class="n">source_context1</span><span class="p">,</span>
          <span class="n">source_padding1</span><span class="p">,</span>
          <span class="n">source_seg_id1</span><span class="p">,</span>
          <span class="n">query_vec</span><span class="p">,</span>
          <span class="n">query_segment_id</span><span class="o">=</span><span class="n">query_seg_id</span><span class="p">)</span>
      <span class="n">atten_vec_2</span><span class="p">,</span> <span class="n">prob_2</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">ComputeContextVectorWithCachedSource</span><span class="p">(</span>
          <span class="n">theta</span><span class="p">,</span>
          <span class="n">source_vec2</span><span class="p">,</span>
          <span class="n">source_context2</span><span class="p">,</span>
          <span class="n">source_padding2</span><span class="p">,</span>
          <span class="n">source_seg_id2</span><span class="p">,</span>
          <span class="n">query_vec</span><span class="p">,</span>
          <span class="n">query_segment_id</span><span class="o">=</span><span class="n">query_seg_id</span><span class="p">)</span>

      <span class="n">source_vec2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">source_vec2</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
      <span class="c1"># source_context1 is of shape [batch_size * num_heads, seq_len, dim]</span>
      <span class="c1"># source_context2 is of shape [batch_size, seq_len, num_heads * dim]</span>
      <span class="n">source_context2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">source_context2</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span>
          <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
      <span class="n">source_padding2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">source_padding2</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>

      <span class="p">(</span><span class="n">source_vec1_v</span><span class="p">,</span> <span class="n">source_context1_v</span><span class="p">,</span> <span class="n">source_padding1_v</span><span class="p">,</span> <span class="n">source_vec2_v</span><span class="p">,</span>
       <span class="n">source_context2_v</span><span class="p">,</span> <span class="n">source_padding2_v</span><span class="p">,</span> <span class="n">atten_vec1_v</span><span class="p">,</span> <span class="n">prob1_v</span><span class="p">,</span>
       <span class="n">atten_vec2_v</span><span class="p">,</span> <span class="n">prob2_v</span><span class="p">)</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span>
           <span class="n">source_vec1</span><span class="p">,</span> <span class="n">source_context1</span><span class="p">,</span> <span class="n">source_padding1</span><span class="p">,</span> <span class="n">source_vec2</span><span class="p">,</span>
           <span class="n">source_context2</span><span class="p">,</span> <span class="n">source_padding2</span><span class="p">,</span> <span class="n">atten_vec_1</span><span class="p">,</span> <span class="n">prob_1</span><span class="p">,</span> <span class="n">atten_vec_2</span><span class="p">,</span>
           <span class="n">prob_2</span>
       <span class="p">])</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">source_vec1_v</span><span class="p">,</span> <span class="n">source_vec2_v</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">source_context1_v</span><span class="p">,</span> <span class="n">source_context2_v</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">source_padding1_v</span><span class="p">,</span> <span class="n">source_padding2_v</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">atten_vec1_v</span><span class="p">,</span> <span class="n">atten_vec2_v</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">prob1_v</span><span class="p">,</span> <span class="n">prob2_v</span><span class="p">)</span>

<div class="viewcode-block" id="AttentionTest.testMultiHeadedAttentionExtendCachedSourceVecsAdditiveFloat32"><a class="viewcode-back" href="../../../lingvo.core.attention_test.html#lingvo.core.attention_test.AttentionTest.testMultiHeadedAttentionExtendCachedSourceVecsAdditiveFloat32">[docs]</a>  <span class="k">def</span> <span class="nf">testMultiHeadedAttentionExtendCachedSourceVecsAdditiveFloat32</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_testMultiHeadedAttentionExtendCachedSourceVecsHelper</span><span class="p">(</span>
        <span class="n">additive_atten</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">fprop_dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span></div>

<div class="viewcode-block" id="AttentionTest.testMultiHeadedAttentionExtendCachedSourceVecsAdditiveFloat32Float16"><a class="viewcode-back" href="../../../lingvo.core.attention_test.html#lingvo.core.attention_test.AttentionTest.testMultiHeadedAttentionExtendCachedSourceVecsAdditiveFloat32Float16">[docs]</a>  <span class="k">def</span> <span class="nf">testMultiHeadedAttentionExtendCachedSourceVecsAdditiveFloat32Float16</span><span class="p">(</span>
      <span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_testMultiHeadedAttentionExtendCachedSourceVecsHelper</span><span class="p">(</span>
        <span class="n">additive_atten</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">fprop_dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span></div>

<div class="viewcode-block" id="AttentionTest.testMultiHeadedAttentionExtendCachedSourceVecsDotFloat32"><a class="viewcode-back" href="../../../lingvo.core.attention_test.html#lingvo.core.attention_test.AttentionTest.testMultiHeadedAttentionExtendCachedSourceVecsDotFloat32">[docs]</a>  <span class="k">def</span> <span class="nf">testMultiHeadedAttentionExtendCachedSourceVecsDotFloat32</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_testMultiHeadedAttentionExtendCachedSourceVecsHelper</span><span class="p">(</span>
        <span class="n">additive_atten</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">fprop_dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span></div>

<div class="viewcode-block" id="AttentionTest.testMultiHeadedAttentionExtendCachedSourceVecsDotFloat32Float16"><a class="viewcode-back" href="../../../lingvo.core.attention_test.html#lingvo.core.attention_test.AttentionTest.testMultiHeadedAttentionExtendCachedSourceVecsDotFloat32Float16">[docs]</a>  <span class="k">def</span> <span class="nf">testMultiHeadedAttentionExtendCachedSourceVecsDotFloat32Float16</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_testMultiHeadedAttentionExtendCachedSourceVecsHelper</span><span class="p">(</span>
        <span class="n">additive_atten</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">fprop_dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span></div>

  <span class="k">def</span> <span class="nf">_testMultiHeadedAttentionExtendCachedSourceVecsNoPaddingsHelper</span><span class="p">(</span>
      <span class="bp">self</span><span class="p">,</span> <span class="n">additive_attention</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># source_batch:3, target_batch:6. Test n = 2 case.</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="p">(</span><span class="n">use_gpu</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
      <span class="p">(</span><span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span>
       <span class="n">_</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_MultiHeadedAttentionInputs</span><span class="p">()</span>
      <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
      <span class="k">if</span> <span class="n">additive_attention</span><span class="p">:</span>
        <span class="n">iap</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">AdditiveAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
        <span class="n">iap</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;add_atten&#39;</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">iap</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">DotProductAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
        <span class="n">iap</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;dot_atten&#39;</span>
      <span class="n">params</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">MultiHeadedAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
          <span class="n">name</span><span class="o">=</span><span class="s1">&#39;multihead_atten&#39;</span><span class="p">,</span>
          <span class="n">source_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
          <span class="n">query_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
          <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
          <span class="n">inner_atten_params</span><span class="o">=</span><span class="n">iap</span><span class="p">,</span>
          <span class="n">num_attention_heads</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
          <span class="n">use_source_vec_as_attention_value</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
      <span class="n">atten</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">cls</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
      <span class="n">source_vec1</span><span class="p">,</span> <span class="n">source_context1</span><span class="p">,</span> <span class="n">source_padding1</span><span class="p">,</span> <span class="n">source_seg_id1</span> <span class="o">=</span> <span class="p">(</span>
          <span class="n">atten</span><span class="o">.</span><span class="n">InitForSourcePacked</span><span class="p">(</span><span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span>
                                    <span class="n">source_padding</span><span class="p">))</span>
      <span class="n">source_vec2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
      <span class="n">source_context2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
        <span class="p">(</span><span class="n">source_vec2</span><span class="p">,</span> <span class="n">source_context2</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">ExtendSourcePacked</span><span class="p">(</span>
            <span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">source_vecs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:],</span> <span class="n">source_contexts</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:],</span> <span class="kc">None</span><span class="p">,</span>
            <span class="kc">None</span><span class="p">,</span> <span class="n">source_vec2</span><span class="p">,</span> <span class="n">source_context2</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>

      <span class="n">atten_vec_1</span><span class="p">,</span> <span class="n">prob_1</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">ComputeContextVectorWithSource</span><span class="p">(</span>
          <span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">source_vec1</span><span class="p">,</span> <span class="n">source_context1</span><span class="p">,</span> <span class="n">source_padding1</span><span class="p">,</span>
          <span class="n">source_seg_id1</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">)</span>
      <span class="n">atten_vec_2</span><span class="p">,</span> <span class="n">prob_2</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">ComputeContextVectorWithCachedSource</span><span class="p">(</span>
          <span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">source_vec2</span><span class="p">,</span> <span class="n">source_context2</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">)</span>

      <span class="n">source_vec2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">source_vec2</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
      <span class="c1"># source_context1 is of shape [batch_size * num_heads, seq_len, dim]</span>
      <span class="c1"># source_context2 is of shape [batch_size, seq_len, num_heads * dim]</span>
      <span class="n">source_context2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">source_context2</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span>
          <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>

      <span class="p">(</span><span class="n">source_vec1_v</span><span class="p">,</span> <span class="n">source_context1_v</span><span class="p">,</span> <span class="n">source_vec2_v</span><span class="p">,</span> <span class="n">source_context2_v</span><span class="p">,</span>
       <span class="n">atten_vec1_v</span><span class="p">,</span> <span class="n">prob1_v</span><span class="p">,</span> <span class="n">atten_vec2_v</span><span class="p">,</span> <span class="n">prob2_v</span><span class="p">)</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span>
           <span class="n">source_vec1</span><span class="p">,</span> <span class="n">source_context1</span><span class="p">,</span> <span class="n">source_vec2</span><span class="p">,</span> <span class="n">source_context2</span><span class="p">,</span>
           <span class="n">atten_vec_1</span><span class="p">,</span> <span class="n">prob_1</span><span class="p">,</span> <span class="n">atten_vec_2</span><span class="p">,</span> <span class="n">prob_2</span>
       <span class="p">])</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">source_vec1_v</span><span class="p">,</span> <span class="n">source_vec2_v</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">source_context1_v</span><span class="p">,</span> <span class="n">source_context2_v</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">atten_vec1_v</span><span class="p">,</span> <span class="n">atten_vec2_v</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">prob1_v</span><span class="p">,</span> <span class="n">prob2_v</span><span class="p">)</span>

<div class="viewcode-block" id="AttentionTest.testMultiHeadedDotAttentionExtendCachedSourceVecsNoPaddings"><a class="viewcode-back" href="../../../lingvo.core.attention_test.html#lingvo.core.attention_test.AttentionTest.testMultiHeadedDotAttentionExtendCachedSourceVecsNoPaddings">[docs]</a>  <span class="k">def</span> <span class="nf">testMultiHeadedDotAttentionExtendCachedSourceVecsNoPaddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_testMultiHeadedAttentionExtendCachedSourceVecsNoPaddingsHelper</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span></div>

<div class="viewcode-block" id="AttentionTest.testMultiHeadedAddAttentionExtendCachedSourceVecsNoPaddings"><a class="viewcode-back" href="../../../lingvo.core.attention_test.html#lingvo.core.attention_test.AttentionTest.testMultiHeadedAddAttentionExtendCachedSourceVecsNoPaddings">[docs]</a>  <span class="k">def</span> <span class="nf">testMultiHeadedAddAttentionExtendCachedSourceVecsNoPaddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_testMultiHeadedAttentionExtendCachedSourceVecsNoPaddingsHelper</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span></div>

<div class="viewcode-block" id="AttentionTest.testMultiHeadedAttentionDotProduct"><a class="viewcode-back" href="../../../lingvo.core.attention_test.html#lingvo.core.attention_test.AttentionTest.testMultiHeadedAttentionDotProduct">[docs]</a>  <span class="k">def</span> <span class="nf">testMultiHeadedAttentionDotProduct</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># source_batch:3, target_batch:6. Test n = 2 case.</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="p">(</span><span class="n">use_gpu</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
      <span class="p">(</span><span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span> <span class="n">source_padding_p</span><span class="p">,</span>
       <span class="n">query_vec</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_MultiHeadedAttentionInputs</span><span class="p">()</span>
      <span class="n">iap</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">DotProductAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
      <span class="n">iap</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;dot_atten&#39;</span>
      <span class="n">params</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">MultiHeadedAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
          <span class="n">name</span><span class="o">=</span><span class="s1">&#39;multihead_atten&#39;</span><span class="p">,</span>
          <span class="n">source_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
          <span class="n">query_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
          <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
          <span class="n">inner_atten_params</span><span class="o">=</span><span class="n">iap</span><span class="p">,</span>
          <span class="n">num_attention_heads</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
          <span class="n">use_source_vec_as_attention_value</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
      <span class="n">atten</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">cls</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
      <span class="n">atten</span><span class="o">.</span><span class="n">InitForSourcePacked</span><span class="p">(</span><span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span>
                                <span class="n">source_padding</span><span class="p">)</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
      <span class="n">atten_vec</span><span class="p">,</span> <span class="n">atten_prob</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">ComputeContextVector</span><span class="p">(</span>
          <span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">)</span>
      <span class="n">atten_vec_out</span><span class="p">,</span> <span class="n">prob_out</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">atten_vec</span><span class="p">,</span> <span class="n">atten_prob</span><span class="p">])</span>
      <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;atten_vec_out&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">atten_vec_out</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">([</span>
          <span class="mf">2.84679317</span><span class="p">,</span> <span class="mf">2.36924601</span><span class="p">,</span> <span class="mf">3.54831171</span><span class="p">,</span> <span class="mf">2.86487937</span><span class="p">,</span> <span class="mf">2.3537426</span><span class="p">,</span> <span class="mf">3.54308939</span>
      <span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">atten_vec_out</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
      <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;atten_vec_out&#39;</span><span class="p">,</span> <span class="n">atten_vec_out</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;prob_out&#39;</span><span class="p">,</span> <span class="n">prob_out</span><span class="p">)</span>
      <span class="n">t_batch_size</span> <span class="o">=</span> <span class="mi">6</span>
      <span class="n">s_batch_size</span> <span class="o">=</span> <span class="mi">3</span>
      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">t_batch_size</span><span class="p">):</span>
        <span class="c1"># Test to make sure we didn&#39;t mess up indexing.</span>
        <span class="n">s_index</span> <span class="o">=</span> <span class="n">i</span> <span class="o">%</span> <span class="n">s_batch_size</span>
        <span class="n">atten</span><span class="o">.</span><span class="n">InitForSourcePacked</span><span class="p">(</span><span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span>
                                  <span class="n">source_vecs</span><span class="p">[:,</span> <span class="n">s_index</span><span class="p">:</span><span class="n">s_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:],</span>
                                  <span class="n">source_contexts</span><span class="p">[:,</span> <span class="n">s_index</span><span class="p">:</span><span class="n">s_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:],</span>
                                  <span class="n">source_padding</span><span class="p">[:,</span> <span class="n">s_index</span><span class="p">:</span><span class="n">s_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
        <span class="n">atten_vec_i</span><span class="p">,</span> <span class="n">prob_i</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">ComputeContextVector</span><span class="p">(</span>
            <span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
        <span class="n">atten_vec_i_out</span><span class="p">,</span> <span class="n">prob_i_out</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">atten_vec_i</span><span class="p">,</span> <span class="n">prob_i</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">prob_i_out</span><span class="p">,</span> <span class="n">prob_out</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">atten_vec_i_out</span><span class="p">,</span> <span class="n">atten_vec_out</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
        <span class="n">padding_i</span> <span class="o">=</span> <span class="n">source_padding_p</span><span class="p">[</span><span class="n">s_index</span><span class="p">]</span>
        <span class="c1"># Check to make sure prob exists only on valid timesteps.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">padding_i</span> <span class="o">*</span> <span class="n">prob_i_out</span><span class="p">))</span></div>

<div class="viewcode-block" id="AttentionTest.testMultiHeadedAttentionDotProductPackedInput"><a class="viewcode-back" href="../../../lingvo.core.attention_test.html#lingvo.core.attention_test.AttentionTest.testMultiHeadedAttentionDotProductPackedInput">[docs]</a>  <span class="k">def</span> <span class="nf">testMultiHeadedAttentionDotProductPackedInput</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># source_batch:3, target_batch:6. Test n = 2 case.</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="p">(</span><span class="n">use_gpu</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
      <span class="p">(</span><span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span> <span class="n">source_padding_p</span><span class="p">,</span>
       <span class="n">query_vec</span><span class="p">,</span> <span class="n">source_seg_id</span><span class="p">,</span>
       <span class="n">query_seg_id</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_MultiHeadedAttentionInputs</span><span class="p">()</span>
      <span class="n">iap</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">DotProductAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
      <span class="n">iap</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;dot_atten&#39;</span>
      <span class="n">params</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">MultiHeadedAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
          <span class="n">name</span><span class="o">=</span><span class="s1">&#39;multihead_atten&#39;</span><span class="p">,</span>
          <span class="n">source_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
          <span class="n">query_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
          <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
          <span class="n">inner_atten_params</span><span class="o">=</span><span class="n">iap</span><span class="p">,</span>
          <span class="n">num_attention_heads</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
          <span class="n">use_source_vec_as_attention_value</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
          <span class="n">packed_input</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="n">atten</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">cls</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
      <span class="n">atten</span><span class="o">.</span><span class="n">InitForSourcePacked</span><span class="p">(</span><span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span>
                                <span class="n">source_padding</span><span class="p">,</span> <span class="n">source_seg_id</span><span class="p">)</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
      <span class="n">atten_vec</span><span class="p">,</span> <span class="n">atten_prob</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">ComputeContextVector</span><span class="p">(</span>
          <span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">query_segment_id</span><span class="o">=</span><span class="n">query_seg_id</span><span class="p">)</span>
      <span class="n">atten_vec_out</span><span class="p">,</span> <span class="n">prob_out</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">atten_vec</span><span class="p">,</span> <span class="n">atten_prob</span><span class="p">])</span>
      <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;atten_vec_out&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">atten_vec_out</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span>
          <span class="p">[</span><span class="mf">2.565648</span><span class="p">,</span> <span class="mf">2.268182</span><span class="p">,</span> <span class="mf">3.739031</span><span class="p">,</span> <span class="mf">3.093884</span><span class="p">,</span> <span class="mf">2.770367</span><span class="p">,</span> <span class="mf">3.580353</span><span class="p">],</span>
          <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">atten_vec_out</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
      <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;atten_vec_out&#39;</span><span class="p">,</span> <span class="n">atten_vec_out</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;prob_out&#39;</span><span class="p">,</span> <span class="n">prob_out</span><span class="p">)</span>
      <span class="n">t_batch_size</span> <span class="o">=</span> <span class="mi">6</span>
      <span class="n">s_batch_size</span> <span class="o">=</span> <span class="mi">3</span>
      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">t_batch_size</span><span class="p">):</span>
        <span class="c1"># Test to make sure we didn&#39;t mess up indexing.</span>
        <span class="n">s_index</span> <span class="o">=</span> <span class="n">i</span> <span class="o">%</span> <span class="n">s_batch_size</span>
        <span class="n">src_seg_id</span> <span class="o">=</span> <span class="n">source_seg_id</span><span class="p">[:,</span> <span class="n">s_index</span><span class="p">:</span><span class="n">s_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">atten</span><span class="o">.</span><span class="n">InitForSourcePacked</span><span class="p">(</span>
            <span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">source_vecs</span><span class="p">[:,</span> <span class="n">s_index</span><span class="p">:</span><span class="n">s_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:],</span>
            <span class="n">source_contexts</span><span class="p">[:,</span> <span class="n">s_index</span><span class="p">:</span><span class="n">s_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:],</span>
            <span class="n">source_padding</span><span class="p">[:,</span> <span class="n">s_index</span><span class="p">:</span><span class="n">s_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="n">src_seg_id</span><span class="p">)</span>
        <span class="n">qry_seg_id</span> <span class="o">=</span> <span class="n">query_seg_id</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">atten_vec_i</span><span class="p">,</span> <span class="n">prob_i</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">ComputeContextVector</span><span class="p">(</span>
            <span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="n">query_segment_id</span><span class="o">=</span><span class="n">qry_seg_id</span><span class="p">)</span>
        <span class="n">atten_vec_i_out</span><span class="p">,</span> <span class="n">prob_i_out</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">atten_vec_i</span><span class="p">,</span> <span class="n">prob_i</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">prob_i_out</span><span class="p">,</span> <span class="n">prob_out</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">atten_vec_i_out</span><span class="p">,</span> <span class="n">atten_vec_out</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
        <span class="n">padding_i</span> <span class="o">=</span> <span class="n">source_padding_p</span><span class="p">[</span><span class="n">s_index</span><span class="p">]</span>
        <span class="c1"># Check to make sure prob exists only on valid timesteps.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">padding_i</span> <span class="o">*</span> <span class="n">prob_i_out</span><span class="p">))</span></div>

<div class="viewcode-block" id="AttentionTest.testMultiHeadedAttentionDotProductDeterministicDropout"><a class="viewcode-back" href="../../../lingvo.core.attention_test.html#lingvo.core.attention_test.AttentionTest.testMultiHeadedAttentionDotProductDeterministicDropout">[docs]</a>  <span class="k">def</span> <span class="nf">testMultiHeadedAttentionDotProductDeterministicDropout</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># source_batch:3, target_batch:6. Test n = 2 case.</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="p">(</span><span class="n">use_gpu</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
      <span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_MultiHeadedAttentionInputs</span><span class="p">())</span>
      <span class="n">iap</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">DotProductAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
      <span class="n">iap</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;dot_atten&#39;</span>
      <span class="n">params</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">MultiHeadedAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
          <span class="n">name</span><span class="o">=</span><span class="s1">&#39;multihead_atten&#39;</span><span class="p">,</span>
          <span class="n">source_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
          <span class="n">query_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
          <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
          <span class="n">inner_atten_params</span><span class="o">=</span><span class="n">iap</span><span class="p">,</span>
          <span class="n">num_attention_heads</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
          <span class="n">atten_dropout_prob</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
          <span class="n">atten_dropout_deterministic</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
          <span class="n">random_seed</span><span class="o">=</span><span class="mi">7249528</span><span class="p">,</span>
          <span class="n">use_source_vec_as_attention_value</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
      <span class="n">atten</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">cls</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
      <span class="n">atten</span><span class="o">.</span><span class="n">InitForSourcePacked</span><span class="p">(</span><span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span>
                                <span class="n">source_padding</span><span class="p">)</span>
      <span class="n">atten_state</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">ZeroAttentionState</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;atten_state:&#39;</span><span class="p">,</span> <span class="n">atten_state</span><span class="p">)</span>

      <span class="n">step_state</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
          <span class="n">global_step</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">GetOrCreateGlobalStep</span><span class="p">(),</span>
          <span class="n">time_step</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">))</span>

      <span class="n">atten_vec</span><span class="p">,</span> <span class="n">atten_prob</span><span class="p">,</span> <span class="n">atten_state</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">ComputeContextVector</span><span class="p">(</span>
          <span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">atten_state</span><span class="p">,</span> <span class="n">step_state</span><span class="o">=</span><span class="n">step_state</span><span class="p">)</span>

      <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
      <span class="n">atten_vec_out</span><span class="p">,</span> <span class="n">prob_out</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">atten_vec</span><span class="p">,</span> <span class="n">atten_prob</span><span class="p">])</span>

      <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;atten_vec_out&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">atten_vec_out</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span>
          <span class="p">[</span><span class="mf">3.248899</span><span class="p">,</span> <span class="mf">1.280898</span><span class="p">,</span> <span class="mf">3.930958</span><span class="p">,</span> <span class="mf">2.564503</span><span class="p">,</span> <span class="mf">2.824608</span><span class="p">,</span> <span class="mf">3.287644</span><span class="p">],</span>
          <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">atten_vec_out</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

      <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;atten_vec_out&#39;</span><span class="p">,</span> <span class="n">atten_vec_out</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;prob_out&#39;</span><span class="p">,</span> <span class="n">prob_out</span><span class="p">)</span>

      <span class="c1"># pyformat: disable</span>
      <span class="c1"># pylint: disable=bad-whitespace</span>
      <span class="n">expected_prob_out</span> <span class="o">=</span> <span class="p">[</span>
          <span class="p">[</span><span class="mf">0.206652</span><span class="p">,</span> <span class="mf">0.271565</span><span class="p">,</span> <span class="mf">0.</span>      <span class="p">,</span> <span class="mf">0.</span>      <span class="p">,</span> <span class="mf">0.264720</span><span class="p">,</span> <span class="mf">0.257063</span><span class="p">],</span>
          <span class="p">[</span><span class="mf">0.</span>      <span class="p">,</span> <span class="mf">0.238421</span><span class="p">,</span> <span class="mf">0.225814</span><span class="p">,</span> <span class="mf">0.255782</span><span class="p">,</span> <span class="mf">0.</span>      <span class="p">,</span> <span class="mf">0.279983</span><span class="p">],</span>
          <span class="p">[</span><span class="mf">0.320409</span><span class="p">,</span> <span class="mf">0.192486</span><span class="p">,</span> <span class="mf">0.</span>      <span class="p">,</span> <span class="mf">0.203681</span><span class="p">,</span> <span class="mf">0.</span>      <span class="p">,</span> <span class="mf">0.283424</span><span class="p">],</span>
          <span class="p">[</span><span class="mf">0.208255</span><span class="p">,</span> <span class="mf">0.272202</span><span class="p">,</span> <span class="mf">0.</span>      <span class="p">,</span> <span class="mf">0.</span>      <span class="p">,</span> <span class="mf">0.256848</span><span class="p">,</span> <span class="mf">0.262695</span><span class="p">],</span>
          <span class="p">[</span><span class="mf">0.</span>      <span class="p">,</span> <span class="mf">0.238912</span><span class="p">,</span> <span class="mf">0.241879</span><span class="p">,</span> <span class="mf">0.249530</span><span class="p">,</span> <span class="mf">0.</span>      <span class="p">,</span> <span class="mf">0.269679</span><span class="p">],</span>
          <span class="p">[</span><span class="mf">0.320843</span><span class="p">,</span> <span class="mf">0.190871</span><span class="p">,</span> <span class="mf">0.</span>      <span class="p">,</span> <span class="mf">0.217272</span><span class="p">,</span> <span class="mf">0.</span>      <span class="p">,</span> <span class="mf">0.271014</span><span class="p">]]</span>
      <span class="n">expected_atten_vec_out</span> <span class="o">=</span> <span class="p">[</span>
          <span class="p">[</span><span class="mf">0.406143</span><span class="p">,</span> <span class="mf">0.751921</span><span class="p">,</span> <span class="mf">0.729597</span><span class="p">,</span> <span class="mf">0.544096</span><span class="p">,</span> <span class="mf">0.604882</span><span class="p">,</span> <span class="mf">0.212260</span><span class="p">],</span>
          <span class="p">[</span><span class="mf">0.150803</span><span class="p">,</span> <span class="mf">0.565698</span><span class="p">,</span> <span class="mf">0.564398</span><span class="p">,</span> <span class="mf">0.</span>      <span class="p">,</span> <span class="mf">0.</span>      <span class="p">,</span> <span class="mf">0.</span>      <span class="p">],</span>
          <span class="p">[</span><span class="mf">0.715291</span><span class="p">,</span> <span class="mf">0.880693</span><span class="p">,</span> <span class="mf">0.951602</span><span class="p">,</span> <span class="mf">0.410311</span><span class="p">,</span> <span class="mf">0.419529</span><span class="p">,</span> <span class="mf">0.553533</span><span class="p">],</span>
          <span class="p">[</span><span class="mf">0.942534</span><span class="p">,</span> <span class="mf">0.704318</span><span class="p">,</span> <span class="mf">0.917652</span><span class="p">,</span> <span class="mf">0.</span>      <span class="p">,</span> <span class="mf">0.</span>      <span class="p">,</span> <span class="mf">0.</span>      <span class="p">],</span>
          <span class="p">[</span><span class="mf">0.325881</span><span class="p">,</span> <span class="mf">0.688331</span><span class="p">,</span> <span class="mf">0.959252</span><span class="p">,</span> <span class="mf">0.347509</span><span class="p">,</span> <span class="mf">0.192640</span><span class="p">,</span> <span class="mf">0.310995</span><span class="p">],</span>
          <span class="p">[</span><span class="mf">0.688853</span><span class="p">,</span> <span class="mf">0.531850</span><span class="p">,</span> <span class="mf">0.670351</span><span class="p">,</span> <span class="mf">0.226215</span><span class="p">,</span> <span class="mf">0.706279</span><span class="p">,</span> <span class="mf">0.464096</span><span class="p">]]</span>
      <span class="c1"># pylint: enable=bad-whitespace</span>
      <span class="c1"># pyformat: enable</span>

      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">expected_prob_out</span><span class="p">,</span> <span class="n">prob_out</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">expected_atten_vec_out</span><span class="p">,</span> <span class="n">atten_vec_out</span><span class="p">)</span></div>

<div class="viewcode-block" id="AttentionTest.testMultiHeadedAttentionMonotonic"><a class="viewcode-back" href="../../../lingvo.core.attention_test.html#lingvo.core.attention_test.AttentionTest.testMultiHeadedAttentionMonotonic">[docs]</a>  <span class="k">def</span> <span class="nf">testMultiHeadedAttentionMonotonic</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># source_batch:3, target_batch:6. Test n = 2 case.</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="p">(</span><span class="n">use_gpu</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
      <span class="p">(</span><span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span> <span class="n">source_padding_p</span><span class="p">,</span>
       <span class="n">query_vec</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_MultiHeadedAttentionInputs</span><span class="p">()</span>
      <span class="n">iap</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">MonotonicAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
      <span class="n">iap</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;mono_atten&#39;</span>
      <span class="n">params</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">MultiHeadedAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
          <span class="n">name</span><span class="o">=</span><span class="s1">&#39;multihead_atten&#39;</span><span class="p">,</span>
          <span class="n">source_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
          <span class="n">query_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
          <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
          <span class="n">inner_atten_params</span><span class="o">=</span><span class="n">iap</span><span class="p">,</span>
          <span class="n">num_attention_heads</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
          <span class="n">use_source_vec_as_attention_value</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
      <span class="n">atten</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">cls</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
      <span class="n">atten</span><span class="o">.</span><span class="n">InitForSourcePacked</span><span class="p">(</span><span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span>
                                <span class="n">source_padding</span><span class="p">)</span>
      <span class="c1"># [batch * 2 heads, time]</span>
      <span class="n">atten_init_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attentionStateWithRandomEmitProbabilities</span><span class="p">(</span>
          <span class="n">atten</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
      <span class="n">atten_vec</span><span class="p">,</span> <span class="n">atten_prob</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">ComputeContextVector</span><span class="p">(</span>
          <span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">atten_init_state</span><span class="p">)</span>
      <span class="n">atten_vec_out</span><span class="p">,</span> <span class="n">prob_out</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">atten_vec</span><span class="p">,</span> <span class="n">atten_prob</span><span class="p">])</span>
      <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;atten_vec_out&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">atten_vec_out</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span>
          <span class="p">[</span><span class="mf">1.494033</span><span class="p">,</span> <span class="mf">1.120422</span><span class="p">,</span> <span class="mf">1.699309</span><span class="p">,</span> <span class="mf">1.508609</span><span class="p">,</span> <span class="mf">1.1329</span><span class="p">,</span> <span class="mf">1.670303</span><span class="p">],</span>
          <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">atten_vec_out</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
      <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;atten_vec_out&#39;</span><span class="p">,</span> <span class="n">atten_vec_out</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;prob_out&#39;</span><span class="p">,</span> <span class="n">prob_out</span><span class="p">)</span>
      <span class="n">t_batch_size</span> <span class="o">=</span> <span class="mi">6</span>
      <span class="n">s_batch_size</span> <span class="o">=</span> <span class="mi">3</span>
      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">t_batch_size</span><span class="p">):</span>
        <span class="c1"># Test to make sure we didn&#39;t mess up indexing.</span>
        <span class="n">s_index</span> <span class="o">=</span> <span class="n">i</span> <span class="o">%</span> <span class="n">s_batch_size</span>
        <span class="n">atten</span><span class="o">.</span><span class="n">InitForSourcePacked</span><span class="p">(</span><span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span>
                                  <span class="n">source_vecs</span><span class="p">[:,</span> <span class="n">s_index</span><span class="p">:</span><span class="n">s_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:],</span>
                                  <span class="n">source_contexts</span><span class="p">[:,</span> <span class="n">s_index</span><span class="p">:</span><span class="n">s_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:],</span>
                                  <span class="n">source_padding</span><span class="p">[:,</span> <span class="n">s_index</span><span class="p">:</span><span class="n">s_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
        <span class="n">j</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="mi">2</span>
        <span class="n">sliced_atten_state</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
            <span class="n">emit_probs</span><span class="o">=</span><span class="n">atten_init_state</span><span class="o">.</span><span class="n">emit_probs</span><span class="p">[</span><span class="n">j</span><span class="p">:</span><span class="n">j</span> <span class="o">+</span> <span class="mi">2</span><span class="p">],</span>
            <span class="n">random_seed</span><span class="o">=</span><span class="n">atten_init_state</span><span class="o">.</span><span class="n">random_seed</span><span class="p">)</span>
        <span class="n">atten_vec_i</span><span class="p">,</span> <span class="n">prob_i</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">ComputeContextVector</span><span class="p">(</span>
            <span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="n">sliced_atten_state</span><span class="p">)</span>
        <span class="n">atten_vec_i_out</span><span class="p">,</span> <span class="n">prob_i_out</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">atten_vec_i</span><span class="p">,</span> <span class="n">prob_i</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">prob_i_out</span><span class="p">,</span> <span class="n">prob_out</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">atten_vec_i_out</span><span class="p">,</span> <span class="n">atten_vec_out</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
        <span class="n">padding_i</span> <span class="o">=</span> <span class="n">source_padding_p</span><span class="p">[</span><span class="n">s_index</span><span class="p">]</span>
        <span class="c1"># Check to make sure prob exists only on valid timesteps.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">padding_i</span> <span class="o">*</span> <span class="n">prob_i_out</span><span class="p">))</span></div>

<div class="viewcode-block" id="AttentionTest.testMultiHeadedAttentionDotProductWithAllProj"><a class="viewcode-back" href="../../../lingvo.core.attention_test.html#lingvo.core.attention_test.AttentionTest.testMultiHeadedAttentionDotProductWithAllProj">[docs]</a>  <span class="k">def</span> <span class="nf">testMultiHeadedAttentionDotProductWithAllProj</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># source_batch:3, target_batch:6. Test n = 2 case.</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="p">(</span><span class="n">use_gpu</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
      <span class="p">(</span><span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span> <span class="n">source_padding_p</span><span class="p">,</span>
       <span class="n">query_vec</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_MultiHeadedAttentionInputs</span><span class="p">()</span>
      <span class="n">iap</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">DotProductAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
      <span class="n">iap</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;dot_atten&#39;</span>
      <span class="n">params</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">MultiHeadedAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
          <span class="n">name</span><span class="o">=</span><span class="s1">&#39;multihead_atten&#39;</span><span class="p">,</span>
          <span class="n">source_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
          <span class="n">query_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
          <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
          <span class="n">inner_atten_params</span><span class="o">=</span><span class="n">iap</span><span class="p">,</span>
          <span class="n">num_attention_heads</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
          <span class="n">use_source_vec_as_attention_value</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
          <span class="n">enable_ctx_pre_proj</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
          <span class="n">enable_ctx_post_proj</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
          <span class="n">ctx_post_proj_dim</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
          <span class="n">context_dim</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
      <span class="n">atten</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">cls</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
      <span class="n">atten</span><span class="o">.</span><span class="n">InitForSourcePacked</span><span class="p">(</span><span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span>
                                <span class="n">source_padding</span><span class="p">)</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
      <span class="n">atten_vec</span><span class="p">,</span> <span class="n">atten_prob</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">ComputeContextVector</span><span class="p">(</span>
          <span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">)</span>
      <span class="n">atten_vec_out</span><span class="p">,</span> <span class="n">prob_out</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">atten_vec</span><span class="p">,</span> <span class="n">atten_prob</span><span class="p">])</span>
      <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;atten_vec_out&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">atten_vec_out</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">([</span>
          <span class="mf">1.356745</span><span class="p">,</span> <span class="mf">0.65274805</span><span class="p">,</span> <span class="mf">1.39460433</span><span class="p">,</span> <span class="mf">1.34961343</span><span class="p">,</span> <span class="mf">0.63025361</span><span class="p">,</span> <span class="mf">1.41543126</span>
      <span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">atten_vec_out</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
      <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;atten_vec_out&#39;</span><span class="p">,</span> <span class="n">atten_vec_out</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;prob_out&#39;</span><span class="p">,</span> <span class="n">prob_out</span><span class="p">)</span>
      <span class="n">t_batch_size</span> <span class="o">=</span> <span class="mi">6</span>
      <span class="n">s_batch_size</span> <span class="o">=</span> <span class="mi">3</span>
      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">t_batch_size</span><span class="p">):</span>
        <span class="c1"># Test to make sure we didn&#39;t mess up indexing.</span>
        <span class="n">s_index</span> <span class="o">=</span> <span class="n">i</span> <span class="o">%</span> <span class="n">s_batch_size</span>
        <span class="n">atten</span><span class="o">.</span><span class="n">InitForSourcePacked</span><span class="p">(</span><span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span>
                                  <span class="n">source_vecs</span><span class="p">[:,</span> <span class="n">s_index</span><span class="p">:</span><span class="n">s_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:],</span>
                                  <span class="n">source_contexts</span><span class="p">[:,</span> <span class="n">s_index</span><span class="p">:</span><span class="n">s_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:],</span>
                                  <span class="n">source_padding</span><span class="p">[:,</span> <span class="n">s_index</span><span class="p">:</span><span class="n">s_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
        <span class="n">atten_vec_i</span><span class="p">,</span> <span class="n">prob_i</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">ComputeContextVector</span><span class="p">(</span>
            <span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
        <span class="n">atten_vec_i_out</span><span class="p">,</span> <span class="n">prob_i_out</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">atten_vec_i</span><span class="p">,</span> <span class="n">prob_i</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">prob_i_out</span><span class="p">,</span> <span class="n">prob_out</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">atten_vec_i_out</span><span class="p">,</span> <span class="n">atten_vec_out</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
        <span class="n">padding_i</span> <span class="o">=</span> <span class="n">source_padding_p</span><span class="p">[</span><span class="n">s_index</span><span class="p">]</span>
        <span class="c1"># Check to make sure prob exists only on valid timesteps.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">padding_i</span> <span class="o">*</span> <span class="n">prob_i_out</span><span class="p">))</span></div>

  <span class="k">def</span> <span class="nf">_testMultiHeadedAttentionAdditiveHelper</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                              <span class="n">source_dim</span><span class="p">,</span>
                                              <span class="n">expected_vec</span><span class="p">,</span>
                                              <span class="n">packed_input</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># source_batch:3, target_batch:6. Test n = 2 case.</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="p">(</span><span class="n">use_gpu</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
      <span class="p">(</span><span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span> <span class="n">source_padding_p</span><span class="p">,</span>
       <span class="n">query_vec</span><span class="p">,</span> <span class="n">source_seg_id</span><span class="p">,</span>
       <span class="n">query_seg_id</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_MultiHeadedAttentionInputs</span><span class="p">(</span><span class="n">source_dim</span><span class="p">)</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">packed_input</span><span class="p">:</span>
        <span class="n">source_seg_id</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">query_seg_id</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="n">iap</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">AdditiveAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
      <span class="n">iap</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;add_atten&#39;</span>
      <span class="n">params</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">MultiHeadedAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
          <span class="n">name</span><span class="o">=</span><span class="s1">&#39;multihead_atten&#39;</span><span class="p">,</span>
          <span class="n">source_dim</span><span class="o">=</span><span class="n">source_dim</span><span class="p">,</span>
          <span class="n">query_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
          <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
          <span class="n">num_attention_heads</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
          <span class="n">inner_atten_params</span><span class="o">=</span><span class="n">iap</span><span class="p">,</span>
          <span class="n">use_source_vec_as_attention_value</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
          <span class="n">vn</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">VariationalNoiseParams</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
          <span class="n">packed_input</span><span class="o">=</span><span class="n">packed_input</span><span class="p">)</span>
      <span class="n">atten</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">cls</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
      <span class="n">atten</span><span class="o">.</span><span class="n">InitForSourcePacked</span><span class="p">(</span><span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span>
                                <span class="n">source_padding</span><span class="p">,</span> <span class="n">source_seg_id</span><span class="p">)</span>
      <span class="n">atten_vec</span><span class="p">,</span> <span class="n">atten_prob</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">ComputeContextVector</span><span class="p">(</span>
          <span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">query_segment_id</span><span class="o">=</span><span class="n">query_seg_id</span><span class="p">)</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
      <span class="n">atten_vec_out</span><span class="p">,</span> <span class="n">prob_out</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">atten_vec</span><span class="p">,</span> <span class="n">atten_prob</span><span class="p">])</span>
      <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;atten_vec_out&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">atten_vec_out</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">expected_vec</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">atten_vec_out</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
      <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;atten_vec_out&#39;</span><span class="p">,</span> <span class="n">atten_vec_out</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;prob_out&#39;</span><span class="p">,</span> <span class="n">prob_out</span><span class="p">)</span>
      <span class="n">t_batch_size</span> <span class="o">=</span> <span class="mi">6</span>
      <span class="n">s_batch_size</span> <span class="o">=</span> <span class="mi">3</span>
      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">t_batch_size</span><span class="p">):</span>
        <span class="c1"># Test to make sure we didn&#39;t mess up indexing.</span>
        <span class="n">s_index</span> <span class="o">=</span> <span class="n">i</span> <span class="o">%</span> <span class="n">s_batch_size</span>
        <span class="n">src_seg_id</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">packed_input</span><span class="p">:</span>
          <span class="n">src_seg_id</span> <span class="o">=</span> <span class="n">source_seg_id</span><span class="p">[:,</span> <span class="n">s_index</span><span class="p">:</span><span class="n">s_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">atten</span><span class="o">.</span><span class="n">InitForSourcePacked</span><span class="p">(</span>
            <span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">source_vecs</span><span class="p">[:,</span> <span class="n">s_index</span><span class="p">:</span><span class="n">s_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:],</span>
            <span class="n">source_contexts</span><span class="p">[:,</span> <span class="n">s_index</span><span class="p">:</span><span class="n">s_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:],</span>
            <span class="n">source_padding</span><span class="p">[:,</span> <span class="n">s_index</span><span class="p">:</span><span class="n">s_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="n">src_seg_id</span><span class="p">)</span>
        <span class="n">qry_seg_id</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">packed_input</span><span class="p">:</span>
          <span class="n">qry_seg_id</span> <span class="o">=</span> <span class="n">query_seg_id</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">atten_vec_i</span><span class="p">,</span> <span class="n">prob_i</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">ComputeContextVector</span><span class="p">(</span>
            <span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="n">query_segment_id</span><span class="o">=</span><span class="n">qry_seg_id</span><span class="p">)</span>
        <span class="n">atten_vec_i_out</span><span class="p">,</span> <span class="n">prob_i_out</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">atten_vec_i</span><span class="p">,</span> <span class="n">prob_i</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">prob_i_out</span><span class="p">,</span> <span class="n">prob_out</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">atten_vec_i_out</span><span class="p">,</span> <span class="n">atten_vec_out</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
        <span class="n">padding_i</span> <span class="o">=</span> <span class="n">source_padding_p</span><span class="p">[</span><span class="n">s_index</span><span class="p">]</span>
        <span class="c1"># Check to make sure prob exists only on valid timesteps.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">padding_i</span> <span class="o">*</span> <span class="n">prob_i_out</span><span class="p">))</span>

<div class="viewcode-block" id="AttentionTest.testMultiHeadedAttentionAdditive"><a class="viewcode-back" href="../../../lingvo.core.attention_test.html#lingvo.core.attention_test.AttentionTest.testMultiHeadedAttentionAdditive">[docs]</a>  <span class="k">def</span> <span class="nf">testMultiHeadedAttentionAdditive</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_testMultiHeadedAttentionAdditiveHelper</span><span class="p">(</span>
        <span class="mi">4</span><span class="p">,</span> <span class="p">[</span><span class="mf">2.858081</span><span class="p">,</span> <span class="mf">2.33295</span><span class="p">,</span> <span class="mf">3.529434</span><span class="p">,</span> <span class="mf">2.856466</span><span class="p">,</span> <span class="mf">2.342262</span><span class="p">,</span> <span class="mf">3.526487</span><span class="p">])</span></div>

<div class="viewcode-block" id="AttentionTest.testMultiHeadedAttentionAdditivePackedInput"><a class="viewcode-back" href="../../../lingvo.core.attention_test.html#lingvo.core.attention_test.AttentionTest.testMultiHeadedAttentionAdditivePackedInput">[docs]</a>  <span class="k">def</span> <span class="nf">testMultiHeadedAttentionAdditivePackedInput</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_testMultiHeadedAttentionAdditiveHelper</span><span class="p">(</span>
        <span class="mi">4</span><span class="p">,</span> <span class="p">[</span><span class="mf">2.585192</span><span class="p">,</span> <span class="mf">2.267683</span><span class="p">,</span> <span class="mf">3.708972</span><span class="p">,</span> <span class="mf">3.107646</span><span class="p">,</span> <span class="mf">2.770367</span><span class="p">,</span> <span class="mf">3.580353</span><span class="p">],</span>
        <span class="n">packed_input</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></div>

<div class="viewcode-block" id="AttentionTest.testMultiHeadedAttentionAdditiveUnequalDim"><a class="viewcode-back" href="../../../lingvo.core.attention_test.html#lingvo.core.attention_test.AttentionTest.testMultiHeadedAttentionAdditiveUnequalDim">[docs]</a>  <span class="k">def</span> <span class="nf">testMultiHeadedAttentionAdditiveUnequalDim</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_testMultiHeadedAttentionAdditiveHelper</span><span class="p">(</span>
        <span class="mi">14</span><span class="p">,</span> <span class="p">[</span><span class="mf">3.189594</span><span class="p">,</span> <span class="mf">2.462574</span><span class="p">,</span> <span class="mf">2.912001</span><span class="p">,</span> <span class="mf">3.19924</span><span class="p">,</span> <span class="mf">2.462459</span><span class="p">,</span> <span class="mf">2.909231</span><span class="p">])</span></div>

<div class="viewcode-block" id="AttentionTest.testLocationSensitiveAttention1"><a class="viewcode-back" href="../../../lingvo.core.attention_test.html#lingvo.core.attention_test.AttentionTest.testLocationSensitiveAttention1">[docs]</a>  <span class="k">def</span> <span class="nf">testLocationSensitiveAttention1</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="p">(</span><span class="n">use_gpu</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
      <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">12345</span><span class="p">)</span>
      <span class="n">source_vecs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
      <span class="p">])</span>
      <span class="n">source_contexts</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
      <span class="p">])</span>
      <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span>
              <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span>
              <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
      <span class="n">query_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

      <span class="n">params</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">LocationSensitiveAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
      <span class="n">params</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;loc_atten&#39;</span>
      <span class="n">params</span><span class="o">.</span><span class="n">params_init</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Gaussian</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">12345</span><span class="p">)</span>
      <span class="n">params</span><span class="o">.</span><span class="n">source_dim</span> <span class="o">=</span> <span class="mi">4</span>
      <span class="n">params</span><span class="o">.</span><span class="n">query_dim</span> <span class="o">=</span> <span class="mi">7</span>
      <span class="n">params</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">7</span>
      <span class="n">params</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">global_vn</span> <span class="o">=</span> <span class="kc">False</span>
      <span class="n">params</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">per_step_vn</span> <span class="o">=</span> <span class="kc">False</span>
      <span class="n">params</span><span class="o">.</span><span class="n">location_filter_size</span> <span class="o">=</span> <span class="mi">3</span>
      <span class="n">params</span><span class="o">.</span><span class="n">location_num_filters</span> <span class="o">=</span> <span class="mi">4</span>

      <span class="n">atten</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">LocationSensitiveAttention</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
      <span class="n">atten</span><span class="o">.</span><span class="n">InitForSourcePacked</span><span class="p">(</span><span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span>
                                <span class="n">source_padding</span><span class="p">)</span>

      <span class="n">atten_init_state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

      <span class="n">atten_vec</span><span class="p">,</span> <span class="n">atten_prob</span><span class="p">,</span> <span class="n">atten_state</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">ComputeContextVector</span><span class="p">(</span>
          <span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">atten_init_state</span><span class="p">)</span>

      <span class="n">atten_vars</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="s1">&#39;LocationSensitiveAttention_vars&#39;</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">atten_vars</span><span class="p">))</span>

      <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>

      <span class="n">atten_vec_out</span><span class="p">,</span> <span class="n">prob_out</span><span class="p">,</span> <span class="n">atten_init_state_out</span><span class="p">,</span> <span class="n">atten_state_out</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
          <span class="p">[</span><span class="n">atten_vec</span><span class="p">,</span> <span class="n">atten_prob</span><span class="p">,</span> <span class="n">atten_init_state</span><span class="p">,</span> <span class="n">atten_state</span><span class="p">])</span>

      <span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="n">atten_init_state_out</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">atten_state_out</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

      <span class="nb">print</span><span class="p">([</span><span class="s1">&#39;additive attention prob_out&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array_repr</span><span class="p">(</span><span class="n">prob_out</span><span class="p">)])</span>
      <span class="nb">print</span><span class="p">([</span><span class="s1">&#39;additive attention atten_vec_out&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array_repr</span><span class="p">(</span><span class="n">atten_vec_out</span><span class="p">)])</span>

      <span class="c1"># pyformat: disable</span>
      <span class="c1"># pylint: disable=bad-whitespace</span>
      <span class="n">expected_prob_out</span> <span class="o">=</span> <span class="p">[</span>
          <span class="p">[</span> <span class="mf">0.25557119</span><span class="p">,</span>  <span class="mf">0.2407331</span> <span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>  <span class="mf">0.25413439</span><span class="p">,</span>
            <span class="mf">0.24956135</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.</span>        <span class="p">,</span>  <span class="mf">0.2539435</span> <span class="p">,</span>  <span class="mf">0.24765202</span><span class="p">,</span>  <span class="mf">0.25480285</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>
            <span class="mf">0.24360162</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.25094694</span><span class="p">,</span>  <span class="mf">0.25000173</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>  <span class="mf">0.24308425</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>
            <span class="mf">0.25596702</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.25559491</span><span class="p">,</span>  <span class="mf">0.24071115</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>  <span class="mf">0.2541317</span> <span class="p">,</span>
            <span class="mf">0.24956223</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.</span>        <span class="p">,</span>  <span class="mf">0.25393987</span><span class="p">,</span>  <span class="mf">0.24765508</span><span class="p">,</span>  <span class="mf">0.25481141</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>
            <span class="mf">0.24359357</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.25112614</span><span class="p">,</span>  <span class="mf">0.24990462</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>  <span class="mf">0.24246819</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>
            <span class="mf">0.25650105</span><span class="p">]]</span>
      <span class="n">expected_atten_vec_out</span> <span class="o">=</span> <span class="p">[</span>
          <span class="p">[</span> <span class="mf">0.49745601</span><span class="p">,</span>  <span class="mf">0.63471878</span><span class="p">,</span>  <span class="mf">0.49220741</span><span class="p">,</span>  <span class="mf">0.56829882</span><span class="p">,</span>  <span class="mf">0.42753279</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.51502693</span><span class="p">,</span>  <span class="mf">0.56184328</span><span class="p">,</span>  <span class="mf">0.37644374</span><span class="p">,</span>  <span class="mf">0.87425017</span><span class="p">,</span>  <span class="mf">0.46182287</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.57862061</span><span class="p">,</span>  <span class="mf">0.44247472</span><span class="p">,</span>  <span class="mf">0.36931327</span><span class="p">,</span>  <span class="mf">0.41002682</span><span class="p">,</span>  <span class="mf">0.14327496</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.49745524</span><span class="p">,</span>  <span class="mf">0.63471991</span><span class="p">,</span>  <span class="mf">0.49221092</span><span class="p">,</span>  <span class="mf">0.56828701</span><span class="p">,</span>  <span class="mf">0.427522</span>  <span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.51502484</span><span class="p">,</span>  <span class="mf">0.5618462</span> <span class="p">,</span>  <span class="mf">0.37644884</span><span class="p">,</span>  <span class="mf">0.87424958</span><span class="p">,</span>  <span class="mf">0.46181911</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.57893252</span><span class="p">,</span>  <span class="mf">0.44248456</span><span class="p">,</span>  <span class="mf">0.36938512</span><span class="p">,</span>  <span class="mf">0.4100675</span> <span class="p">,</span>  <span class="mf">0.14328022</span><span class="p">]]</span>
      <span class="c1"># pyformat: enable</span>
      <span class="c1"># pylint: enable=bad-whitespace</span>

      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">expected_prob_out</span><span class="p">,</span> <span class="n">prob_out</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">expected_atten_vec_out</span><span class="p">,</span> <span class="n">atten_vec_out</span><span class="p">)</span></div>

<div class="viewcode-block" id="AttentionTest.testLocationSensitiveAttention2"><a class="viewcode-back" href="../../../lingvo.core.attention_test.html#lingvo.core.attention_test.AttentionTest.testLocationSensitiveAttention2">[docs]</a>  <span class="k">def</span> <span class="nf">testLocationSensitiveAttention2</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="p">(</span><span class="n">use_gpu</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
      <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">12345</span><span class="p">)</span>
      <span class="n">source_vecs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
      <span class="p">])</span>
      <span class="n">source_contexts</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
      <span class="p">])</span>
      <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span>
              <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span>
              <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
      <span class="n">query_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

      <span class="n">params</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">LocationSensitiveAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
      <span class="n">params</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;loc_atten&#39;</span>
      <span class="n">params</span><span class="o">.</span><span class="n">params_init</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Gaussian</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">12345</span><span class="p">)</span>
      <span class="n">params</span><span class="o">.</span><span class="n">source_dim</span> <span class="o">=</span> <span class="mi">4</span>
      <span class="n">params</span><span class="o">.</span><span class="n">query_dim</span> <span class="o">=</span> <span class="mi">7</span>
      <span class="n">params</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">7</span>
      <span class="n">params</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">global_vn</span> <span class="o">=</span> <span class="kc">False</span>
      <span class="n">params</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">per_step_vn</span> <span class="o">=</span> <span class="kc">False</span>
      <span class="n">params</span><span class="o">.</span><span class="n">location_filter_size</span> <span class="o">=</span> <span class="mi">3</span>
      <span class="n">params</span><span class="o">.</span><span class="n">location_num_filters</span> <span class="o">=</span> <span class="mi">4</span>
      <span class="n">params</span><span class="o">.</span><span class="n">location_features</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;PREV_PROBS&#39;</span><span class="p">,</span> <span class="s1">&#39;CUMULATIVE_PROBS&#39;</span><span class="p">]</span>

      <span class="n">atten</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">LocationSensitiveAttention</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
      <span class="n">atten</span><span class="o">.</span><span class="n">InitForSourcePacked</span><span class="p">(</span><span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span>
                                <span class="n">source_padding</span><span class="p">)</span>

      <span class="n">atten_init_state</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">ZeroAttentionState</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">source_vecs</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">6</span><span class="p">)</span>

      <span class="p">(</span><span class="n">unused_atten_vec</span><span class="p">,</span>
       <span class="n">unused_atten_prob</span><span class="p">,</span> <span class="n">atten_state</span><span class="p">)</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">ComputeContextVector</span><span class="p">(</span>
           <span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">atten_init_state</span><span class="p">)</span>

      <span class="n">atten_vars</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="s1">&#39;LocationSensitiveAttention_vars&#39;</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">atten_vars</span><span class="p">))</span>

      <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>

      <span class="n">atten_init_state_out</span><span class="p">,</span> <span class="n">atten_state_out</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
          <span class="p">[</span><span class="n">atten_init_state</span><span class="p">,</span> <span class="n">atten_state</span><span class="p">])</span>

      <span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="n">atten_init_state_out</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">atten_state_out</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span></div>

  <span class="k">def</span> <span class="nf">_testLocationSensitiveAttentionSameBatchSizeHelper</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">same_batch_size</span><span class="p">):</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">(),</span> <span class="n">use_gpu</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
      <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">12345</span><span class="p">)</span>
      <span class="n">source_vecs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
      <span class="p">])</span>
      <span class="n">source_contexts</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
      <span class="p">])</span>
      <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span>
              <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span>
              <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">))</span>

      <span class="n">query_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

      <span class="n">params</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">LocationSensitiveAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
      <span class="n">params</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float64</span>
      <span class="n">params</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;loc_atten&#39;</span>
      <span class="n">params</span><span class="o">.</span><span class="n">params_init</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Gaussian</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">12345</span><span class="p">)</span>
      <span class="n">params</span><span class="o">.</span><span class="n">source_dim</span> <span class="o">=</span> <span class="mi">4</span>
      <span class="n">params</span><span class="o">.</span><span class="n">query_dim</span> <span class="o">=</span> <span class="mi">7</span>
      <span class="n">params</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">7</span>
      <span class="n">params</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">global_vn</span> <span class="o">=</span> <span class="kc">False</span>
      <span class="n">params</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">per_step_vn</span> <span class="o">=</span> <span class="kc">False</span>
      <span class="n">params</span><span class="o">.</span><span class="n">location_filter_size</span> <span class="o">=</span> <span class="mi">3</span>
      <span class="n">params</span><span class="o">.</span><span class="n">location_num_filters</span> <span class="o">=</span> <span class="mi">4</span>
      <span class="n">params</span><span class="o">.</span><span class="n">same_batch_size</span> <span class="o">=</span> <span class="n">same_batch_size</span>

      <span class="n">atten</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">LocationSensitiveAttention</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
      <span class="n">atten</span><span class="o">.</span><span class="n">InitForSourcePacked</span><span class="p">(</span><span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span>
                                <span class="n">source_padding</span><span class="p">)</span>

      <span class="n">atten_init_state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">))</span>

      <span class="n">atten_vec</span><span class="p">,</span> <span class="n">atten_prob</span><span class="p">,</span> <span class="n">atten_state</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">ComputeContextVector</span><span class="p">(</span>
          <span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">atten_init_state</span><span class="p">)</span>

      <span class="n">atten_vars</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="s1">&#39;LocationSensitiveAttention_vars&#39;</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">atten_vars</span><span class="p">))</span>

      <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>

      <span class="n">atten_vec_out</span><span class="p">,</span> <span class="n">prob_out</span><span class="p">,</span> <span class="n">atten_init_state_out</span><span class="p">,</span> <span class="n">atten_state_out</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
          <span class="p">[</span><span class="n">atten_vec</span><span class="p">,</span> <span class="n">atten_prob</span><span class="p">,</span> <span class="n">atten_init_state</span><span class="p">,</span> <span class="n">atten_state</span><span class="p">])</span>

      <span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="n">atten_init_state_out</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">atten_state_out</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">atten_vec_out</span><span class="p">,</span> <span class="n">prob_out</span><span class="p">,</span> <span class="n">atten_init_state_out</span><span class="p">,</span> <span class="n">atten_state_out</span>

<div class="viewcode-block" id="AttentionTest.testLocationSensitiveAttentionSameBatchSize"><a class="viewcode-back" href="../../../lingvo.core.attention_test.html#lingvo.core.attention_test.AttentionTest.testLocationSensitiveAttentionSameBatchSize">[docs]</a>  <span class="k">def</span> <span class="nf">testLocationSensitiveAttentionSameBatchSize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">same_batch_size</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="p">(</span><span class="n">atten_vec_out1</span><span class="p">,</span> <span class="n">prob_out1</span><span class="p">,</span> <span class="n">atten_init_state_out1</span><span class="p">,</span> <span class="n">atten_state_out1</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_testLocationSensitiveAttentionSameBatchSizeHelper</span><span class="p">(</span><span class="kc">True</span><span class="p">))</span>
    <span class="p">(</span><span class="n">atten_vec_out2</span><span class="p">,</span> <span class="n">prob_out2</span><span class="p">,</span> <span class="n">atten_init_state_out2</span><span class="p">,</span> <span class="n">atten_state_out2</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_testLocationSensitiveAttentionSameBatchSizeHelper</span><span class="p">(</span><span class="kc">False</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">atten_vec_out1</span><span class="p">,</span> <span class="n">atten_vec_out2</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-04</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-04</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">prob_out1</span><span class="p">,</span> <span class="n">prob_out2</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-04</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-04</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span>
        <span class="n">atten_init_state_out1</span><span class="p">,</span> <span class="n">atten_init_state_out2</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-04</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-04</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span>
        <span class="n">atten_state_out1</span><span class="p">,</span> <span class="n">atten_state_out2</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-04</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-04</span><span class="p">)</span></div>

  <span class="k">def</span> <span class="nf">_attentionStateWithRandomEmitProbabilities</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                                 <span class="n">atten</span><span class="p">,</span>
                                                 <span class="n">batch_size</span><span class="p">,</span>
                                                 <span class="n">time</span><span class="p">,</span>
                                                 <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
    <span class="n">atten_state</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">ZeroAttentionState</span><span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
    <span class="n">atten_state</span><span class="o">.</span><span class="n">emit_probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">time</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">atten_state</span>

<div class="viewcode-block" id="AttentionTest.testMonotonicAttention"><a class="viewcode-back" href="../../../lingvo.core.attention_test.html#lingvo.core.attention_test.AttentionTest.testMonotonicAttention">[docs]</a>  <span class="k">def</span> <span class="nf">testMonotonicAttention</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="p">(</span><span class="n">use_gpu</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
      <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">12345</span><span class="p">)</span>
      <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">3</span>
      <span class="n">source_dim</span> <span class="o">=</span> <span class="mi">4</span>
      <span class="n">context_dim</span> <span class="o">=</span> <span class="mi">5</span>
      <span class="n">time</span> <span class="o">=</span> <span class="mi">6</span>
      <span class="n">query_dim</span> <span class="o">=</span> <span class="mi">7</span>
      <span class="n">source_vecs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span>
          <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">source_dim</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
      <span class="n">source_contexts</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span>
          <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">context_dim</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
      <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span>
              <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span>
              <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
      <span class="n">query_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span>
          <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">query_dim</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

      <span class="n">params</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">MonotonicAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
      <span class="n">params</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;monotonic_attention&#39;</span>
      <span class="n">params</span><span class="o">.</span><span class="n">params_init</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Gaussian</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">12345</span><span class="p">)</span>
      <span class="n">params</span><span class="o">.</span><span class="n">source_dim</span> <span class="o">=</span> <span class="n">source_dim</span>
      <span class="n">params</span><span class="o">.</span><span class="n">query_dim</span> <span class="o">=</span> <span class="n">query_dim</span>
      <span class="n">params</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">query_dim</span>
      <span class="n">params</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">global_vn</span> <span class="o">=</span> <span class="kc">False</span>
      <span class="n">params</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">per_step_vn</span> <span class="o">=</span> <span class="kc">False</span>

      <span class="n">atten</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">MonotonicAttention</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
      <span class="n">atten</span><span class="o">.</span><span class="n">InitForSourcePacked</span><span class="p">(</span><span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span>
                                <span class="n">source_padding</span><span class="p">)</span>

      <span class="n">atten_init_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attentionStateWithRandomEmitProbabilities</span><span class="p">(</span>
          <span class="n">atten</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">time</span><span class="p">)</span>
      <span class="n">atten_vec</span><span class="p">,</span> <span class="n">atten_prob</span><span class="p">,</span> <span class="n">atten_state</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">ComputeContextVector</span><span class="p">(</span>
          <span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">atten_init_state</span><span class="p">)</span>

      <span class="n">atten_vars</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="s1">&#39;MonotonicAttention_vars&#39;</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">atten_vars</span><span class="p">))</span>

      <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>

      <span class="n">atten_vec_out</span><span class="p">,</span> <span class="n">prob_out</span><span class="p">,</span> <span class="n">atten_init_state_out</span><span class="p">,</span> <span class="n">atten_state_out</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
          <span class="p">[</span><span class="n">atten_vec</span><span class="p">,</span> <span class="n">atten_prob</span><span class="p">,</span> <span class="n">atten_init_state</span><span class="p">,</span> <span class="n">atten_state</span><span class="p">])</span>

      <span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="n">atten_init_state_out</span><span class="o">.</span><span class="n">emit_probs</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
                       <span class="n">atten_state_out</span><span class="o">.</span><span class="n">emit_probs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

      <span class="nb">print</span><span class="p">([</span><span class="s1">&#39;monotonic attention prob_out&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array_repr</span><span class="p">(</span><span class="n">prob_out</span><span class="p">)])</span>
      <span class="nb">print</span><span class="p">([</span><span class="s1">&#39;monotonic attention atten_vec_out&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array_repr</span><span class="p">(</span><span class="n">atten_vec_out</span><span class="p">)])</span>

      <span class="n">expected_prob_out</span> <span class="o">=</span> <span class="p">[[</span>
          <span class="mf">0.03654566</span><span class="p">,</span> <span class="mf">0.05925026</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.20958641</span><span class="p">,</span> <span class="mf">0.19560105</span>
      <span class="p">],</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.09670404</span><span class="p">,</span> <span class="mf">0.13182665</span><span class="p">,</span> <span class="mf">0.13221622</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span>
          <span class="mf">0.18074416</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.04112773</span><span class="p">,</span> <span class="mf">0.07072841</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.13837409</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.23935230</span><span class="p">]]</span>

      <span class="n">expected_atten_vec_out</span> <span class="o">=</span> <span class="p">[[</span>
          <span class="mf">0.2937718</span><span class="p">,</span> <span class="mf">0.30372939</span><span class="p">,</span> <span class="mf">0.27034321</span><span class="p">,</span> <span class="mf">0.31328040</span><span class="p">,</span> <span class="mf">0.19393572</span>
      <span class="p">],</span> <span class="p">[</span><span class="mf">0.2553753</span><span class="p">,</span> <span class="mf">0.26388022</span><span class="p">,</span> <span class="mf">0.20429659</span><span class="p">,</span> <span class="mf">0.47469878</span><span class="p">,</span> <span class="mf">0.27512118</span><span class="p">],</span> <span class="p">[</span>
          <span class="mf">0.33394262</span><span class="p">,</span> <span class="mf">0.1191523</span><span class="p">,</span> <span class="mf">0.22405925</span><span class="p">,</span> <span class="mf">0.21366173</span><span class="p">,</span> <span class="mf">0.03946214</span>
      <span class="p">]]</span>

      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">expected_prob_out</span><span class="p">,</span> <span class="n">prob_out</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">expected_atten_vec_out</span><span class="p">,</span> <span class="n">atten_vec_out</span><span class="p">)</span></div>

<div class="viewcode-block" id="AttentionTest.testMonotonicAttentionHard"><a class="viewcode-back" href="../../../lingvo.core.attention_test.html#lingvo.core.attention_test.AttentionTest.testMonotonicAttentionHard">[docs]</a>  <span class="k">def</span> <span class="nf">testMonotonicAttentionHard</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="p">(</span><span class="n">use_gpu</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
      <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">3</span>
      <span class="n">source_dim</span> <span class="o">=</span> <span class="mi">4</span>
      <span class="n">context_dim</span> <span class="o">=</span> <span class="mi">5</span>
      <span class="n">time</span> <span class="o">=</span> <span class="mi">6</span>
      <span class="n">query_dim</span> <span class="o">=</span> <span class="mi">10</span>
      <span class="n">source_vecs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span>
          <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">source_dim</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
      <span class="n">source_contexts</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span>
          <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">context_dim</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
      <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">time</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
      <span class="n">query_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span>
          <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">query_dim</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

      <span class="n">params</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">MonotonicAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
      <span class="n">params</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;monotonic_attention&#39;</span>
      <span class="n">params</span><span class="o">.</span><span class="n">params_init</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Gaussian</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
      <span class="n">params</span><span class="o">.</span><span class="n">source_dim</span> <span class="o">=</span> <span class="n">source_dim</span>
      <span class="n">params</span><span class="o">.</span><span class="n">query_dim</span> <span class="o">=</span> <span class="n">query_dim</span>
      <span class="n">params</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">query_dim</span>
      <span class="n">params</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">global_vn</span> <span class="o">=</span> <span class="kc">False</span>
      <span class="n">params</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">per_step_vn</span> <span class="o">=</span> <span class="kc">False</span>
      <span class="n">params</span><span class="o">.</span><span class="n">hard_sigmoid</span> <span class="o">=</span> <span class="kc">True</span>
      <span class="c1"># To encourage some probabilities to be &gt; 0</span>
      <span class="n">params</span><span class="o">.</span><span class="n">hidden_bias_init</span> <span class="o">=</span> <span class="mf">0.</span>

      <span class="n">atten</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">MonotonicAttention</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
      <span class="n">atten</span><span class="o">.</span><span class="n">InitForSourcePacked</span><span class="p">(</span><span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span>
                                <span class="n">source_padding</span><span class="p">)</span>

      <span class="n">atten_init_state</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">ZeroAttentionState</span><span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>

      <span class="n">_</span><span class="p">,</span> <span class="n">atten_prob</span><span class="p">,</span> <span class="n">atten_state</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">ComputeContextVector</span><span class="p">(</span>
          <span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">atten_init_state</span><span class="p">)</span>

      <span class="n">atten_vars</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="s1">&#39;MonotonicAttention_vars&#39;</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">atten_vars</span><span class="p">))</span>

      <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>

      <span class="n">prob_out</span><span class="p">,</span> <span class="n">atten_state_out</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">atten_prob</span><span class="p">,</span> <span class="n">atten_state</span><span class="p">])</span>
      <span class="nb">print</span><span class="p">([</span><span class="s1">&#39;hard monotonic prob&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array_repr</span><span class="p">(</span><span class="n">prob_out</span><span class="p">)])</span>
      <span class="c1"># Make sure all probabilities are binary</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertTrue</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">logical_or</span><span class="p">(</span><span class="n">prob_out</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="n">prob_out</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)))</span>
      <span class="c1"># Make sure either one index was attended or none were</span>
      <span class="n">prob_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">prob_out</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertTrue</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">logical_or</span><span class="p">(</span><span class="n">prob_sum</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="n">prob_sum</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)))</span>

      <span class="n">query_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span>
          <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">query_dim</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
      <span class="c1"># Feed state back in</span>
      <span class="n">_</span><span class="p">,</span> <span class="n">atten_prob</span><span class="p">,</span> <span class="n">atten_state</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">ComputeContextVector</span><span class="p">(</span>
          <span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">atten_state_out</span><span class="p">)</span>
      <span class="n">prob_out2</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">atten_prob</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">([</span><span class="s1">&#39;hard monotonic prob2&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array_repr</span><span class="p">(</span><span class="n">prob_out2</span><span class="p">)])</span>
      <span class="c1"># Get indices of where attention was assigned at each output timestep</span>
      <span class="n">idx1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">prob_out</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
      <span class="n">idx2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">prob_out2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
      <span class="c1"># Either the index must have increased, or all probs were 0</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertTrue</span><span class="p">(</span>
          <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">logical_or</span><span class="p">(</span><span class="n">idx1</span> <span class="o">&lt;=</span> <span class="n">idx2</span><span class="p">,</span>
                               <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">prob_out2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)))</span></div>

<div class="viewcode-block" id="AttentionTest.testMonotonicAttentionBackProp"><a class="viewcode-back" href="../../../lingvo.core.attention_test.html#lingvo.core.attention_test.AttentionTest.testMonotonicAttentionBackProp">[docs]</a>  <span class="k">def</span> <span class="nf">testMonotonicAttentionBackProp</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="p">(</span><span class="n">use_gpu</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
      <span class="c1"># Use float64 dtype for numeric checks</span>
      <span class="n">dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float64</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">set_random_seed</span><span class="p">(</span><span class="mi">398847392</span><span class="p">)</span>
      <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">12345</span><span class="p">)</span>
      <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">3</span>
      <span class="n">source_dim</span> <span class="o">=</span> <span class="mi">4</span>
      <span class="n">context_dim</span> <span class="o">=</span> <span class="mi">5</span>
      <span class="n">time</span> <span class="o">=</span> <span class="mi">6</span>
      <span class="n">query_dim</span> <span class="o">=</span> <span class="mi">7</span>
      <span class="n">source_vecs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span>
          <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">source_dim</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
      <span class="n">source_contexts</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span>
          <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">context_dim</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
      <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">time</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
      <span class="n">query_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span>
          <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">query_dim</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

      <span class="n">params</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">MonotonicAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
      <span class="n">params</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;monotonic_attention&#39;</span>
      <span class="n">params</span><span class="o">.</span><span class="n">params_init</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Gaussian</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">12345</span><span class="p">)</span>
      <span class="n">params</span><span class="o">.</span><span class="n">source_dim</span> <span class="o">=</span> <span class="n">source_dim</span>
      <span class="n">params</span><span class="o">.</span><span class="n">query_dim</span> <span class="o">=</span> <span class="n">query_dim</span>
      <span class="n">params</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">query_dim</span>
      <span class="n">params</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span>
      <span class="n">params</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">global_vn</span> <span class="o">=</span> <span class="kc">False</span>
      <span class="n">params</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">per_step_vn</span> <span class="o">=</span> <span class="kc">False</span>

      <span class="n">atten</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">MonotonicAttention</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
      <span class="n">atten</span><span class="o">.</span><span class="n">InitForSourcePacked</span><span class="p">(</span><span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span>
                                <span class="n">source_padding</span><span class="p">)</span>

      <span class="n">atten_init_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attentionStateWithRandomEmitProbabilities</span><span class="p">(</span>
          <span class="n">atten</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">time</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

      <span class="n">atten_vec</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">ComputeContextVector</span><span class="p">(</span><span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span>
                                                   <span class="n">atten_init_state</span><span class="p">)</span>

      <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">atten_vec</span><span class="p">)</span>

      <span class="n">all_vars</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">()</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_vars</span><span class="p">))</span>

      <span class="n">grads</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">all_vars</span><span class="p">)</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
      <span class="n">sym_grads</span> <span class="o">=</span> <span class="p">[</span><span class="n">sg</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span> <span class="k">for</span> <span class="n">sg</span> <span class="ow">in</span> <span class="n">grads</span><span class="p">]</span>
      <span class="n">num_grads</span> <span class="o">=</span> <span class="p">[</span>
          <span class="n">test_utils</span><span class="o">.</span><span class="n">ComputeNumericGradient</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">all_vars</span>
      <span class="p">]</span>

      <span class="nb">print</span><span class="p">(</span><span class="n">sym_grads</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">num_grads</span><span class="p">)</span>

      <span class="k">for</span> <span class="n">sg</span><span class="p">,</span> <span class="n">ng</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">sym_grads</span><span class="p">,</span> <span class="n">num_grads</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">sg</span><span class="p">,</span> <span class="n">ng</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-06</span><span class="p">)</span></div>

  <span class="k">def</span> <span class="nf">_testPerStepSourcePaddingHelper</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">atten</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">atten_state</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="p">(</span><span class="n">use_gpu</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
      <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">505837249</span><span class="p">)</span>
      <span class="n">source_vecs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">depth</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
          <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
      <span class="p">])</span>
      <span class="n">source_contexts</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">depth</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
          <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
      <span class="p">])</span>
      <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span>
              <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
      <span class="n">query_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">depth</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
      <span class="n">query_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">query_vec</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>

      <span class="n">atten</span><span class="o">.</span><span class="n">InitForSourcePacked</span><span class="p">(</span><span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span>
                                <span class="n">source_padding</span><span class="p">)</span>
      <span class="c1"># No per_step_padding.</span>
      <span class="n">atten_vec1</span><span class="p">,</span> <span class="n">atten_prob1</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">ComputeContextVector</span><span class="p">(</span>
          <span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span>
          <span class="n">query_vec</span><span class="p">,</span>
          <span class="n">attention_state</span><span class="o">=</span><span class="n">atten_state</span><span class="p">,</span>
          <span class="n">per_step_source_padding</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
      <span class="n">per_step_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span>
          <span class="p">[[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span>
           <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]],</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
      <span class="n">atten_vec2</span><span class="p">,</span> <span class="n">atten_prob2</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">ComputeContextVector</span><span class="p">(</span>
          <span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span>
          <span class="n">query_vec</span><span class="p">,</span>
          <span class="n">attention_state</span><span class="o">=</span><span class="n">atten_state</span><span class="p">,</span>
          <span class="n">per_step_source_padding</span><span class="o">=</span><span class="n">per_step_padding</span><span class="p">)</span>

      <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
      <span class="n">atten_vec1_out</span><span class="p">,</span> <span class="n">atten_prob1_out</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">atten_vec1</span><span class="p">,</span> <span class="n">atten_prob1</span><span class="p">])</span>
      <span class="n">atten_vec2_out</span><span class="p">,</span> <span class="n">atten_prob2_out</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">atten_vec2</span><span class="p">,</span> <span class="n">atten_prob2</span><span class="p">])</span>
      <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;atten_prob1_out&#39;</span><span class="p">,</span> <span class="n">atten_prob1_out</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;atten_prob2_out&#39;</span><span class="p">,</span> <span class="n">atten_prob2_out</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;atten_vec1_out&#39;</span><span class="p">,</span> <span class="n">atten_vec1_out</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;atten_vec2_out&#39;</span><span class="p">,</span> <span class="n">atten_vec2_out</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">atten_prob1_out</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span> <span class="n">atten_prob1_out</span><span class="p">[</span><span class="mi">2</span><span class="p">:])</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">atten_vec1_out</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span> <span class="n">atten_vec1_out</span><span class="p">[</span><span class="mi">2</span><span class="p">:])</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">atten_prob1_out</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">atten_prob2_out</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">atten_vec1_out</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">atten_vec2_out</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">atten_prob1_out</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">atten_prob2_out</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">atten_vec1_out</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">atten_vec2_out</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">atten_prob2_out</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">atten_prob2_out</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">atten_vec2_out</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">atten_vec2_out</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertGreater</span><span class="p">(</span>
          <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">atten_prob1_out</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">atten_prob2_out</span><span class="p">[</span><span class="mi">0</span><span class="p">])),</span> <span class="mf">0.1</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertGreater</span><span class="p">(</span>
          <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">atten_prob1_out</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">atten_prob2_out</span><span class="p">[</span><span class="mi">2</span><span class="p">])),</span> <span class="mf">0.1</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertGreater</span><span class="p">(</span>
          <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">atten_prob2_out</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">atten_prob2_out</span><span class="p">[</span><span class="mi">2</span><span class="p">])),</span> <span class="mf">0.1</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">atten_prob2_out</span><span class="p">,</span> <span class="n">atten_vec2_out</span>

<div class="viewcode-block" id="AttentionTest.testPerStepSourcePaddingAdditiveAttention"><a class="viewcode-back" href="../../../lingvo.core.attention_test.html#lingvo.core.attention_test.AttentionTest.testPerStepSourcePaddingAdditiveAttention">[docs]</a>  <span class="k">def</span> <span class="nf">testPerStepSourcePaddingAdditiveAttention</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">AdditiveAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">params</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;atten&#39;</span>
    <span class="n">params</span><span class="o">.</span><span class="n">params_init</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Gaussian</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">877374</span><span class="p">)</span>
    <span class="n">depth</span> <span class="o">=</span> <span class="mi">6</span>
    <span class="n">params</span><span class="o">.</span><span class="n">source_dim</span> <span class="o">=</span> <span class="n">depth</span>
    <span class="n">params</span><span class="o">.</span><span class="n">query_dim</span> <span class="o">=</span> <span class="n">depth</span>
    <span class="n">params</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">depth</span>
    <span class="n">params</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">global_vn</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">params</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">per_step_vn</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">atten</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">cls</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">prob_out</span><span class="p">,</span> <span class="n">vec_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_testPerStepSourcePaddingHelper</span><span class="p">(</span><span class="n">atten</span><span class="p">,</span> <span class="n">depth</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;vec_out&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array_repr</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">vec_out</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">([</span><span class="mf">2.00084352</span><span class="p">,</span> <span class="mf">3.2933836</span><span class="p">,</span> <span class="mf">2.30622029</span><span class="p">,</span> <span class="mf">3.2933836</span><span class="p">],</span>
                        <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">vec_out</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">prob_out</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span></div>

<div class="viewcode-block" id="AttentionTest.testPerStepSourcePaddingDotProductAttention"><a class="viewcode-back" href="../../../lingvo.core.attention_test.html#lingvo.core.attention_test.AttentionTest.testPerStepSourcePaddingDotProductAttention">[docs]</a>  <span class="k">def</span> <span class="nf">testPerStepSourcePaddingDotProductAttention</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">DotProductAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">params</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;atten&#39;</span>
    <span class="n">depth</span> <span class="o">=</span> <span class="mi">6</span>
    <span class="n">params</span><span class="o">.</span><span class="n">source_dim</span> <span class="o">=</span> <span class="n">depth</span>
    <span class="n">params</span><span class="o">.</span><span class="n">query_dim</span> <span class="o">=</span> <span class="n">depth</span>
    <span class="n">params</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">depth</span>
    <span class="n">params</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">global_vn</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">params</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">per_step_vn</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">atten</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">cls</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">prob_out</span><span class="p">,</span> <span class="n">vec_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_testPerStepSourcePaddingHelper</span><span class="p">(</span><span class="n">atten</span><span class="p">,</span> <span class="n">depth</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;vec_out&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array_repr</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">vec_out</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">([</span><span class="mf">2.02671742</span><span class="p">,</span> <span class="mf">3.38590097</span><span class="p">,</span> <span class="mf">2.34964013</span><span class="p">,</span> <span class="mf">3.38590097</span><span class="p">],</span>
                        <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">vec_out</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">prob_out</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span></div>

<div class="viewcode-block" id="AttentionTest.testPerStepSourcePaddingMultiHeadedAttention"><a class="viewcode-back" href="../../../lingvo.core.attention_test.html#lingvo.core.attention_test.AttentionTest.testPerStepSourcePaddingMultiHeadedAttention">[docs]</a>  <span class="k">def</span> <span class="nf">testPerStepSourcePaddingMultiHeadedAttention</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">MultiHeadedAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">params</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;atten&#39;</span>
    <span class="n">params</span><span class="o">.</span><span class="n">params_init</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Gaussian</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">877374</span><span class="p">)</span>
    <span class="n">depth</span> <span class="o">=</span> <span class="mi">6</span>
    <span class="n">params</span><span class="o">.</span><span class="n">source_dim</span> <span class="o">=</span> <span class="n">depth</span>
    <span class="n">params</span><span class="o">.</span><span class="n">query_dim</span> <span class="o">=</span> <span class="n">depth</span>
    <span class="n">params</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">depth</span>
    <span class="n">params</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">global_vn</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">params</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">per_step_vn</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">atten</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">cls</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">prob_out</span><span class="p">,</span> <span class="n">vec_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_testPerStepSourcePaddingHelper</span><span class="p">(</span><span class="n">atten</span><span class="p">,</span> <span class="n">depth</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;vec_out&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array_repr</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">vec_out</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">([</span><span class="o">-</span><span class="mf">0.006338</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.025153</span><span class="p">,</span> <span class="mf">0.041647</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.025153</span><span class="p">],</span>
                        <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">vec_out</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">prob_out</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span></div>

<div class="viewcode-block" id="AttentionTest.testPerStepSourcePaddingLocationSensitiveAttention"><a class="viewcode-back" href="../../../lingvo.core.attention_test.html#lingvo.core.attention_test.AttentionTest.testPerStepSourcePaddingLocationSensitiveAttention">[docs]</a>  <span class="k">def</span> <span class="nf">testPerStepSourcePaddingLocationSensitiveAttention</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">LocationSensitiveAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">params</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;atten&#39;</span>
    <span class="n">params</span><span class="o">.</span><span class="n">params_init</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Gaussian</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">877374</span><span class="p">)</span>
    <span class="n">depth</span> <span class="o">=</span> <span class="mi">6</span>
    <span class="n">params</span><span class="o">.</span><span class="n">source_dim</span> <span class="o">=</span> <span class="n">depth</span>
    <span class="n">params</span><span class="o">.</span><span class="n">query_dim</span> <span class="o">=</span> <span class="n">depth</span>
    <span class="n">params</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">depth</span>
    <span class="n">params</span><span class="o">.</span><span class="n">location_filter_size</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="n">params</span><span class="o">.</span><span class="n">location_num_filters</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="n">params</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">global_vn</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">params</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">per_step_vn</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">atten_state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
        <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
         <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)],</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">atten_state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">atten_state</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">atten</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">cls</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">prob_out</span><span class="p">,</span> <span class="n">vec_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_testPerStepSourcePaddingHelper</span><span class="p">(</span>
        <span class="n">atten</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">atten_state</span><span class="o">=</span><span class="n">atten_state</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;vec_out&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array_repr</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">vec_out</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">([</span><span class="mf">2.001103</span><span class="p">,</span> <span class="mf">3.293414</span><span class="p">,</span> <span class="mf">2.306448</span><span class="p">,</span> <span class="mf">3.293414</span><span class="p">],</span>
                        <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">vec_out</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">prob_out</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span></div>

<div class="viewcode-block" id="AttentionTest.testPerStepSourcePaddingMonotonicAttention"><a class="viewcode-back" href="../../../lingvo.core.attention_test.html#lingvo.core.attention_test.AttentionTest.testPerStepSourcePaddingMonotonicAttention">[docs]</a>  <span class="k">def</span> <span class="nf">testPerStepSourcePaddingMonotonicAttention</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">MonotonicAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">params</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;atten&#39;</span>
    <span class="n">params</span><span class="o">.</span><span class="n">params_init</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Gaussian</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">877374</span><span class="p">)</span>
    <span class="n">depth</span> <span class="o">=</span> <span class="mi">6</span>
    <span class="n">params</span><span class="o">.</span><span class="n">source_dim</span> <span class="o">=</span> <span class="n">depth</span>
    <span class="n">params</span><span class="o">.</span><span class="n">query_dim</span> <span class="o">=</span> <span class="n">depth</span>
    <span class="n">params</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">depth</span>
    <span class="n">params</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">global_vn</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">params</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">per_step_vn</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">atten</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">cls</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">atten_state</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">ZeroAttentionState</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">atten_state</span><span class="o">.</span><span class="n">emit_probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
        <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
         <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)],</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">prob_out</span><span class="p">,</span> <span class="n">vec_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_testPerStepSourcePaddingHelper</span><span class="p">(</span>
        <span class="n">atten</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">atten_state</span><span class="o">=</span><span class="n">atten_state</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;prob_out&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array_repr</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">prob_out</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;vec_out&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array_repr</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">vec_out</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span></div>

<div class="viewcode-block" id="AttentionTest.testGmmMonotonicAttentionDropout"><a class="viewcode-back" href="../../../lingvo.core.attention_test.html#lingvo.core.attention_test.AttentionTest.testGmmMonotonicAttentionDropout">[docs]</a>  <span class="k">def</span> <span class="nf">testGmmMonotonicAttentionDropout</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">GmmMonotonicAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;gmm_monotonic_attention&#39;</span><span class="p">,</span> <span class="n">atten_dropout_prob</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">assertRaises</span><span class="p">(</span><span class="ne">NotImplementedError</span><span class="p">):</span>
      <span class="n">p</span><span class="o">.</span><span class="n">cls</span><span class="p">(</span><span class="n">p</span><span class="p">)</span></div>

<div class="viewcode-block" id="AttentionTest.testGmmMonotonicAttention"><a class="viewcode-back" href="../../../lingvo.core.attention_test.html#lingvo.core.attention_test.AttentionTest.testGmmMonotonicAttention">[docs]</a>  <span class="k">def</span> <span class="nf">testGmmMonotonicAttention</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="p">(</span><span class="n">use_gpu</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
      <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">12345</span><span class="p">)</span>
      <span class="n">source_vecs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
      <span class="p">])</span>
      <span class="n">source_contexts</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
      <span class="p">])</span>
      <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span>
              <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span>
              <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
      <span class="n">query_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

      <span class="n">params</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">GmmMonotonicAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
      <span class="n">params</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;gmm_atten&#39;</span>
      <span class="n">params</span><span class="o">.</span><span class="n">params_init</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Gaussian</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">12345</span><span class="p">)</span>
      <span class="n">params</span><span class="o">.</span><span class="n">source_dim</span> <span class="o">=</span> <span class="mi">4</span>
      <span class="n">params</span><span class="o">.</span><span class="n">query_dim</span> <span class="o">=</span> <span class="mi">7</span>
      <span class="n">params</span><span class="o">.</span><span class="n">gmm_mlp_hidden_dim</span> <span class="o">=</span> <span class="mi">7</span>
      <span class="n">params</span><span class="o">.</span><span class="n">num_mixtures</span> <span class="o">=</span> <span class="mi">2</span>
      <span class="n">params</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">global_vn</span> <span class="o">=</span> <span class="kc">False</span>
      <span class="n">params</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">per_step_vn</span> <span class="o">=</span> <span class="kc">False</span>

      <span class="n">atten</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">GmmMonotonicAttention</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
      <span class="n">atten</span><span class="o">.</span><span class="n">InitForSourcePacked</span><span class="p">(</span><span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span>
                                <span class="n">source_padding</span><span class="p">)</span>
      <span class="c1"># target_batch=6</span>
      <span class="n">atten_init_state</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">ZeroAttentionState</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">source_vecs</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">6</span><span class="p">)</span>

      <span class="n">atten_vec</span><span class="p">,</span> <span class="n">atten_prob</span><span class="p">,</span> <span class="n">atten_state</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">ComputeContextVector</span><span class="p">(</span>
          <span class="n">atten</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">atten_init_state</span><span class="p">)</span>

      <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>

      <span class="n">atten_vec_out</span><span class="p">,</span> <span class="n">prob_out</span><span class="p">,</span> <span class="n">atten_init_state_out</span><span class="p">,</span> <span class="n">atten_state_out</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
          <span class="p">[</span><span class="n">atten_vec</span><span class="p">,</span> <span class="n">atten_prob</span><span class="p">,</span> <span class="n">atten_init_state</span><span class="p">,</span> <span class="n">atten_state</span><span class="p">])</span>

      <span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="n">atten_init_state_out</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">atten_state_out</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="n">atten_init_state_out</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

      <span class="nb">print</span><span class="p">([</span><span class="s1">&#39;gmm attention prob_out&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array_repr</span><span class="p">(</span><span class="n">prob_out</span><span class="p">)])</span>
      <span class="nb">print</span><span class="p">([</span><span class="s1">&#39;gmm attention atten_vec_out&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array_repr</span><span class="p">(</span><span class="n">atten_vec_out</span><span class="p">)])</span>

      <span class="c1"># pyformat: disable</span>
      <span class="c1"># pylint: disable=bad-whitespace</span>
      <span class="n">expected_prob_out</span> <span class="o">=</span> <span class="p">[</span>
          <span class="p">[</span> <span class="mf">2.45764434e-01</span><span class="p">,</span> <span class="mf">3.97835493e-01</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">4.25808690e-03</span><span class="p">,</span>
            <span class="mf">1.29864624e-04</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">3.98021877e-01</span><span class="p">,</span> <span class="mf">2.37964690e-01</span><span class="p">,</span> <span class="mf">5.23146540e-02</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span>
            <span class="mf">1.29256863e-04</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">2.46294901e-01</span><span class="p">,</span> <span class="mf">3.97767872e-01</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">5.21243662e-02</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span>
            <span class="mf">1.29372784e-04</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">2.45875627e-01</span><span class="p">,</span> <span class="mf">3.97635251e-01</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">4.27022483e-03</span><span class="p">,</span>
            <span class="mf">1.30706903e-04</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">3.97709191e-01</span><span class="p">,</span> <span class="mf">2.37897262e-01</span><span class="p">,</span> <span class="mf">5.24106659e-02</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span>
            <span class="mf">1.30714150e-04</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">2.46048093e-01</span><span class="p">,</span> <span class="mf">3.97871077e-01</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">5.21884784e-02</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span>
            <span class="mf">1.29211781e-04</span><span class="p">]]</span>
      <span class="n">expected_atten_vec_out</span> <span class="o">=</span> <span class="p">[</span>
          <span class="p">[</span> <span class="mf">0.23010808</span><span class="p">,</span>  <span class="mf">0.43757612</span><span class="p">,</span>  <span class="mf">0.25150469</span><span class="p">,</span>  <span class="mf">0.3631629</span> <span class="p">,</span>  <span class="mf">0.37140277</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.54693544</span><span class="p">,</span>  <span class="mf">0.56182981</span><span class="p">,</span>  <span class="mf">0.21333349</span><span class="p">,</span>  <span class="mf">0.58108622</span><span class="p">,</span>  <span class="mf">0.21566363</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.4048025</span> <span class="p">,</span>  <span class="mf">0.53986353</span><span class="p">,</span>  <span class="mf">0.13288836</span><span class="p">,</span>  <span class="mf">0.22497796</span><span class="p">,</span>  <span class="mf">0.17450145</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.23008531</span><span class="p">,</span>  <span class="mf">0.4375343</span> <span class="p">,</span>  <span class="mf">0.25150725</span><span class="p">,</span>  <span class="mf">0.36303982</span><span class="p">,</span>  <span class="mf">0.37127423</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.54661846</span><span class="p">,</span>  <span class="mf">0.5615437</span> <span class="p">,</span>  <span class="mf">0.21332006</span><span class="p">,</span>  <span class="mf">0.58084518</span><span class="p">,</span>  <span class="mf">0.21558265</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.40484226</span><span class="p">,</span>  <span class="mf">0.53978455</span><span class="p">,</span>  <span class="mf">0.13283314</span><span class="p">,</span>  <span class="mf">0.22490481</span><span class="p">,</span>  <span class="mf">0.17447782</span><span class="p">]]</span>
      <span class="c1"># pyformat: enable</span>
      <span class="c1"># pylint: enable=bad-whitespace</span>

      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">expected_prob_out</span><span class="p">,</span> <span class="n">prob_out</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assertAllClose</span><span class="p">(</span><span class="n">expected_atten_vec_out</span><span class="p">,</span> <span class="n">atten_vec_out</span><span class="p">)</span></div></div>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">main</span><span class="p">()</span>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../../',
            VERSION:'',
            LANGUAGE:'en',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>