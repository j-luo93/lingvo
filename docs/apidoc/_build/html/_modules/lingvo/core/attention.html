

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>lingvo.core.attention &mdash; lingvo  documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 

  
  <script src="../../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../../index.html" class="icon icon-home"> lingvo
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../lingvo.html">lingvo package</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">lingvo</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../index.html">Module code</a> &raquo;</li>
        
      <li>lingvo.core.attention</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for lingvo.core.attention</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2018 The TensorFlow Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ==============================================================================</span>
<span class="sd">&quot;&quot;&quot;Attention models.&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">function</span>

<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="k">import</span> <span class="n">base_layer</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="k">import</span> <span class="n">layers</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="k">import</span> <span class="n">py_utils</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="k">import</span> <span class="n">quant_utils</span>


<div class="viewcode-block" id="_ApplyAttentionDropout"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention._ApplyAttentionDropout">[docs]</a><span class="k">def</span> <span class="nf">_ApplyAttentionDropout</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">step_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prng_seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Apply attention dropout according to the given parameters.</span>

<span class="sd">  If `params.atten_dropout_deterministic` is set to True, the dropout will be</span>
<span class="sd">  fully deterministic (requires `step_state` and `prng_seed`).</span>

<span class="sd">  Args:</span>
<span class="sd">    params: The parameters of attention layer.</span>
<span class="sd">    x: A float Tensor on which to apply dropout.</span>
<span class="sd">    step_state: (Optional) A `.NestedMap` with `global_step` and `time_step`.</span>
<span class="sd">      Required for deterministic dropout.</span>
<span class="sd">    prng_seed: (Optional) An int seed for pseudo random number generator.</span>
<span class="sd">      Required for deterministic dropout.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A Tensor with the same shape as `x`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">atten_dropout_prob</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">x</span>

  <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">atten_dropout_deterministic</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">step_state</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">):</span>
      <span class="k">assert</span> <span class="s1">&#39;global_step&#39;</span> <span class="ow">in</span> <span class="n">step_state</span><span class="p">,</span> <span class="n">step_state</span><span class="o">.</span><span class="n">DebugString</span><span class="p">()</span>
      <span class="k">assert</span> <span class="s1">&#39;time_step&#39;</span> <span class="ow">in</span> <span class="n">step_state</span><span class="p">,</span> <span class="n">step_state</span><span class="o">.</span><span class="n">DebugString</span><span class="p">()</span>
      <span class="k">assert</span> <span class="n">prng_seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
      <span class="n">seeds</span> <span class="o">=</span> <span class="n">prng_seed</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span>
          <span class="p">[</span><span class="n">step_state</span><span class="o">.</span><span class="n">global_step</span><span class="p">,</span> <span class="n">step_state</span><span class="o">.</span><span class="n">time_step</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">prng_seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
      <span class="n">seeds</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetOpSeedPair</span><span class="p">(</span><span class="n">prng_seed</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">DeterministicDropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">params</span><span class="o">.</span><span class="n">atten_dropout_prob</span><span class="p">,</span>
                                         <span class="n">seeds</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">seed</span> <span class="o">=</span> <span class="kc">None</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">params</span><span class="o">.</span><span class="n">random_seed</span> <span class="k">else</span> <span class="n">prng_seed</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">params</span><span class="o">.</span><span class="n">atten_dropout_prob</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span></div>


<div class="viewcode-block" id="BaseAttentionLayer"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.BaseAttentionLayer">[docs]</a><span class="k">class</span> <span class="nc">BaseAttentionLayer</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">LayerBase</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A base class for all attention layers.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="BaseAttentionLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.BaseAttentionLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">BaseAttentionLayer</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;atten_dropout_prob&#39;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span>
             <span class="s1">&#39;Probability at which we apply dropout to the attention weights.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;atten_dropout_deterministic&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s1">&#39;Whether to dropout in a fully deterministic way, which is more &#39;</span>
        <span class="s1">&#39;suitable for TPU.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;random_seed&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s1">&#39;If set, this decides the random seed to apply in dropout. &#39;</span>
        <span class="s1">&#39;Only set this random_seed for unit tests.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;packed_input&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
             <span class="s1">&#39;If True, each training example may pack multiple sequences.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="nd">@base_layer</span><span class="o">.</span><span class="n">initializer</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a BaseAttentionLayer object.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">params</span><span class="o">.</span><span class="n">name</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;params.name is not set.&#39;</span><span class="p">)</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">BaseAttentionLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>

    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_source_init_done</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_prng_seed</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GenerateSeedFromName</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">random_seed</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_prng_seed</span> <span class="o">+=</span> <span class="n">p</span><span class="o">.</span><span class="n">random_seed</span>

<div class="viewcode-block" id="BaseAttentionLayer.InitForSourcePacked"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.BaseAttentionLayer.InitForSourcePacked">[docs]</a>  <span class="k">def</span> <span class="nf">InitForSourcePacked</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                          <span class="n">theta</span><span class="p">,</span>
                          <span class="n">source_vecs</span><span class="p">,</span>
                          <span class="n">source_contexts</span><span class="p">,</span>
                          <span class="n">source_padding</span><span class="p">,</span>
                          <span class="n">source_segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initialize attention for the given source vectors.</span>

<span class="sd">    Must set `_source_init_done` to True in the function.</span>

<span class="sd">    Note: `source_segment_id`, if present, should always have the same shape as</span>
<span class="sd">    `source_padding`.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this</span>
<span class="sd">        layer and its children layers.</span>
<span class="sd">      source_vecs: A single tensor of shape [time, batch_size, source_dim].</span>
<span class="sd">      source_contexts: A single tensor of shape [time, batch_size, some_dim].</span>
<span class="sd">      source_padding: A tensor of shape [time, batch_size].</span>
<span class="sd">      source_segment_id: A tensor of shape [time, batch_size].</span>
<span class="sd">        source_segment_id is not None for packed inputs where one training</span>
<span class="sd">        example may pack multiple sequences.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tuple (concated_source_vecs, concated_source_contexts, source_padding,</span>
<span class="sd">      source_segment_id), where `concated_source_vecs` is a tensor of shape</span>
<span class="sd">      [time, batch_size, hidden_dim], `concated_source_contexts` is a tensor</span>
<span class="sd">      of shape [batch_size, time, some_dim], `source_padding` is a tensor of</span>
<span class="sd">      shape [time, batch_size], `source_segment_id` is a tensor of shape</span>
<span class="sd">      [time, batch_size].</span>

<span class="sd">      Note the mismatch between `concated_source_vecs` and</span>
<span class="sd">      `concated_source_contexts`. In `concated_source_vecs`, time is the first</span>
<span class="sd">      dim, while it is the second dim in `concated_source_contexts`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s1">&#39;Abstract method.&#39;</span><span class="p">)</span></div>

<div class="viewcode-block" id="BaseAttentionLayer.ComputeContextVectorWithSource"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.BaseAttentionLayer.ComputeContextVectorWithSource">[docs]</a>  <span class="k">def</span> <span class="nf">ComputeContextVectorWithSource</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                     <span class="n">theta</span><span class="p">,</span>
                                     <span class="n">concated_source_vecs</span><span class="p">,</span>
                                     <span class="n">concated_source_contexts</span><span class="p">,</span>
                                     <span class="n">source_padding</span><span class="p">,</span>
                                     <span class="n">source_segment_id</span><span class="p">,</span>
                                     <span class="n">query_vec</span><span class="p">,</span>
                                     <span class="n">attention_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                     <span class="n">per_step_source_padding</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                     <span class="n">step_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                     <span class="n">query_segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the context vector given the current query output.</span>

<span class="sd">    Note: `concated_source_vecs` are the vectors that are used to compute the</span>
<span class="sd">    attention score between the `query_vec` and each `concated_source_vec`.</span>
<span class="sd">    The `concated_source_contexts` are the vectors that compose the result.</span>
<span class="sd">    The attention context vector is computed as a weighted average of the</span>
<span class="sd">    `concated_source_contexts`, using the scores that were computed using</span>
<span class="sd">    `concated_source_vecs`.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this</span>
<span class="sd">        layer and its children layers.</span>
<span class="sd">      concated_source_vecs: Concated source vectors with shape</span>
<span class="sd">        [time, batch_size, hidden_dim].</span>
<span class="sd">      concated_source_contexts: Concated source contexts with shape</span>
<span class="sd">        [ batch_size, time, context_dim].</span>
<span class="sd">      source_padding: Source padding with shape [time, batch_size].</span>
<span class="sd">      source_segment_id: Source segment ids with shape [time, batch_size].</span>
<span class="sd">      query_vec: a tensor of shape [batch_size, query_dim].</span>
<span class="sd">      attention_state: previous attention state.</span>
<span class="sd">      per_step_source_padding: Source sequence padding to apply at this step. If</span>
<span class="sd">        not None, it should have shape [target_batch_size, source_seq_length].</span>
<span class="sd">      step_state: A `.NestedMap` containing `global_step` and `time_step`.</span>
<span class="sd">        Required for deterministic dropout.</span>
<span class="sd">      query_segment_id: a tensor of shape [batch_size].</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tuple of 3 elements.</span>
<span class="sd">        The attention context vector:</span>
<span class="sd">          [batch_size, context_dim]</span>
<span class="sd">        The attention probability vector:</span>
<span class="sd">          [batch_size, time]</span>
<span class="sd">        The new attention mechanism state:</span>
<span class="sd">          possibly nested tuple of tensors with dimensions [target_batch, ...]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s1">&#39;Abstract method.&#39;</span><span class="p">)</span></div>

<div class="viewcode-block" id="BaseAttentionLayer.ComputeContextVector"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.BaseAttentionLayer.ComputeContextVector">[docs]</a>  <span class="k">def</span> <span class="nf">ComputeContextVector</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                           <span class="n">theta</span><span class="p">,</span>
                           <span class="n">query_vec</span><span class="p">,</span>
                           <span class="n">attention_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                           <span class="n">per_step_source_padding</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                           <span class="n">step_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                           <span class="n">query_segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the context vector given the current query output.</span>

<span class="sd">    Unlike `ComputeContextVectorWithSource` which explicitly asks for the source</span>
<span class="sd">    tensors (concated_source_vecs, concated_source_contexts, source_padding),</span>
<span class="sd">    `ComputeContextVector` uses the class&#39; internal variables.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this</span>
<span class="sd">        layer and its children layers.</span>
<span class="sd">      query_vec: a tensor of shape [batch_size, query_dim].</span>
<span class="sd">      attention_state: previous attention state.</span>
<span class="sd">      per_step_source_padding: Source sequence padding to apply at this step.</span>
<span class="sd">        If not None, it should be of shape [target_batch_size,</span>
<span class="sd">        source_seq_length].</span>
<span class="sd">      step_state: A `.NestedMap` containing `global_step` and `time_step`.</span>
<span class="sd">        Required for deterministic dropout.</span>
<span class="sd">      query_segment_id: a tensor of shape [batch_size].</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tuple of 3 elements.</span>

<span class="sd">      * The attention context vector.</span>
<span class="sd">      * The attention probability vector.</span>
<span class="sd">      * The new attention mechanism state: possibly nested tuple of tensors with</span>
<span class="sd">        dimensions [target_batch, ...]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_source_init_done</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">ComputeContextVectorWithSource</span><span class="p">(</span>
        <span class="n">theta</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_concated_source_vecs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_concated_source_contexts</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_source_padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_source_segment_id</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span>
        <span class="n">attention_state</span><span class="p">,</span> <span class="n">per_step_source_padding</span><span class="p">,</span> <span class="n">step_state</span><span class="p">,</span> <span class="n">query_segment_id</span><span class="p">)</span></div>

<div class="viewcode-block" id="BaseAttentionLayer.GetInitializationSourceState"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.BaseAttentionLayer.GetInitializationSourceState">[docs]</a>  <span class="k">def</span> <span class="nf">GetInitializationSourceState</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Gets the attention initialization state.</span>

<span class="sd">    The base class only preserves the `concated_source_vecs`,</span>
<span class="sd">    `concated_source_contexts` and `source_padding`. If subclasses use more</span>
<span class="sd">    state than this and need to interact with inference code that must</span>
<span class="sd">    fetch and reload state, this and `SetInitializationSourceState` must</span>
<span class="sd">    be overridden.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `.NestedMap` of Tensors that can be preserved and reset via</span>
<span class="sd">      `SetInitializationSourceState()` at a later point. This allows, for</span>
<span class="sd">      example, for attention computations to span session runs.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_source_init_done</span>
    <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
        <span class="n">concated_source_vecs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_concated_source_vecs</span><span class="p">,</span>
        <span class="n">concated_source_contexts</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_concated_source_contexts</span><span class="p">,</span>
        <span class="n">source_padding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_source_padding</span><span class="p">)</span></div>

<div class="viewcode-block" id="BaseAttentionLayer.SetInitializationSourceState"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.BaseAttentionLayer.SetInitializationSourceState">[docs]</a>  <span class="k">def</span> <span class="nf">SetInitializationSourceState</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_init_state</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Sets the attention initialization state.</span>

<span class="sd">    Args:</span>
<span class="sd">      new_init_state: A `.NestedMap` matching what was returned from</span>
<span class="sd">      `GetInitializationSourceState`, which will return this layer to that</span>
<span class="sd">      initialization state.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_source_init_done</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_concated_source_vecs</span> <span class="o">=</span> <span class="n">new_init_state</span><span class="o">.</span><span class="n">concated_source_vecs</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_concated_source_contexts</span> <span class="o">=</span> <span class="n">new_init_state</span><span class="o">.</span><span class="n">concated_source_contexts</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_source_padding</span> <span class="o">=</span> <span class="n">new_init_state</span><span class="o">.</span><span class="n">source_padding</span></div>

<div class="viewcode-block" id="BaseAttentionLayer._PaddedSoftmax"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.BaseAttentionLayer._PaddedSoftmax">[docs]</a>  <span class="k">def</span> <span class="nf">_PaddedSoftmax</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">padding</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Performs a softmax as if padding were applied after exponentiation.</span>

<span class="sd">    The default implementation uses numerical techniques to approximate this</span>
<span class="sd">    with a standard `tf.nn.softmax` (using large negative logits for padded</span>
<span class="sd">    values). It defers to a `Defun` that may be replaced on low-range</span>
<span class="sd">    implementations with a version that is numerically correct.</span>

<span class="sd">    Args:</span>
<span class="sd">      logits: Logits.</span>
<span class="sd">      padding: Padding (must be the same shape as logits).</span>
<span class="sd">    Returns:</span>
<span class="sd">      Result of the softmax.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_floating</span>
    <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="s1">&#39;max&#39;</span><span class="p">)</span>
    <span class="n">very_negative_logits</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span> <span class="o">*</span> <span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">max</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span>
            <span class="o">-</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
    <span class="n">padded_logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">padding</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">very_negative_logits</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">padded_logits</span><span class="p">)</span></div>

<div class="viewcode-block" id="BaseAttentionLayer._UpdatePaddingWithPackedInputMask"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.BaseAttentionLayer._UpdatePaddingWithPackedInputMask">[docs]</a>  <span class="k">def</span> <span class="nf">_UpdatePaddingWithPackedInputMask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">source_segment_ids</span><span class="p">,</span>
                                        <span class="n">query_segment_ids</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates an attention mask based on source and query segment ids.</span>

<span class="sd">    This creates a mask that removes invalid attention, where the query vector</span>
<span class="sd">    might assign some weight to neighboring sequences in a packed input example.</span>
<span class="sd">    Assumes `n = target_batch // source_batch`.</span>

<span class="sd">    Args:</span>
<span class="sd">      padding: Padding for logits, a tensor of shape [time, n, source_batch].</span>
<span class="sd">      source_segment_ids: a tensor of shape [time, source_batch].</span>
<span class="sd">      query_segment_ids: a tensor of shape [target_batch].</span>

<span class="sd">    Returns:</span>
<span class="sd">      Logits with mask applied.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Generating packed input mask for attention padding.</span>
    <span class="n">source_segment_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">source_segment_ids</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">query_segment_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
        <span class="n">query_segment_ids</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">source_segment_ids</span><span class="p">)[</span><span class="mi">2</span><span class="p">]])</span>
    <span class="n">padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">source_segment_ids</span><span class="p">,</span> <span class="n">query_segment_ids</span><span class="p">),</span> <span class="n">padding</span><span class="p">,</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">padding</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">padding</span></div></div>


<div class="viewcode-block" id="AdditiveAttention"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.AdditiveAttention">[docs]</a><span class="k">class</span> <span class="nc">AdditiveAttention</span><span class="p">(</span><span class="n">BaseAttentionLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Implements additive attention (also known as &quot;Bahdanau Attention&quot;),</span>
<span class="sd">  as described in:</span>

<span class="sd">  Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio.</span>
<span class="sd">  &quot;Neural Machine Translation by Jointly Learning to Align and Translate.&quot;</span>
<span class="sd">  ICLR 2015.</span>
<span class="sd">  https://arxiv.org/abs/1409.0473</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="AdditiveAttention.Params"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.AdditiveAttention.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Params for this `AdditiveAttention` class.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">AdditiveAttention</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;source_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of source nodes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;query_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of query nodes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;hidden_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of hidden nodes.&#39;</span><span class="p">)</span>
    <span class="c1"># Fill in reasonable default for params init</span>
    <span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="o">.</span><span class="n">method</span> <span class="o">=</span> <span class="s1">&#39;gaussian_sqrt_dim&#39;</span>
    <span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;same_batch_size&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s1">&#39;True iff the source and target sequence has the same batch size.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="nd">@base_layer</span><span class="o">.</span><span class="n">initializer</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs an `AdditiveAttention` object.&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">AdditiveAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="n">pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
          <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
          <span class="n">init</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="p">,</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;AdditiveAttention_vars&#39;</span><span class="p">])</span>
      <span class="n">source_var_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">source_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">]</span>
      <span class="n">pc</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">source_var_shape</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;source_var&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">AddGlobalVN</span><span class="p">)</span>
      <span class="n">query_var_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">query_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">]</span>
      <span class="n">pc</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">query_var_shape</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;query_var&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">AddGlobalVN</span><span class="p">)</span>
      <span class="n">hidden_var_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">]</span>
      <span class="n">pc</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">hidden_var_shape</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;hidden_var&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">AddGlobalVN</span><span class="p">)</span>

    <span class="c1"># noinline and compiled cannot be set at the same time</span>
    <span class="nd">@function</span><span class="o">.</span><span class="n">Defun</span><span class="p">(</span>
        <span class="o">*</span><span class="p">([</span><span class="n">layers</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">)]</span> <span class="o">*</span> <span class="mi">7</span><span class="p">),</span> <span class="n">noinline</span><span class="o">=</span><span class="ow">not</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">use_tpu</span><span class="p">())</span>
    <span class="k">def</span> <span class="nf">AttenProbs</span><span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span> <span class="n">query_vec_reshaped</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span>
                   <span class="n">per_step_source_padding</span><span class="p">,</span> <span class="n">source_segment_id</span><span class="p">,</span>
                   <span class="n">query_segment_id</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Generates probs.&quot;&quot;&quot;</span>
      <span class="n">source_batch</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">source_padding</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
      <span class="n">target_batch</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">per_step_source_padding</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">multiplier</span> <span class="o">=</span> <span class="n">target_batch</span> <span class="o">//</span> <span class="n">source_batch</span>

      <span class="c1"># Shape of summed is [sl, tb/sb, sb, hidden_dim].</span>
      <span class="n">summed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">concated_source_vecs</span> <span class="o">+</span> <span class="n">query_vec_reshaped</span><span class="p">)</span>
      <span class="c1"># logits is of shape [sl * tb/sb * sb, 1]. Computes dot product</span>
      <span class="c1"># between v with every rows in &#39;summed&#39;. Then we reshape the</span>
      <span class="c1"># result to be of shape [sl, tb/sb, sb].</span>
      <span class="c1">#</span>
      <span class="c1"># Another equivalent way is to do:</span>
      <span class="c1">#  logits = tf.reduce_sum(summed *</span>
      <span class="c1">#                         tf.reshape(v, [1, 1, 1, hidden_dim]), 3)</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">Matmul</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">summed</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">]),</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">summed</span><span class="p">)[:</span><span class="mi">3</span><span class="p">])</span>
      <span class="c1"># Take out the padding states.</span>
      <span class="c1"># _source_padding is of shape [source_length, source_batch].</span>
      <span class="c1"># reshaped to [source_length, 1, source_batch].</span>
      <span class="c1"># per_step_source_padding is reshaped to the same but with &#39;multiplier&#39;</span>
      <span class="c1"># for the second dim.</span>
      <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">source_padding</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
      <span class="n">per_step_source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">per_step_source_padding</span><span class="p">),</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">multiplier</span><span class="p">,</span> <span class="n">source_batch</span><span class="p">])</span>
      <span class="n">source_padding</span> <span class="o">+=</span> <span class="n">per_step_source_padding</span>

      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span><span class="p">:</span>
        <span class="n">source_padding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_UpdatePaddingWithPackedInputMask</span><span class="p">(</span>
            <span class="n">source_padding</span><span class="p">,</span> <span class="n">source_segment_id</span><span class="p">,</span> <span class="n">query_segment_id</span><span class="p">)</span>
      <span class="c1"># Reshape logits to a matrix of shape [target_batch, source_length] and</span>
      <span class="c1"># takes the softmax to compute the probabilities.</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">target_batch</span><span class="p">]))</span>
      <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">source_padding</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">target_batch</span><span class="p">]))</span>
      <span class="n">probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_PaddedSoftmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">probs</span>

    <span class="c1"># Adds the atten function into the graph&#39;s library.</span>
    <span class="k">def</span> <span class="nf">Atten</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span> <span class="n">source_segment_id</span><span class="p">,</span> <span class="n">concated_source_vecs</span><span class="p">,</span>
              <span class="n">concated_source_contexts</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">query_segment_id</span><span class="p">,</span>
              <span class="n">per_step_source_padding</span><span class="p">,</span> <span class="n">step_state</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Computes the attention context vector.</span>

<span class="sd">      Args:</span>
<span class="sd">        v: hidden weight. [hidden_dim, 1].</span>
<span class="sd">        w: query weight. [query_dim, hidden_dim].</span>
<span class="sd">        source_padding: [source_length, source_batch].</span>
<span class="sd">        source_segment_id: [source_lentgh, source_batch]</span>
<span class="sd">        concated_source_vecs: [source_length, source_batch, hidden_dim].</span>
<span class="sd">        concated_source_contexts: [source_batch, source_length, context_dim]</span>
<span class="sd">        query_vec: [target_batch, query_dim]</span>
<span class="sd">        query_segment_id: [target_batch]</span>
<span class="sd">        per_step_source_padding: [target_batch, source_length]</span>
<span class="sd">        step_state: A NestedMap containing &#39;global_step&#39; and &#39;time_step&#39;.</span>
<span class="sd">          Required for deterministic dropout.</span>

<span class="sd">      Note: concated_source_vecs are the vectors that are used to compute the</span>
<span class="sd">      attention score between the query_vec and each concated_source_vec.</span>
<span class="sd">      The concated_source_contexts are the vectors that compose the result.</span>
<span class="sd">      The attention context vector is computed as a weighted average of the</span>
<span class="sd">      concated_source_contexts, using the scores that were computed using</span>
<span class="sd">      concated_source_vecs.</span>
<span class="sd">      Returns:</span>
<span class="sd">        attention context vectors and probabilities.</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="n">source_batch</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
      <span class="n">target_batch</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">multiplier</span> <span class="o">=</span> <span class="n">target_batch</span> <span class="o">//</span> <span class="n">source_batch</span>
      <span class="c1"># concated_source_vecs is reshaped to</span>
      <span class="c1"># [source_length, 1, source_batch, hidden_dims]</span>
      <span class="n">concated_source_vecs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
      <span class="n">query_vec_transformed</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">Matmul</span><span class="p">(</span><span class="n">query_vec</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>

      <span class="c1"># query_vec is reshaped to</span>
      <span class="c1"># [1, target_batch/source_batch, source_batch, hidden_dims].</span>
      <span class="n">query_vec_reshaped</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
          <span class="n">query_vec_transformed</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">multiplier</span><span class="p">,</span> <span class="n">source_batch</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">])</span>
      <span class="c1"># probs is of shape [target_batch, source_length]</span>
      <span class="n">probs</span> <span class="o">=</span> <span class="n">AttenProbs</span><span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span>
                         <span class="n">query_vec_reshaped</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">per_step_source_padding</span><span class="p">,</span>
                         <span class="n">source_segment_id</span><span class="p">,</span> <span class="n">query_segment_id</span><span class="p">)</span>

      <span class="c1"># Apply dropout to weights if applicable.</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">is_eval</span><span class="p">:</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">_ApplyAttentionDropout</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">step_state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prng_seed</span><span class="p">)</span>

      <span class="c1"># Reshape probs to be of shape</span>
      <span class="c1"># [target_batch/source_batch, source_batch, source_length]</span>
      <span class="n">probs_reshaped</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="p">[</span><span class="n">multiplier</span><span class="p">,</span> <span class="n">source_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="c1"># Transpose probs to be of shape</span>
      <span class="c1"># [source_batch, target_batch/source_batch, source_length]</span>
      <span class="n">probs_reshaped</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">probs_reshaped</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
      <span class="c1"># Batched matmul</span>
      <span class="c1"># [source_batch, target_batch/source_batch, source_length] *</span>
      <span class="c1"># [source_batch, source_length, context_dim] =</span>
      <span class="c1"># [source_batch, target_batch/source_batch, context_dim]</span>
      <span class="n">summed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">probs_reshaped</span><span class="p">,</span> <span class="n">concated_source_contexts</span><span class="p">)</span>

      <span class="c1"># summed is of shape</span>
      <span class="c1"># [target_batch/source_batch, source_batch, context_dim]</span>
      <span class="n">summed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">summed</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>

      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">summed</span><span class="p">,</span> <span class="p">[</span><span class="n">target_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">probs</span>

    <span class="c1"># The source batch size equals to the target batch size.</span>
    <span class="k">def</span> <span class="nf">AttenSameBatchSize</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span> <span class="n">source_segment_id</span><span class="p">,</span>
                           <span class="n">concated_source_vecs</span><span class="p">,</span> <span class="n">concated_source_contexts</span><span class="p">,</span>
                           <span class="n">query_vec</span><span class="p">,</span> <span class="n">query_segment_id</span><span class="p">,</span> <span class="n">per_step_source_padding</span><span class="p">,</span>
                           <span class="n">step_state</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Computes the attention context vector.</span>

<span class="sd">      Args:</span>
<span class="sd">        v: hidden weight. [hidden_dim].</span>
<span class="sd">        w: query weight. [query_dim, hidden_dim].</span>
<span class="sd">        source_padding: [sl, b]</span>
<span class="sd">        source_segment_id: [sl, b]</span>
<span class="sd">        concated_source_vecs: [sl, b, hidden_dim].</span>
<span class="sd">        concated_source_contexts: [b, sl, hidden_dim]</span>
<span class="sd">        query_vec: [b, query_dim]</span>
<span class="sd">        query_segment_id: [b]</span>
<span class="sd">        per_step_source_padding: [b, sl]</span>
<span class="sd">        step_state: A NestedMap containing &#39;global_step&#39; and &#39;time_step&#39;.</span>
<span class="sd">          Required for deterministic dropout.</span>

<span class="sd">      Returns:</span>
<span class="sd">        attention context vectors and probabilities.</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="c1"># TODO(jiaye): support dropout</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">atten_dropout_prob</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s1">&#39;dropout is not supported&#39;</span><span class="p">)</span>
      <span class="k">del</span> <span class="n">step_state</span>

      <span class="c1"># [b, hidden_dim]</span>
      <span class="n">query_vec</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">Matmul</span><span class="p">(</span><span class="n">query_vec</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
      <span class="c1"># [sl, b]</span>
      <span class="nd">@function</span><span class="o">.</span><span class="n">Defun</span><span class="p">(</span>
          <span class="o">*</span><span class="p">([</span><span class="n">layers</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">)]</span> <span class="o">*</span> <span class="mi">7</span><span class="p">),</span> <span class="n">noinline</span><span class="o">=</span><span class="ow">not</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">use_tpu</span><span class="p">())</span>
      <span class="k">def</span> <span class="nf">AttenProbs</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">per_step_source_padding</span><span class="p">,</span>
                     <span class="n">source_segment_id</span><span class="p">,</span> <span class="n">query_segment_id</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Calculates atten probs with padding.&quot;&quot;&quot;</span>
        <span class="c1"># tf.tanh(x+y) shape [sl, b, hidden_dim]</span>
        <span class="n">summed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">)</span>
        <span class="c1"># [-1, hidden_dim] * [hidden_dim, 1] = [-1, 1]</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">Matmul</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">summed</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">]),</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="c1"># Reshape res to [sl, b]</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">summed</span><span class="p">)[:</span><span class="mi">2</span><span class="p">])</span>
        <span class="c1"># Take out the padding states. _source_padding is of shape [sl, b].</span>
        <span class="n">source_padding</span> <span class="o">+=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">per_step_source_padding</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span><span class="p">:</span>
          <span class="n">source_padding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_UpdatePaddingWithPackedInputMask</span><span class="p">(</span>
              <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">source_padding</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">source_segment_id</span><span class="p">,</span>
              <span class="n">query_segment_id</span><span class="p">)</span>
          <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">source_padding</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="c1"># [b, sl]</span>
        <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">source_padding</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
        <span class="c1"># softmax to compute the probabilities. [b, sl]</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_PaddedSoftmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">probs</span>

      <span class="n">probs</span> <span class="o">=</span> <span class="n">AttenProbs</span><span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span>
                         <span class="n">per_step_source_padding</span><span class="p">,</span> <span class="n">source_segment_id</span><span class="p">,</span>
                         <span class="n">query_segment_id</span><span class="p">)</span>

      <span class="c1"># contexts[i, :] is a weighted (probs[i, :]) average of</span>
      <span class="c1"># concated_source_vecs[i, :, :].</span>
      <span class="c1"># Reshaped probs is of shape [b, 1, sl]</span>
      <span class="n">reshaped_probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
      <span class="c1"># [b, 1, sl] * [b, sl, hidden_dim] = [b, 1, hidden_dim]</span>
      <span class="n">contexts</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">reshaped_probs</span><span class="p">,</span> <span class="n">concated_source_contexts</span><span class="p">)</span>
      <span class="c1"># Reshaped context is of shape [b, hidden_dim]</span>
      <span class="n">contexts</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">contexts</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">contexts</span><span class="p">,</span> <span class="n">probs</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">same_batch_size</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_ctx_vec</span> <span class="o">=</span> <span class="n">AttenSameBatchSize</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_ctx_vec</span> <span class="o">=</span> <span class="n">Atten</span>

    <span class="k">def</span> <span class="nf">EncodeSource</span><span class="p">(</span><span class="n">src_w</span><span class="p">,</span> <span class="n">vecs</span><span class="p">,</span> <span class="n">ctxs</span><span class="p">):</span>
      <span class="n">time</span><span class="p">,</span> <span class="n">batch</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">vecs</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
      <span class="n">ctxs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">ctxs</span><span class="p">,</span> <span class="p">[</span><span class="n">time</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="n">transformed_vecs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
          <span class="n">py_utils</span><span class="o">.</span><span class="n">Matmul</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">vecs</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">source_dim</span><span class="p">]),</span> <span class="n">src_w</span><span class="p">),</span>
          <span class="p">[</span><span class="n">time</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="n">transposed_ctxs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">ctxs</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
      <span class="k">return</span> <span class="n">transformed_vecs</span><span class="p">,</span> <span class="n">transposed_ctxs</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_encode_source</span> <span class="o">=</span> <span class="n">EncodeSource</span>

<div class="viewcode-block" id="AdditiveAttention.PackSource"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.AdditiveAttention.PackSource">[docs]</a>  <span class="k">def</span> <span class="nf">PackSource</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">theta</span><span class="p">,</span>
                 <span class="n">source_vecs</span><span class="p">,</span>
                 <span class="n">source_contexts</span><span class="p">,</span>
                 <span class="n">source_padding</span><span class="p">,</span>
                 <span class="n">source_segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Packs source vectors. Does not change attention state.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this</span>
<span class="sd">        layer and its children layers.</span>
<span class="sd">      source_vecs: A single tensor of shape [time, batch_size, source_dim].</span>
<span class="sd">      source_contexts: A single tensor of shape [time, batch_size, some_dim].</span>
<span class="sd">      source_padding: A tensor of shape [time, batch_size].</span>
<span class="sd">      source_segment_id: A tensor of shape [time, batch_size].</span>

<span class="sd">    Returns:</span>
<span class="sd">      Concated source vectors, concated source contexts, and source paddings.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">source_segment_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">source_segment_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">source_padding</span><span class="p">)</span>

      <span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">,</span> <span class="n">concated_source_contexts</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_encode_source</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">source_var</span><span class="p">,</span> <span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">))</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">,</span> <span class="n">concated_source_contexts</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span>
            <span class="n">source_segment_id</span><span class="p">)</span></div>

<div class="viewcode-block" id="AdditiveAttention.InitForSourcePacked"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.AdditiveAttention.InitForSourcePacked">[docs]</a>  <span class="k">def</span> <span class="nf">InitForSourcePacked</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                          <span class="n">theta</span><span class="p">,</span>
                          <span class="n">source_vecs</span><span class="p">,</span>
                          <span class="n">source_contexts</span><span class="p">,</span>
                          <span class="n">source_padding</span><span class="p">,</span>
                          <span class="n">source_segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initialize attention for the given source vectors.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this</span>
<span class="sd">        layer and its children layers.</span>
<span class="sd">      source_vecs: A single tensor of shape [time, batch_size, source_dim].</span>
<span class="sd">      source_contexts: A single tensor of shape [time, batch_size, some_dim].</span>
<span class="sd">      source_padding: A tensor of shape [time, batch_size].</span>
<span class="sd">      source_segment_id: A tensor of shape [time, batch_size].</span>

<span class="sd">    Returns:</span>
<span class="sd">      Concated source vectors, concated source contexts, and source paddings.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_source_init_done</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_concated_source_vecs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_concated_source_contexts</span><span class="p">,</span>
     <span class="bp">self</span><span class="o">.</span><span class="n">_source_padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_source_segment_id</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">PackSource</span><span class="p">(</span>
         <span class="n">theta</span><span class="p">,</span> <span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span> <span class="n">source_segment_id</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_concated_source_vecs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_concated_source_contexts</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_source_padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_source_segment_id</span><span class="p">)</span></div>

<div class="viewcode-block" id="AdditiveAttention.ZeroAttentionState"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.AdditiveAttention.ZeroAttentionState">[docs]</a>  <span class="k">def</span> <span class="nf">ZeroAttentionState</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source_seq_length</span><span class="p">,</span> <span class="n">decoder_batch_size</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="c1"># This is just a dummy state. The first dimension of the state has to match</span>
    <span class="c1"># decoder_batch_size.</span>
    <span class="n">zs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">decoder_batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">layers</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">zs</span></div>

<div class="viewcode-block" id="AdditiveAttention.ComputeContextVectorWithSource"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.AdditiveAttention.ComputeContextVectorWithSource">[docs]</a>  <span class="k">def</span> <span class="nf">ComputeContextVectorWithSource</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                     <span class="n">theta</span><span class="p">,</span>
                                     <span class="n">concated_source_vecs</span><span class="p">,</span>
                                     <span class="n">concated_source_contexts</span><span class="p">,</span>
                                     <span class="n">source_padding</span><span class="p">,</span>
                                     <span class="n">source_segment_id</span><span class="p">,</span>
                                     <span class="n">query_vec</span><span class="p">,</span>
                                     <span class="n">attention_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                     <span class="n">per_step_source_padding</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                     <span class="n">step_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                     <span class="n">query_segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the context vector given the current query output.</span>

<span class="sd">    Note: `concated_source_vecs` are the vectors that are used to compute the</span>
<span class="sd">    attention score between the `query_vec` and each `concated_source_vec`.</span>
<span class="sd">    The `concated_source_contexts` are the vectors that compose the result.</span>
<span class="sd">    The attention context vector is computed as a weighted average of the</span>
<span class="sd">    `concated_source_contexts`, using the scores that were computed using</span>
<span class="sd">    `concated_source_vecs`.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this</span>
<span class="sd">        layer and its children layers.</span>
<span class="sd">      concated_source_vecs: Concated source vectors with shape [time,</span>
<span class="sd">        batch_size, hidden_dim].</span>
<span class="sd">      concated_source_contexts: Concated source contexts with shape o</span>
<span class="sd">        [batch_size, time, context_dim].</span>
<span class="sd">      source_padding: Source padding with shape [time, batch_size].</span>
<span class="sd">      source_segment_id: Tensor of source segment ids, with shape [time,</span>
<span class="sd">        batch_size].</span>
<span class="sd">      query_vec: a tensor of shape [batch_size, query_dim].</span>
<span class="sd">      attention_state: previous attention state. It is not used in</span>
<span class="sd">          `AdditiveAttention`, and is simply passed through.</span>
<span class="sd">      per_step_source_padding: Source sequence padding to apply at this step.</span>
<span class="sd">        If not None, it should be of shape [target_batch_size,</span>
<span class="sd">        source_seq_length].</span>
<span class="sd">      step_state: A `.NestedMap` containing `global_step` and `time_step`.</span>
<span class="sd">        Required for deterministic dropout.</span>
<span class="sd">      query_segment_id: a tensor of shape [batch_size]</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tuple of 3 elements.</span>
<span class="sd">        The attention context vector:</span>
<span class="sd">          [batch_size, context_dim]</span>
<span class="sd">        The attention probability vector:</span>
<span class="sd">          [batch_size, time]</span>
<span class="sd">        The new attention mechanism state:</span>
<span class="sd">          possibly nested tuple of tensors with dimensions [target_batch, ...]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">query_batch_size</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">source_seq_length</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">source_padding</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">per_step_source_padding</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">zero</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">query_vec</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="n">per_step_source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">fill</span><span class="p">([</span><span class="n">query_batch_size</span><span class="p">,</span> <span class="n">source_seq_length</span><span class="p">],</span>
                                        <span class="n">zero</span><span class="p">)</span>
    <span class="n">per_step_source_padding</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span>
        <span class="n">per_step_source_padding</span><span class="p">,</span> <span class="p">[</span><span class="n">query_batch_size</span><span class="p">,</span> <span class="n">source_seq_length</span><span class="p">])</span>
    <span class="n">hidden</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">AddPerStepVN</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">hidden_var</span><span class="p">)</span>
    <span class="n">query</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">AddPerStepVN</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">query_var</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">source_segment_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">source_segment_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">source_padding</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">query_segment_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">query_segment_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">source_padding</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="n">ctx_vec</span><span class="p">,</span> <span class="n">prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ctx_vec</span><span class="p">(</span>
        <span class="n">hidden</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span> <span class="n">source_segment_id</span><span class="p">,</span> <span class="n">concated_source_vecs</span><span class="p">,</span>
        <span class="n">concated_source_contexts</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">query_segment_id</span><span class="p">,</span>
        <span class="n">per_step_source_padding</span><span class="p">,</span> <span class="n">step_state</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">ctx_vec</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">attention_state</span></div></div>


<div class="viewcode-block" id="DotProductAttention"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.DotProductAttention">[docs]</a><span class="k">class</span> <span class="nc">DotProductAttention</span><span class="p">(</span><span class="n">BaseAttentionLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Implements dot-product attention (also known as &quot;Luong Attention&quot;)</span>
<span class="sd">  as described in:</span>

<span class="sd">  Minh-Thang Luong, Hieu Pham, Christopher D. Manning.</span>
<span class="sd">  &quot;Effective Approaches to Attention-based Neural Machine Translation.&quot;</span>
<span class="sd">  EMNLP 2015.</span>
<span class="sd">  https://arxiv.org/abs/1508.04025</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="DotProductAttention.Params"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.DotProductAttention.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Params for `DotProductAttention`.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">DotProductAttention</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;source_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of source nodes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;query_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of query nodes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;hidden_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of hidden nodes.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="nd">@base_layer</span><span class="o">.</span><span class="n">initializer</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a DotProductAttention object.&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">DotProductAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="c1"># TODO(yonghui): relax these constraints.</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">source_dim</span> <span class="o">==</span> <span class="n">p</span><span class="o">.</span><span class="n">query_dim</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">source_dim</span> <span class="o">==</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="n">pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
          <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">],</span>
          <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;DotProductAttention_vars&#39;</span><span class="p">])</span>

      <span class="k">def</span> <span class="nf">ScaleFn</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">/</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>

      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;per_dim_scale&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">,</span> <span class="n">ScaleFn</span><span class="p">)</span>

    <span class="nd">@function</span><span class="o">.</span><span class="n">Defun</span><span class="p">(</span>
        <span class="o">*</span><span class="p">[</span><span class="n">layers</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">)]</span> <span class="o">*</span> <span class="mi">7</span><span class="p">,</span> <span class="n">noinline</span><span class="o">=</span><span class="ow">not</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">use_tpu</span><span class="p">())</span>
    <span class="k">def</span> <span class="nf">AttenProbs</span><span class="p">(</span><span class="n">per_dim_scale</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span> <span class="n">concated_source_vecs</span><span class="p">,</span>
                   <span class="n">query_vec</span><span class="p">,</span> <span class="n">per_step_source_padding</span><span class="p">,</span> <span class="n">source_segment_id</span><span class="p">,</span>
                   <span class="n">query_segment_id</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Main attention function.</span>

<span class="sd">      Args:</span>
<span class="sd">        per_dim_scale:            [source_dim], a vec to scale individual dims.</span>
<span class="sd">        source_padding:           [time, source_batch].</span>
<span class="sd">        concated_source_vecs:     [time, source_batch, source_dim].</span>
<span class="sd">        query_vec:                [target_batch, source_dim].</span>
<span class="sd">        per_step_source_padding:  [target_batch, source_seq_length]</span>
<span class="sd">        source_segment_id:        [time, source_batch].</span>
<span class="sd">        query_segment_id:         [target_batch].</span>

<span class="sd">      Returns:</span>
<span class="sd">        logits: [target_batch, source_time].</span>

<span class="sd">      target_batch = source_batch * n where n is an integer &gt;= 1.</span>
<span class="sd">      In this case query_vec contains:</span>
<span class="sd">              -------------------------</span>
<span class="sd">              | instance    1         |</span>
<span class="sd">              | instance    2         |</span>
<span class="sd">           0  |          ...          |</span>
<span class="sd">              | instance source_batch |</span>
<span class="sd">              -------------------------</span>
<span class="sd">              | instance    1         |</span>
<span class="sd">              | instance    2         |</span>
<span class="sd">           1  |          ...          |</span>
<span class="sd">              | instance source_batch |</span>
<span class="sd">              -------------------------</span>
<span class="sd">                           ...</span>
<span class="sd">              -------------------------</span>
<span class="sd">              | instance    1         |</span>
<span class="sd">              | instance    2         |</span>
<span class="sd">          n-1 |          ...          |</span>
<span class="sd">              | instance source_batch |</span>
<span class="sd">              -------------------------</span>
<span class="sd">      One use case is beam search where n = beam size.</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">source_padding</span><span class="p">)</span>
      <span class="n">concated_source_vecs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>

      <span class="n">logit_scale</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">)[</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">layers</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">))))</span>
      <span class="n">source_batch</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">target_batch</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">query_vec</span> <span class="o">*=</span> <span class="n">per_dim_scale</span>
      <span class="c1"># The n here refers to the &quot;n&quot; described in the comment above.</span>
      <span class="n">n</span> <span class="o">=</span> <span class="n">target_batch</span> <span class="o">//</span> <span class="n">source_batch</span>
      <span class="n">query_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">,</span> <span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">source_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="c1"># =&gt; [source_batch, source_dim, n]</span>
      <span class="n">query_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">query_vec</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
      <span class="c1"># =&gt; [n, source_batch, source_sequence_len]</span>
      <span class="n">per_step_source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">per_step_source_padding</span><span class="p">,</span>
                                           <span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">source_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="c1"># =&gt; [source_batch, source_sequence_len, n]</span>
      <span class="n">per_step_source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">per_step_source_padding</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
      <span class="c1"># Dot-product part.</span>
      <span class="c1"># Calls batch_mat_mul since dim &gt; 2 for per-instance matmul.</span>
      <span class="c1"># [source_batch, time, source_dim] * [source_batch, source_dim, n]</span>
      <span class="c1"># =&gt; [source_batch, time, n]</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">)</span>
      <span class="n">logits</span> <span class="o">*=</span> <span class="n">logit_scale</span>
      <span class="c1"># Exclude padding frames.</span>
      <span class="c1"># [source_batch, time] =&gt; [source_batch, time, 1]</span>
      <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">source_padding</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
      <span class="n">source_padding</span> <span class="o">+=</span> <span class="n">per_step_source_padding</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span><span class="p">:</span>
        <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">source_padding</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
        <span class="n">source_padding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_UpdatePaddingWithPackedInputMask</span><span class="p">(</span>
            <span class="n">source_padding</span><span class="p">,</span> <span class="n">source_segment_id</span><span class="p">,</span> <span class="n">query_segment_id</span><span class="p">)</span>
        <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">source_padding</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">source_padding</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

      <span class="c1"># =&gt; [n, source_batch, time]</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

      <span class="c1"># =&gt; [n * source_batch, time].</span>
      <span class="c1"># This makes logits store content in the same order as query_vec.</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="p">[</span><span class="n">target_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">source_padding</span><span class="p">,</span> <span class="p">[</span><span class="n">target_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="n">probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_PaddedSoftmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">probs</span>

    <span class="k">def</span> <span class="nf">Atten</span><span class="p">(</span><span class="n">per_dim_scale</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span> <span class="n">source_segment_id</span><span class="p">,</span>
              <span class="n">concated_source_vecs</span><span class="p">,</span> <span class="n">concated_source_contexts</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span>
              <span class="n">query_segment_id</span><span class="p">,</span> <span class="n">per_step_source_padding</span><span class="p">,</span> <span class="n">step_state</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Main attention function.</span>

<span class="sd">      Args:</span>
<span class="sd">        per_dim_scale:            [source_dim], a vec to scale individual dims.</span>
<span class="sd">        source_padding:           [time, source_batch].</span>
<span class="sd">        source_segment_id:        [time, source_batch].</span>
<span class="sd">        concated_source_vecs:     [time, source_batch, source_dim].</span>
<span class="sd">        concated_source_contexts: [source_batch, time, context_dim].</span>
<span class="sd">        query_vec:                [target_batch, source_dim].</span>
<span class="sd">        query_segment_id:         [target_batch].</span>
<span class="sd">        per_step_source_padding:  [target_batch, source_seq_length]</span>
<span class="sd">        step_state:               A NestedMap containing &#39;global_step&#39; and</span>
<span class="sd">                                  &#39;time_step&#39;. Required for deterministic</span>
<span class="sd">                                  dropout.</span>

<span class="sd">      Note: concated_source_vecs are the vectors that are used to compute the</span>
<span class="sd">      attention score between the query_vec and each concated_source_vec.</span>
<span class="sd">      The concated_source_contexts are the vectors that compose the result.</span>
<span class="sd">      The attention context vector is computed as a weighted average of the</span>
<span class="sd">      concated_source_contexts, using the scores that were computed using</span>
<span class="sd">      concated_source_vecs.</span>

<span class="sd">      Returns:</span>
<span class="sd">        context_vector: [target_batch, context_dim].</span>
<span class="sd">        probs:          [target_batch, time].</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="n">py_utils</span><span class="o">.</span><span class="n">assert_shape_match</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">)[</span><span class="mi">2</span><span class="p">]],</span>
                                  <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">)[</span><span class="mi">1</span><span class="p">]])</span>
      <span class="n">py_utils</span><span class="o">.</span><span class="n">assert_shape_match</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">)[</span><span class="mi">2</span><span class="p">]],</span>
                                  <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">source_dim</span><span class="p">])</span>
      <span class="n">source_batch</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
      <span class="n">target_batch</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">n</span> <span class="o">=</span> <span class="n">target_batch</span> <span class="o">//</span> <span class="n">source_batch</span>
      <span class="n">returned_probs</span> <span class="o">=</span> <span class="n">AttenProbs</span><span class="p">(</span>
          <span class="n">per_dim_scale</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span> <span class="n">concated_source_vecs</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span>
          <span class="n">per_step_source_padding</span><span class="p">,</span> <span class="n">source_segment_id</span><span class="p">,</span> <span class="n">query_segment_id</span><span class="p">)</span>

      <span class="c1"># =&gt; [n, source_batch, time].</span>
      <span class="n">probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">returned_probs</span><span class="p">,</span> <span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">source_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="c1"># =&gt; [source_batch, n, time].</span>
      <span class="n">probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>

      <span class="c1"># Apply dropout to weights if applicable.</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">is_eval</span><span class="p">:</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">_ApplyAttentionDropout</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">step_state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prng_seed</span><span class="p">)</span>

      <span class="c1"># Weight each frame with the probability and sum them.</span>
      <span class="c1"># [source_batch, n, time] * [source_batch, time, context_dim]</span>
      <span class="c1"># =&gt; [source_batch, n, context_dim].</span>
      <span class="n">context_vector</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">concated_source_contexts</span><span class="p">)</span>
      <span class="c1"># =&gt; [n, source_batch, context_dim].</span>
      <span class="n">context_vector</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">context_vector</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
      <span class="c1"># =&gt; [n * source_batch, context_dim].</span>
      <span class="n">context_vector</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">context_vector</span><span class="p">,</span> <span class="p">[</span><span class="n">target_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>

      <span class="k">return</span> <span class="n">context_vector</span><span class="p">,</span> <span class="n">returned_probs</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_ctx_vec</span> <span class="o">=</span> <span class="n">Atten</span>

<div class="viewcode-block" id="DotProductAttention.PackSource"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.DotProductAttention.PackSource">[docs]</a>  <span class="k">def</span> <span class="nf">PackSource</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">theta</span><span class="p">,</span>
                 <span class="n">source_vecs</span><span class="p">,</span>
                 <span class="n">source_contexts</span><span class="p">,</span>
                 <span class="n">source_padding</span><span class="p">,</span>
                 <span class="n">source_segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Packs source vectors. Does not change attention state.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this</span>
<span class="sd">        layer and its children layers.</span>
<span class="sd">      source_vecs: A tensor of shape [time, source_batch, source_dim].</span>
<span class="sd">      source_contexts: A tensor of shape [time, source_batch, context_dim].</span>
<span class="sd">      source_padding: A tensor of shape [time, source_batch].</span>
<span class="sd">      source_segment_id: A tensor of shape [time, source_batch].</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tuple (concated_source_vecs, concated_source_contexts, source_padding)</span>
<span class="sd">      where `concated_source_vecs` is a tensor of shape [time, batch_size,</span>
<span class="sd">      hidden_dim], `concated_source_contexts` is a tensor of shape</span>
<span class="sd">      [batch_size, time, some_dim] and `source_padding` is a tensor of shape</span>
<span class="sd">      [time, batch_size].</span>

<span class="sd">      Note the mismatch between `concated_source_vecs` and</span>
<span class="sd">      `concated_source_contexts`. In `concated_source_vecs`, time is the first</span>
<span class="sd">      dim, while it is the second dim in `concated_source_contexts`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">concated_source_vecs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">source_vecs</span><span class="p">)</span>
    <span class="n">concated_source_contexts</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">source_contexts</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">source_segment_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">source_segment_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">source_padding</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">,</span> <span class="n">concated_source_contexts</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span>
            <span class="n">source_segment_id</span><span class="p">)</span></div>

<div class="viewcode-block" id="DotProductAttention.InitForSourcePacked"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.DotProductAttention.InitForSourcePacked">[docs]</a>  <span class="k">def</span> <span class="nf">InitForSourcePacked</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                          <span class="n">theta</span><span class="p">,</span>
                          <span class="n">source_vecs</span><span class="p">,</span>
                          <span class="n">source_contexts</span><span class="p">,</span>
                          <span class="n">source_padding</span><span class="p">,</span>
                          <span class="n">source_segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initialize attention for the given source vectors.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this</span>
<span class="sd">        layer and its children layers.</span>
<span class="sd">      source_vecs: A tensor of shape [time, source_batch, source_dim].</span>
<span class="sd">      source_contexts: A tensor of shape [time, source_batch, context_dim].</span>
<span class="sd">      source_padding: A tensor of shape [time, source_batch].</span>
<span class="sd">      source_segment_id: A tensor of shape [time, source_batch].</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tuple (concated_source_vecs, concated_source_contexts, source_padding),</span>
<span class="sd">      where concated_source_vecs is a tensor of shape [time, batch_size,</span>
<span class="sd">      hidden_dim], concated_source_contexts is a tensor of shape [batch_size,</span>
<span class="sd">      time, some_dim] and source_padding is a tensor of shape [time,</span>
<span class="sd">      batch_size]. Note the mismatch between concated_source_vecs and</span>
<span class="sd">      concated_source_contexts. In concated_source_vecs, time is the first dim,</span>
<span class="sd">      while it is the second dim in concated_source_contexts.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_source_init_done</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_concated_source_vecs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_concated_source_contexts</span><span class="p">,</span>
     <span class="bp">self</span><span class="o">.</span><span class="n">_source_padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_source_segment_id</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">PackSource</span><span class="p">(</span>
         <span class="n">theta</span><span class="p">,</span> <span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span> <span class="n">source_segment_id</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_concated_source_vecs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_concated_source_contexts</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_source_padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_source_segment_id</span><span class="p">)</span></div>

<div class="viewcode-block" id="DotProductAttention.ZeroAttentionState"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.DotProductAttention.ZeroAttentionState">[docs]</a>  <span class="k">def</span> <span class="nf">ZeroAttentionState</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source_seq_length</span><span class="p">,</span> <span class="n">decoder_batch_size</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="c1"># No states to keep track of currently.</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">decoder_batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span></div>

<div class="viewcode-block" id="DotProductAttention.ComputeContextVectorWithSource"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.DotProductAttention.ComputeContextVectorWithSource">[docs]</a>  <span class="k">def</span> <span class="nf">ComputeContextVectorWithSource</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                     <span class="n">theta</span><span class="p">,</span>
                                     <span class="n">concated_source_vecs</span><span class="p">,</span>
                                     <span class="n">concated_source_contexts</span><span class="p">,</span>
                                     <span class="n">source_padding</span><span class="p">,</span>
                                     <span class="n">source_segment_id</span><span class="p">,</span>
                                     <span class="n">query_vec</span><span class="p">,</span>
                                     <span class="n">attention_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                     <span class="n">per_step_source_padding</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                     <span class="n">step_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                     <span class="n">query_segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the context vector given the current query output.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this</span>
<span class="sd">        layer and its children layers.</span>
<span class="sd">      concated_source_vecs: Concated source vectors with shape [time,</span>
<span class="sd">        source_batch, hidden_dim].</span>
<span class="sd">      concated_source_contexts: Concated source contexts with shape [</span>
<span class="sd">        source_batch, time, context_dim].</span>
<span class="sd">      source_padding: Source padding with shape [time, source_batch].</span>
<span class="sd">      source_segment_id: Source segment id with shape [time, source_batch].</span>
<span class="sd">      query_vec: a tensor of shape [target_batch, query_dim], where</span>
<span class="sd">        target_batch = n * source_batch (e.g., n = num_hyps_per_beam in</span>
<span class="sd">        beamsearch). Along the target_batch dimension, there are n groups of</span>
<span class="sd">        consecutive rows, each group containing source_batch rows.</span>
<span class="sd">      attention_state: previous attention state. It is not used in</span>
<span class="sd">          AdditiveAttention, and is simply passed through.</span>
<span class="sd">      per_step_source_padding: Source sequence padding to apply at this step.</span>
<span class="sd">        If not None, it should be of shape [target_batch, source_seq_length].</span>
<span class="sd">      step_state: A NestedMap containing &#39;global_step&#39; and &#39;time_step&#39;.</span>
<span class="sd">        Required for deterministic dropout.</span>
<span class="sd">      query_segment_id: Query segment id with shape [target_batch].</span>

<span class="sd">    Note: concated_source_vecs are the vectors that are used to compute the</span>
<span class="sd">    attention score between the query_vec and each concated_source_vec.</span>
<span class="sd">    The concated_source_contexts are the vectors that compose the result.</span>
<span class="sd">    The attention context vector is computed as a weighted average of the</span>
<span class="sd">    concated_source_contexts, using the scores that were computed using</span>
<span class="sd">    concated_source_vecs.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tuple of 3 elements.</span>
<span class="sd">        The attention context vector:</span>
<span class="sd">          [batch_size, context_dim]</span>
<span class="sd">        The attention probability vector:</span>
<span class="sd">          [batch_size, time]</span>
<span class="sd">        The new attention mechanism state:</span>
<span class="sd">          possibly nested tuple of tensors with dimensions [target_batch, ...]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">query_batch_size</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">source_sequence_length</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">source_padding</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">per_step_source_padding</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">zero</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">query_vec</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="n">per_step_source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span>
          <span class="p">[</span><span class="n">query_batch_size</span><span class="p">,</span> <span class="n">source_sequence_length</span><span class="p">],</span> <span class="n">zero</span><span class="p">)</span>
    <span class="n">per_step_source_padding</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span>
        <span class="n">per_step_source_padding</span><span class="p">,</span> <span class="p">[</span><span class="n">query_batch_size</span><span class="p">,</span> <span class="n">source_sequence_length</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">source_segment_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">source_segment_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">source_padding</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">query_segment_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">query_segment_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">source_padding</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="n">ctx_vec</span><span class="p">,</span> <span class="n">prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ctx_vec</span><span class="p">(</span>
        <span class="n">theta</span><span class="o">.</span><span class="n">per_dim_scale</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span> <span class="n">source_segment_id</span><span class="p">,</span>
        <span class="n">concated_source_vecs</span><span class="p">,</span> <span class="n">concated_source_contexts</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span>
        <span class="n">query_segment_id</span><span class="p">,</span> <span class="n">per_step_source_padding</span><span class="p">,</span> <span class="n">step_state</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ctx_vec</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">attention_state</span></div></div>


<div class="viewcode-block" id="_RecursiveReshape"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention._RecursiveReshape">[docs]</a><span class="k">def</span> <span class="nf">_RecursiveReshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="kc">None</span>
  <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">Transform</span><span class="p">(</span><span class="k">lambda</span> <span class="n">y</span><span class="p">:</span> <span class="n">_RecursiveReshape</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">shape</span><span class="p">))</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="o">==</span> <span class="mi">2</span> <span class="k">else</span> <span class="n">x</span></div>


<div class="viewcode-block" id="MultiHeadedAttention"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.MultiHeadedAttention">[docs]</a><span class="k">class</span> <span class="nc">MultiHeadedAttention</span><span class="p">(</span><span class="n">BaseAttentionLayer</span><span class="p">,</span> <span class="n">quant_utils</span><span class="o">.</span><span class="n">QuantizableLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Attention with multiple attention heads.</span>

<span class="sd">  Conceptually, the algorithm works as follows:</span>

<span class="sd">  1. Source vectors (attention keys) are first projected to vectors of dim</span>
<span class="sd">     p.hidden_dim.</span>
<span class="sd">  2. Query vectors are projected to vectors of dim p.hidden_dim as well.</span>
<span class="sd">  3. Context vectors (attention values) are not projected.</span>
<span class="sd">  4. Source vectors, query vectors and context vectors are all split into</span>
<span class="sd">     p.num_attention_heads chunks.</span>
<span class="sd">  5. The inner atten mechanism is computed separately on each of the chunks.</span>
<span class="sd">  6. Attention contexts from each of the chunk are concatenated to form the</span>
<span class="sd">     final context.</span>
<span class="sd">  7. Attention probs from each of the chunk are averaged to form the final</span>
<span class="sd">     attention prob.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="MultiHeadedAttention.Params"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.MultiHeadedAttention.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Params for MultiHeadedAttention.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadedAttention</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;source_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of source nodes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;query_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of query nodes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;context_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of context nodes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;hidden_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of hidden nodes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_attention_heads&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;Num of attention heads.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;use_source_vec_as_attention_value&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s1">&#39;Whether or not to use source_vec as the attention value as well.&#39;</span>
        <span class="s1">&#39; If True, we expect source_vec and source_contexts are the same.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;enable_source_proj&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span>
             <span class="s1">&#39;If False, source side linear projection is disabled.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;enable_query_proj&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span>
             <span class="s1">&#39;If False, query side linear projection is disabled.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;inner_atten_params&#39;</span><span class="p">,</span> <span class="n">DotProductAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
             <span class="s1">&#39;Params for underlying attention mechanism.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;enable_ctx_pre_proj&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s1">&#39;If True, context is pre-projected before processing into&#39;</span>
        <span class="s1">&#39; hidden_dim.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;enable_ctx_post_proj&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s1">&#39;If True, computed context is post projected into&#39;</span>
        <span class="s1">&#39; ctx_post_proj_dim.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;ctx_post_proj_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of post projection nodes.&#39;</span><span class="p">)</span>

    <span class="c1"># Often the attention context output needs to be concated</span>
    <span class="c1"># with tensors from another layer. This allows them to share</span>
    <span class="c1"># quantization parameters. By convention, all attention layers</span>
    <span class="c1"># need to include their context output vectors in this domain.</span>
    <span class="n">p</span><span class="o">.</span><span class="n">qdomain</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;atten_context&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
                     <span class="s1">&#39;Quantization domain for attention context.&#39;</span><span class="p">)</span>

    <span class="n">p</span><span class="o">.</span><span class="n">params_init</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Xavier</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">p</span></div>

  <span class="nd">@base_layer</span><span class="o">.</span><span class="n">initializer</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a MultiHeadedAttention object.&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadedAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">%</span> <span class="n">p</span><span class="o">.</span><span class="n">num_attention_heads</span> <span class="o">==</span> <span class="mi">0</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">TrackQTensor</span><span class="p">(</span><span class="s1">&#39;source_proj_matmul&#39;</span><span class="p">,</span> <span class="s1">&#39;source_proj_add&#39;</span><span class="p">,</span>
                      <span class="s1">&#39;query_proj_matmul&#39;</span><span class="p">,</span> <span class="s1">&#39;query_proj_add&#39;</span><span class="p">,</span>
                      <span class="s1">&#39;ctx_pre_proj_matmul&#39;</span><span class="p">,</span> <span class="s1">&#39;ctx_pre_proj_add&#39;</span><span class="p">)</span>
    <span class="c1"># TODO: Remove the p.is_eval check below once brop quant within defun is</span>
    <span class="c1"># fixed on the training side. This is less than ideal as-is because</span>
    <span class="c1"># training will just trend to match downstream quant constraints vs force</span>
    <span class="c1"># alignment.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">TrackQTensor</span><span class="p">(</span>
        <span class="s1">&#39;ctx_post_proj_matmul&#39;</span><span class="p">,</span> <span class="s1">&#39;ctx_post_proj_add&#39;</span><span class="p">,</span> <span class="n">domain</span><span class="o">=</span><span class="s1">&#39;atten_context&#39;</span><span class="p">)</span>

    <span class="n">pc_bias</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">],</span>
        <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_source_proj</span><span class="p">:</span>
        <span class="n">pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">source_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">],</span>
            <span class="n">init</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;source_proj&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;source_proj_b&#39;</span><span class="p">,</span> <span class="n">pc_bias</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">source_dim</span> <span class="o">==</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span>

      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_query_proj</span><span class="p">:</span>
        <span class="n">pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">query_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">],</span>
            <span class="n">init</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;query_proj&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;query_proj_b&#39;</span><span class="p">,</span> <span class="n">pc_bias</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">query_dim</span> <span class="o">==</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span>

      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_ctx_pre_proj</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">use_source_vec_as_attention_value</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">context_dim</span>
        <span class="n">pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">context_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">],</span>
            <span class="n">init</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;ctx_proj&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;ctx_proj_b&#39;</span><span class="p">,</span> <span class="n">pc_bias</span><span class="p">)</span>

      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_ctx_post_proj</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">ctx_post_proj_dim</span>
        <span class="n">pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">ctx_post_proj_dim</span><span class="p">],</span>
            <span class="n">init</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;ctx_post_proj&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">)</span>
        <span class="n">pc_bias_post_proj</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">ctx_post_proj_dim</span><span class="p">],</span>
            <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;ctx_post_proj_b&#39;</span><span class="p">,</span> <span class="n">pc_bias_post_proj</span><span class="p">)</span>

      <span class="n">att_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">//</span> <span class="n">p</span><span class="o">.</span><span class="n">num_attention_heads</span>

      <span class="n">att_p</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">inner_atten_params</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
          <span class="n">source_dim</span><span class="o">=</span><span class="n">att_dim</span><span class="p">,</span>
          <span class="n">query_dim</span><span class="o">=</span><span class="n">att_dim</span><span class="p">,</span>
          <span class="n">hidden_dim</span><span class="o">=</span><span class="n">att_dim</span><span class="p">,</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">random_seed</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">random_seed</span><span class="p">,</span>
          <span class="n">atten_dropout_prob</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">atten_dropout_prob</span><span class="p">,</span>
          <span class="n">atten_dropout_deterministic</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">atten_dropout_deterministic</span><span class="p">,</span>
          <span class="n">packed_input</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">packed_input</span><span class="p">)</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">att_p</span><span class="o">.</span><span class="n">name</span><span class="p">:</span>
        <span class="n">att_p</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;inner_att&#39;</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;atten&#39;</span><span class="p">,</span> <span class="n">att_p</span><span class="p">)</span>

<div class="viewcode-block" id="MultiHeadedAttention.InitForSourcePacked"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.MultiHeadedAttention.InitForSourcePacked">[docs]</a>  <span class="k">def</span> <span class="nf">InitForSourcePacked</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                          <span class="n">theta</span><span class="p">,</span>
                          <span class="n">source_vecs</span><span class="p">,</span>
                          <span class="n">source_contexts</span><span class="p">,</span>
                          <span class="n">source_padding</span><span class="p">,</span>
                          <span class="n">source_segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initialize attention for the given source vectors.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this</span>
<span class="sd">        layer and its children layers.</span>
<span class="sd">      source_vecs: A tensor of shape [time, source_batch, source_dim].</span>
<span class="sd">      source_contexts: A tensor of shape [time, source_batch, context_dim].</span>
<span class="sd">      source_padding: A tensor of shape [time, source_batch].</span>
<span class="sd">      source_segment_id: A tensor of shape [time, source_batch].</span>

<span class="sd">    Returns:</span>
<span class="sd">      (concated_source_vecs, concated_source_contexts, source_padding,</span>
<span class="sd">      source_segment_id) tuple where concated_source_vecs is a tensor of shape</span>
<span class="sd">      [source_seq_len, batch_size * num_heads, orig_source_dim / num_heads],</span>
<span class="sd">      concated_source_contexts is a tensor of shape [source_batch_size *</span>
<span class="sd">      num_heads, source_seq_len,  orig_context_dim / num_heads],</span>
<span class="sd">      source_padding is a tensor of shape [source_seq_len, batch_size *</span>
<span class="sd">      num_heads] and source_segment_id is a tensor of shape</span>
<span class="sd">      [source_seq_len, batch_size * num_heads].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_source_init_done</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_concated_source_vecs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_concated_source_contexts</span><span class="p">,</span>
     <span class="bp">self</span><span class="o">.</span><span class="n">_source_padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_source_segment_id</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">PackSource</span><span class="p">(</span>
         <span class="n">theta</span><span class="p">,</span> <span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span> <span class="n">source_segment_id</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_concated_source_vecs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_concated_source_contexts</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_source_padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_source_segment_id</span><span class="p">)</span></div>

  <span class="nd">@py_utils</span><span class="o">.</span><span class="n">NameScopeDecorator</span><span class="p">(</span><span class="s1">&#39;MultiHeadedAttention/PackSource&#39;</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">PackSource</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">theta</span><span class="p">,</span>
                 <span class="n">source_vecs</span><span class="p">,</span>
                 <span class="n">source_contexts</span><span class="p">,</span>
                 <span class="n">source_padding</span><span class="p">,</span>
                 <span class="n">source_segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Packs source vectors. Does not change attention state.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this</span>
<span class="sd">        layer and its children layers.</span>
<span class="sd">      source_vecs: A tensor of shape [time, source_batch, source_dim].</span>
<span class="sd">      source_contexts: A tensor of shape [time, source_batch, context_dim].</span>
<span class="sd">      source_padding: A tensor of shape [time, source_batch].</span>
<span class="sd">      source_segment_id: A tensor of shape [time, source_batch].</span>

<span class="sd">    Returns:</span>
<span class="sd">      (concated_source_vecs, concated_source_contexts, source_padding,</span>
<span class="sd">      source_segment_id) tuple where concated_source_vecs is a tensor of shape</span>
<span class="sd">      [source_seq_len, batch_size * num_heads, orig_source_dim / num_heads],</span>
<span class="sd">      concated_source_contexts is a tensor of shape [source_batch_size *</span>
<span class="sd">      num_heads, source_seq_len,  orig_context_dim / num_heads],</span>
<span class="sd">      source_padding is a tensor of shape [source_seq_len, batch_size *</span>
<span class="sd">      num_heads] and source_segment_id is a tensor of shape</span>
<span class="sd">      [source_seq_len, batch_size * num_heads].</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">fns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fns</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_source_proj</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">source_dim</span> <span class="o">==</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_query_proj</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">query_dim</span> <span class="o">==</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;init__0&#39;</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_source_vec_as_attention_value</span><span class="p">:</span>
        <span class="n">source_vecs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">source_vecs</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">source_contexts</span><span class="p">))</span>
      <span class="n">time_steps</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">source_vecs</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">batch_size</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">source_vecs</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
      <span class="c1"># source_projected shape [time * source_batch, hidden]</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;init__0a&#39;</span><span class="p">):</span>
        <span class="n">source_vec_depth</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">source_vecs</span><span class="p">)[</span><span class="mi">2</span><span class="p">]</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;init__0b&#39;</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_source_proj</span><span class="p">:</span>
          <span class="n">source_projected</span> <span class="o">=</span> <span class="p">(</span>
              <span class="n">fns</span><span class="o">.</span><span class="n">qbatchmatmul</span><span class="p">(</span>
                  <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">source_vecs</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">source_vec_depth</span><span class="p">]),</span>
                  <span class="n">fns</span><span class="o">.</span><span class="n">qweight</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">source_proj</span><span class="p">),</span>
                  <span class="n">qt</span><span class="o">=</span><span class="s1">&#39;source_proj_matmul&#39;</span><span class="p">))</span>
          <span class="n">source_projected</span> <span class="o">=</span> <span class="n">fns</span><span class="o">.</span><span class="n">qadd</span><span class="p">(</span>
              <span class="n">source_projected</span><span class="p">,</span>
              <span class="n">fns</span><span class="o">.</span><span class="n">qweight</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">source_proj_b</span><span class="p">),</span>
              <span class="n">qt</span><span class="o">=</span><span class="s1">&#39;source_proj_add&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">source_projected</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">source_vecs</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">source_vec_depth</span><span class="p">])</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;init__1&#39;</span><span class="p">):</span>
      <span class="n">hidden_depth</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span>
      <span class="n">num_heads</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_attention_heads</span>
      <span class="c1"># =&gt; [time, source_batch * num_heads, hidden / num_heads]</span>
      <span class="n">source_projected</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
          <span class="n">source_projected</span><span class="p">,</span>
          <span class="p">[</span><span class="n">time_steps</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">hidden_depth</span> <span class="o">//</span> <span class="n">num_heads</span><span class="p">])</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_source_vec_as_attention_value</span><span class="p">:</span>
        <span class="n">source_contexts_reshaped</span> <span class="o">=</span> <span class="n">source_projected</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_ctx_pre_proj</span><span class="p">:</span>
          <span class="n">source_context_depth</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">source_contexts</span><span class="p">)[</span><span class="mi">2</span><span class="p">]</span>
          <span class="n">source_contexts_projected</span> <span class="o">=</span> <span class="n">fns</span><span class="o">.</span><span class="n">qbatchmatmul</span><span class="p">(</span>
              <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">source_contexts</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">source_context_depth</span><span class="p">]),</span>
              <span class="n">fns</span><span class="o">.</span><span class="n">qweight</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">ctx_proj</span><span class="p">),</span>
              <span class="n">qt</span><span class="o">=</span><span class="s1">&#39;ctx_pre_proj_matmul&#39;</span><span class="p">)</span>
          <span class="n">source_contexts_projected</span> <span class="o">=</span> <span class="n">fns</span><span class="o">.</span><span class="n">qadd</span><span class="p">(</span>
              <span class="n">source_contexts_projected</span><span class="p">,</span>
              <span class="n">fns</span><span class="o">.</span><span class="n">qweight</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">ctx_proj_b</span><span class="p">),</span>
              <span class="n">qt</span><span class="o">=</span><span class="s1">&#39;ctx_pre_proj_add&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">source_contexts_projected</span> <span class="o">=</span> <span class="n">source_contexts</span>
        <span class="n">source_contexts_reshaped</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">source_contexts_projected</span><span class="p">,</span> <span class="p">[</span><span class="n">time_steps</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;init__2&#39;</span><span class="p">):</span>
      <span class="n">source_padding_replicated</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span>
              <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">source_padding</span><span class="p">,</span> <span class="p">[</span><span class="n">time_steps</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span>
              <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">]),</span> <span class="p">[</span><span class="n">time_steps</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">])</span>
      <span class="k">if</span> <span class="n">source_segment_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">source_segment_id_repl</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">source_padding_replicated</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">source_segment_id_repl</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span>
                <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">source_segment_id</span><span class="p">,</span> <span class="p">[</span><span class="n">time_steps</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span>
                <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">]),</span> <span class="p">[</span><span class="n">time_steps</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">])</span>

      <span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">,</span> <span class="n">concated_source_contexts</span><span class="p">,</span>
       <span class="n">source_padding</span><span class="p">,</span> <span class="n">source_segment_id</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">atten</span><span class="o">.</span><span class="n">PackSource</span><span class="p">(</span>
           <span class="n">theta</span><span class="o">.</span><span class="n">atten</span><span class="p">,</span> <span class="n">source_projected</span><span class="p">,</span> <span class="n">source_contexts_reshaped</span><span class="p">,</span>
           <span class="n">source_padding_replicated</span><span class="p">,</span> <span class="n">source_segment_id_repl</span><span class="p">)</span>
      <span class="k">return</span> <span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">,</span> <span class="n">concated_source_contexts</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span>
              <span class="n">source_segment_id</span><span class="p">)</span>

  <span class="nd">@py_utils</span><span class="o">.</span><span class="n">NameScopeDecorator</span><span class="p">(</span><span class="s1">&#39;MultiHeadedAttention/ExtendSourcePacked&#39;</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">ExtendSourcePacked</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">new_source_vecs</span><span class="p">,</span> <span class="n">new_source_contexts</span><span class="p">,</span>
                         <span class="n">new_source_paddings</span><span class="p">,</span> <span class="n">new_source_segment_ids</span><span class="p">,</span>
                         <span class="n">cached_prev_source_vecs</span><span class="p">,</span> <span class="n">cached_prev_source_contexts</span><span class="p">,</span>
                         <span class="n">cached_prev_source_paddings</span><span class="p">,</span>
                         <span class="n">cached_prev_source_segment_ids</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Extend cached source_vecs and source_contexts by one more timestep.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this</span>
<span class="sd">        layer and its children layers.</span>
<span class="sd">      new_source_vecs: A tensor of shape [source_batch, source_dim].</span>
<span class="sd">      new_source_contexts: A tensor of shape [source_batch, context_dim].</span>
<span class="sd">        new_source_vecs and new_source_contexts are source_vecs and</span>
<span class="sd">        source_contexts for the new timestep to be extended.</span>
<span class="sd">      new_source_paddings: If not None, a tensor of shape [source_batch].</span>
<span class="sd">        source_padding for the new timestep.</span>
<span class="sd">      new_source_segment_ids: If not None, a tensor of shape [source_batch].</span>
<span class="sd">        source_segment_id for the new timestep.</span>

<span class="sd">      cached_prev_source_vecs: A tensor of shape [source_batch,</span>
<span class="sd">        t - 1, hidden_dim].</span>
<span class="sd">      cached_prev_source_contexts: A tensor of shape [source_batch,</span>
<span class="sd">        t - 1, hidden_dim].</span>
<span class="sd">        &#39;cached_prev_source_vecs&#39; and &#39;cached_prev_source_contexts&#39; are the</span>
<span class="sd">        already preprocessed source_vecs and source_contexts for the previous</span>
<span class="sd">        t-1 steps.</span>
<span class="sd">      cached_prev_source_paddings: If not None, a tensor of shape [source_batch,</span>
<span class="sd">        t - 1, num_heads], cached source padding for the previous t - 1</span>
<span class="sd">        timesteps.</span>
<span class="sd">      cached_prev_source_segment_ids: If not None, a tensor of shape</span>
<span class="sd">        [source_batch, t - 1, num_heads], cached source segment id for the</span>
<span class="sd">        previous t - 1 timesteps.</span>
<span class="sd">    Returns:</span>
<span class="sd">      Extended cached source_vecs, source_contexts and source_paddings.</span>
<span class="sd">      &#39;extended_source_vec&#39; is of shape [batch_size, t, num_heads * dim],</span>
<span class="sd">      &#39;extended_source_context&#39; is of shape [batch_size, t, num_heads * dim],</span>
<span class="sd">      source_padding is of shape [batch_size, t, num_heads], source_segment_id</span>
<span class="sd">      is of shape [batch_size, t, num_heads].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">new_source_vecs</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span>
    <span class="n">num_heads</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_attention_heads</span>
    <span class="k">if</span> <span class="n">new_source_paddings</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">new_source_paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">batch_size</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">new_source_vecs</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">new_source_segment_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">new_source_segment_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
          <span class="p">[</span><span class="n">batch_size</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">new_source_vecs</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="p">(</span><span class="n">processed_source_vecs</span><span class="p">,</span> <span class="n">processed_source_contexts</span><span class="p">,</span>
     <span class="n">processed_source_paddings</span><span class="p">,</span>
     <span class="n">processed_source_segment_ids</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">InitForSourcePacked</span><span class="p">(</span>
         <span class="n">theta</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">new_source_vecs</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
         <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">new_source_contexts</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
         <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">new_source_paddings</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
         <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">new_source_segment_ids</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">processed_source_vecs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">processed_source_vecs</span><span class="p">,</span>
                                       <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">])</span>
    <span class="n">processed_source_contexts</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">processed_source_contexts</span><span class="p">,</span>
                                           <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">processed_source_paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">processed_source_paddings</span><span class="p">,</span>
                                           <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">])</span>
    <span class="n">processed_source_segment_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">processed_source_segment_ids</span><span class="p">,</span>
                                              <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">])</span>
    <span class="n">cached_source_vecs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
        <span class="p">[</span><span class="n">cached_prev_source_vecs</span><span class="p">,</span> <span class="n">processed_source_vecs</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">cached_source_contexts</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
        <span class="p">[</span><span class="n">cached_prev_source_contexts</span><span class="p">,</span> <span class="n">processed_source_contexts</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">cached_prev_source_paddings</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">cached_source_paddings</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">cached_source_paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
          <span class="p">[</span><span class="n">cached_prev_source_paddings</span><span class="p">,</span> <span class="n">processed_source_paddings</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">cached_prev_source_segment_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">cached_source_segment_ids</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">cached_source_segment_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
          <span class="p">[</span><span class="n">cached_prev_source_segment_ids</span><span class="p">,</span> <span class="n">processed_source_segment_ids</span><span class="p">],</span>
          <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">cached_source_vecs</span><span class="p">,</span> <span class="n">cached_source_contexts</span><span class="p">,</span> <span class="n">cached_source_paddings</span><span class="p">,</span>
            <span class="n">cached_source_segment_ids</span><span class="p">)</span>

  <span class="nd">@py_utils</span><span class="o">.</span><span class="n">NameScopeDecorator</span><span class="p">(</span><span class="s1">&#39;MultiHeadedAttention/ZeroAttentionState&#39;</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">ZeroAttentionState</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source_seq_length</span><span class="p">,</span> <span class="n">decoder_batch_size</span><span class="p">):</span>
    <span class="n">zero_att_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">atten</span><span class="o">.</span><span class="n">ZeroAttentionState</span><span class="p">(</span>
        <span class="n">source_seq_length</span><span class="p">,</span> <span class="n">decoder_batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">)</span>
    <span class="c1"># [batch * num_heads, length] =&gt; [batch, num_heads * length].</span>
    <span class="n">zero_att_state</span> <span class="o">=</span> <span class="n">_RecursiveReshape</span><span class="p">(</span><span class="n">zero_att_state</span><span class="p">,</span> <span class="p">[</span><span class="n">decoder_batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">zero_att_state</span>

  <span class="nd">@py_utils</span><span class="o">.</span><span class="n">NameScopeDecorator</span><span class="p">(</span>
      <span class="s1">&#39;MultiHeadedAttention/ComputeContextVectorWithSource&#39;</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">ComputeContextVectorWithSource</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                     <span class="n">theta</span><span class="p">,</span>
                                     <span class="n">concated_source_vecs</span><span class="p">,</span>
                                     <span class="n">concated_source_contexts</span><span class="p">,</span>
                                     <span class="n">source_padding</span><span class="p">,</span>
                                     <span class="n">source_segment_id</span><span class="p">,</span>
                                     <span class="n">query_vec</span><span class="p">,</span>
                                     <span class="n">attention_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                     <span class="n">per_step_source_padding</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                     <span class="n">step_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                     <span class="n">query_segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the context vector given the current query output.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this</span>
<span class="sd">        layer and its children layers.</span>
<span class="sd">      concated_source_vecs: Concated source vectors with shape [time,</span>
<span class="sd">        batch_size, hidden_dim].</span>
<span class="sd">      concated_source_contexts: Concated source contexts with shape [</span>
<span class="sd">        batch_size, time, context_dim].</span>
<span class="sd">      source_padding: Source padding with shape [time, batch_size].</span>
<span class="sd">      source_segment_id: Source segment id with shape [time, batch_size].</span>
<span class="sd">      query_vec: a tensor of shape [target_batch, query_dim].</span>
<span class="sd">      attention_state: previous attention state. It is not used in</span>
<span class="sd">          AdditiveAttention, and is simply passed through.</span>
<span class="sd">      per_step_source_padding: Source sequence padding to apply at this step.</span>
<span class="sd">        If not None, it should be of shape [target_batch_size,</span>
<span class="sd">        source_seq_length].</span>
<span class="sd">      step_state: A NestedMap containing &#39;global_step&#39; and &#39;time_step&#39;.</span>
<span class="sd">        Required for deterministic dropout.</span>
<span class="sd">      query_segment_id: a tensor of shape [target_batch].</span>

<span class="sd">    Note: concated_source_vecs are the vectors that are used to compute the</span>
<span class="sd">    attention score between the query_vec and each concated_source_vec.</span>
<span class="sd">    The concated_source_contexts are the vectors that compose the result.</span>
<span class="sd">    The attention context vector is computed as a weighted average of the</span>
<span class="sd">    concated_source_contexts, using the scores that were computed using</span>
<span class="sd">    concated_source_vecs.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tuple of 3 elements.</span>
<span class="sd">        The attention context vector:</span>
<span class="sd">          [batch_size, context_dim]</span>
<span class="sd">        The attention probability vector:</span>
<span class="sd">          [batch_size, time]</span>
<span class="sd">        The new attention mechanism state:</span>
<span class="sd">          possibly nested tuple of tensors with dimensions [target_batch, ...]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">fns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fns</span>
    <span class="n">source_seq_len</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">source_padding</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">num_heads</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_attention_heads</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_query_proj</span><span class="p">:</span>
      <span class="n">query_vec_projected</span> <span class="o">=</span> <span class="n">fns</span><span class="o">.</span><span class="n">qbatchmatmul</span><span class="p">(</span>
          <span class="n">query_vec</span><span class="p">,</span> <span class="n">fns</span><span class="o">.</span><span class="n">qweight</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">query_proj</span><span class="p">),</span> <span class="n">qt</span><span class="o">=</span><span class="s1">&#39;query_proj_matmul&#39;</span><span class="p">)</span>
      <span class="n">query_vec_projected</span> <span class="o">=</span> <span class="n">fns</span><span class="o">.</span><span class="n">qadd</span><span class="p">(</span>
          <span class="n">query_vec_projected</span><span class="p">,</span>
          <span class="n">fns</span><span class="o">.</span><span class="n">qweight</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">query_proj_b</span><span class="p">),</span>
          <span class="n">qt</span><span class="o">=</span><span class="s1">&#39;query_proj_add&#39;</span><span class="p">)</span>
      <span class="n">query_vec_projected</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
          <span class="n">query_vec_projected</span><span class="p">,</span>
          <span class="p">[</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">//</span> <span class="n">num_heads</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">query_vec_projected</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
          <span class="n">query_vec</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">//</span> <span class="n">num_heads</span><span class="p">])</span>

    <span class="n">query_batch_size</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">query_segment_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">query_segment_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
          <span class="n">query_batch_size</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">source_padding</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">query_segment_id_repl</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">query_segment_id</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">])</span>
      <span class="n">query_segment_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">query_segment_id_repl</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="k">if</span> <span class="n">per_step_source_padding</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">zero</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">query_vec</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="n">per_step_source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">fill</span><span class="p">([</span><span class="n">query_batch_size</span><span class="p">,</span> <span class="n">source_seq_len</span><span class="p">],</span>
                                        <span class="n">zero</span><span class="p">)</span>
    <span class="n">per_step_source_padding</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span>
        <span class="n">per_step_source_padding</span><span class="p">,</span> <span class="p">[</span><span class="n">query_batch_size</span><span class="p">,</span> <span class="n">source_seq_len</span><span class="p">])</span>
    <span class="n">per_step_source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">per_step_source_padding</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">]),</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">source_seq_len</span><span class="p">])</span>
    <span class="n">attention_state</span> <span class="o">=</span> <span class="n">_RecursiveReshape</span><span class="p">(</span><span class="n">attention_state</span><span class="p">,</span>
                                        <span class="p">[</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">ctx_vec</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">att_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">atten</span><span class="o">.</span><span class="n">ComputeContextVectorWithSource</span><span class="p">(</span>
        <span class="n">theta</span><span class="o">.</span><span class="n">atten</span><span class="p">,</span> <span class="n">concated_source_vecs</span><span class="p">,</span> <span class="n">concated_source_contexts</span><span class="p">,</span>
        <span class="n">source_padding</span><span class="p">,</span> <span class="n">source_segment_id</span><span class="p">,</span> <span class="n">query_vec_projected</span><span class="p">,</span> <span class="n">attention_state</span><span class="p">,</span>
        <span class="n">per_step_source_padding</span><span class="p">,</span> <span class="n">step_state</span><span class="p">,</span> <span class="n">query_segment_id</span><span class="p">)</span>
    <span class="n">ctx_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">ctx_vec</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_ctx_post_proj</span><span class="p">:</span>
      <span class="n">ctx_vec</span> <span class="o">=</span> <span class="n">fns</span><span class="o">.</span><span class="n">qbatchmatmul</span><span class="p">(</span>
          <span class="n">ctx_vec</span><span class="p">,</span> <span class="n">fns</span><span class="o">.</span><span class="n">qweight</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">ctx_post_proj</span><span class="p">),</span> <span class="n">qt</span><span class="o">=</span><span class="s1">&#39;ctx_post_proj_matmul&#39;</span><span class="p">)</span>
      <span class="n">ctx_vec</span> <span class="o">=</span> <span class="n">fns</span><span class="o">.</span><span class="n">qadd</span><span class="p">(</span>
          <span class="n">ctx_vec</span><span class="p">,</span> <span class="n">fns</span><span class="o">.</span><span class="n">qweight</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">ctx_post_proj_b</span><span class="p">),</span> <span class="n">qt</span><span class="o">=</span><span class="s1">&#39;ctx_post_proj_add&#39;</span><span class="p">)</span>
    <span class="c1"># TODO(laurenzo): Use a better named range function (we want to represent</span>
    <span class="c1"># 0..1 probs).</span>
    <span class="n">prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">QRSoftmax</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">att_state</span> <span class="o">=</span> <span class="n">_RecursiveReshape</span><span class="p">(</span><span class="n">att_state</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">ctx_vec</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">att_state</span>

  <span class="nd">@py_utils</span><span class="o">.</span><span class="n">NameScopeDecorator</span><span class="p">(</span>
      <span class="s1">&#39;MultiHeadedAttention/ComputeContextVectorWithAttenProbs&#39;</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">ComputeContextVectorWithAttenProbs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">packed_context</span><span class="p">,</span>
                                         <span class="n">atten_probs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the context vector given the attention probailities.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this</span>
<span class="sd">        layer and its children layers.</span>
<span class="sd">      packed_context: Concated source contexts with shape [</span>
<span class="sd">        batch_size * num_heads, time, context_dim // num_heads].</span>
<span class="sd">      atten_probs: The attention probability vector:</span>
<span class="sd">        [batch_size * num_heads, time].</span>

<span class="sd">    Returns:</span>
<span class="sd">      The attention context vector: [target_batch, source_dim]</span>
<span class="sd">      If p.enable_ctx_post_proj is false, source_dim = context_dim,</span>
<span class="sd">      otherwise, source_dim = p.ctx_post_proj_dim.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">num_heads</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_attention_heads</span>
    <span class="c1"># packed_context: [batch_size * num_head, num_style,</span>
    <span class="c1"># hidden_dim / num_head]</span>
    <span class="c1"># inp: [batch_size * num_head, num_style]</span>
    <span class="n">packed_context</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">with_dependencies</span><span class="p">([</span>
        <span class="n">py_utils</span><span class="o">.</span><span class="n">assert_shape_match</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">packed_context</span><span class="p">)[</span><span class="mi">0</span><span class="p">]],</span>
                                    <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">atten_probs</span><span class="p">)[</span><span class="mi">0</span><span class="p">]])</span>
    <span class="p">],</span> <span class="n">packed_context</span><span class="p">)</span>
    <span class="n">b_size</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">packed_context</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="n">num_heads</span>
    <span class="n">ctx_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">atten_probs</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">packed_context</span><span class="p">),</span> <span class="p">[</span><span class="n">b_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_ctx_post_proj</span><span class="p">:</span>
      <span class="n">ctx_vec_proj</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">ctx_vec</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">ctx_post_proj</span><span class="p">)</span>
      <span class="n">ctx_vec_proj</span> <span class="o">+=</span> <span class="n">theta</span><span class="o">.</span><span class="n">ctx_post_proj_b</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">ctx_vec_proj</span> <span class="o">=</span> <span class="n">ctx_vec</span>
    <span class="k">return</span> <span class="n">ctx_vec_proj</span><span class="p">,</span> <span class="n">ctx_vec</span>

  <span class="nd">@py_utils</span><span class="o">.</span><span class="n">NameScopeDecorator</span><span class="p">(</span>
      <span class="s1">&#39;MultiHeadedAttention/ComputeContextVectorWithCachedSource&#39;</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">ComputeContextVectorWithCachedSource</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                           <span class="n">theta</span><span class="p">,</span>
                                           <span class="n">concated_source_vecs</span><span class="p">,</span>
                                           <span class="n">concated_source_contexts</span><span class="p">,</span>
                                           <span class="n">source_padding</span><span class="p">,</span>
                                           <span class="n">source_segment_id</span><span class="p">,</span>
                                           <span class="n">query_vec</span><span class="p">,</span>
                                           <span class="n">attention_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                           <span class="n">per_step_source_padding</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                           <span class="n">step_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                           <span class="n">query_segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Same as the ComputeContextVectorWithSource api above, except values ...</span>

<span class="sd">    in source_vecs, source_contexts and source_padding are ordered differently.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this</span>
<span class="sd">        layer and its children layers.</span>
<span class="sd">      concated_source_vecs: Concated source vectors with shape [source_batch,</span>
<span class="sd">        time, hidden_dim].</span>
<span class="sd">      concated_source_contexts: Concated source contexts with shape [</span>
<span class="sd">        source_batch, time, context_dim].</span>
<span class="sd">      source_padding: Source padding with shape [source_batch, time, num_heads].</span>
<span class="sd">        If None, assume no padding.</span>
<span class="sd">      source_segment_id: Source segment id with shape</span>
<span class="sd">        [source_batch, time, num_heads].</span>
<span class="sd">      query_vec: a tensor of shape [target_batch, query_dim].</span>
<span class="sd">      attention_state: previous attention state. It is not used in</span>
<span class="sd">          AdditiveAttention, and is simply passed through.</span>
<span class="sd">      per_step_source_padding: Source sequence padding to apply at this step.</span>
<span class="sd">        If not None, it should be of shape [target_batch_size,</span>
<span class="sd">        source_seq_length].</span>
<span class="sd">      step_state: A NestedMap containing &#39;global_step&#39; and &#39;time_step&#39;.</span>
<span class="sd">        Required for deterministic dropout.</span>
<span class="sd">      query_segment_id: a tensor of shape [target_batch].</span>

<span class="sd">    Returns:</span>
<span class="sd">      The attention context vector:     [target_batch, source_dim]</span>
<span class="sd">      The attention probability vector: [target_batch, time]</span>
<span class="sd">      The new attention mechanism state: possibly nested tuple of tensors with</span>
<span class="sd">        dimensions [target_batch....]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">src_seq_len</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">num_heads</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_attention_heads</span>
    <span class="n">concated_source_vecs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span>
        <span class="p">[</span><span class="n">src_seq_len</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="c1"># TODO(yonghui): Rewrite the following with just one transpose.</span>
    <span class="n">concated_source_contexts</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">concated_source_contexts</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span>
            <span class="p">[</span><span class="n">src_seq_len</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">source_padding</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">source_padding</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span>
          <span class="p">[</span><span class="n">src_seq_len</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">src_seq_len</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">source_segment_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">source_segment_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
          <span class="p">[</span><span class="n">src_seq_len</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">source_padding</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">source_segment_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">source_segment_id</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span>
          <span class="p">[</span><span class="n">src_seq_len</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">])</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">ComputeContextVectorWithSource</span><span class="p">(</span>
        <span class="n">theta</span><span class="p">,</span> <span class="n">concated_source_vecs</span><span class="p">,</span> <span class="n">concated_source_contexts</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span>
        <span class="n">source_segment_id</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">attention_state</span><span class="p">,</span> <span class="n">per_step_source_padding</span><span class="p">,</span>
        <span class="n">step_state</span><span class="p">,</span> <span class="n">query_segment_id</span><span class="p">)</span></div>


<div class="viewcode-block" id="LocationSensitiveAttention"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.LocationSensitiveAttention">[docs]</a><span class="k">class</span> <span class="nc">LocationSensitiveAttention</span><span class="p">(</span><span class="n">BaseAttentionLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;An attention that also takes into account previously attended locations.</span>

<span class="sd">  See section 2.2 of this paper for a description of this technique:</span>
<span class="sd">  http://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition.pdf</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="LocationSensitiveAttention.Params"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.LocationSensitiveAttention.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Params for this LocationSensitiveAttention class.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">LocationSensitiveAttention</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;source_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of source nodes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;location_filter_size&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
             <span class="s1">&#39;Location filter size, should be an odd number e.g. 31.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;location_num_filters&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of location filters, e.g. 32.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;query_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of query nodes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;hidden_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of hidden nodes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;same_batch_size&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s1">&#39;True iff the source and target sequence has the same batch size.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;location_features&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;PREV_PROBS&#39;</span><span class="p">],</span>
        <span class="s1">&#39;List signals to run the convolutions on. Possible options are: &#39;</span>
        <span class="s1">&#39;PREV_PROBS, CUMULATIVE_PROBS.&#39;</span><span class="p">)</span>

    <span class="c1"># Fill in reasonable default for params init</span>
    <span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="o">.</span><span class="n">method</span> <span class="o">=</span> <span class="s1">&#39;gaussian_sqrt_dim&#39;</span>
    <span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="nd">@base_layer</span><span class="o">.</span><span class="n">initializer</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs an LocationSensitiveAttention object.&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">LocationSensitiveAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;Packed input is not supported yet for &#39;</span>
                                     <span class="s1">&#39;LocationsensitiveAttention.&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">atten_dropout_prob</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s1">&#39;dropout is not supported&#39;</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
      <span class="n">pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
          <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
          <span class="n">init</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="p">,</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;LocationSensitiveAttention_vars&#39;</span><span class="p">])</span>
      <span class="n">source_var_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">source_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">]</span>
      <span class="n">pc</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">source_var_shape</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;source_var&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">AddGlobalVN</span><span class="p">)</span>
      <span class="n">query_var_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">query_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">]</span>
      <span class="n">pc</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">query_var_shape</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;query_var&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">AddGlobalVN</span><span class="p">)</span>
      <span class="n">hidden_var_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">]</span>
      <span class="n">pc</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">hidden_var_shape</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;hidden_var&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">AddGlobalVN</span><span class="p">)</span>

      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">location_filter_size</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">1</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">location_num_filters</span> <span class="o">&gt;</span> <span class="mi">0</span>

      <span class="n">location_filter_shape</span> <span class="o">=</span> <span class="p">[</span>
          <span class="n">p</span><span class="o">.</span><span class="n">location_filter_size</span><span class="p">,</span>
          <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">location_features</span><span class="p">),</span> <span class="n">p</span><span class="o">.</span><span class="n">location_num_filters</span>
      <span class="p">]</span>
      <span class="c1"># TODO(yonghui): Don&#39;t hard code how params are initialized.</span>
      <span class="n">location_filter_pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
          <span class="n">shape</span><span class="o">=</span><span class="n">location_filter_shape</span><span class="p">,</span>
          <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="mf">0.05</span><span class="p">),</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;LocationSensitiveAttention_vars&#39;</span><span class="p">])</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;location_filter_var&#39;</span><span class="p">,</span> <span class="n">location_filter_pc</span><span class="p">,</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">AddGlobalVN</span><span class="p">)</span>
      <span class="n">location_var_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">location_num_filters</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">]</span>
      <span class="n">location_pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
          <span class="n">shape</span><span class="o">=</span><span class="n">location_var_shape</span><span class="p">,</span>
          <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="mf">0.05</span><span class="p">),</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;LocationSensitiveAttention_vars&#39;</span><span class="p">])</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;location_var&#39;</span><span class="p">,</span> <span class="n">location_pc</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">AddGlobalVN</span><span class="p">)</span>

    <span class="nd">@function</span><span class="o">.</span><span class="n">Defun</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="n">noinline</span><span class="o">=</span><span class="ow">not</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">use_tpu</span><span class="p">())</span>
    <span class="k">def</span> <span class="nf">AttenLogits</span><span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">,</span> <span class="n">query_vec_reshaped</span><span class="p">,</span> <span class="n">hidden_v</span><span class="p">,</span>
                    <span class="n">location_feats</span><span class="p">,</span> <span class="n">location_var</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Generates logits.&quot;&quot;&quot;</span>

      <span class="k">def</span> <span class="nf">CollapseOutDim</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>

      <span class="n">location_hidden</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">Matmul</span><span class="p">(</span>
          <span class="n">CollapseOutDim</span><span class="p">(</span><span class="n">location_feats</span><span class="p">),</span> <span class="n">location_var</span><span class="p">)</span>
      <span class="n">sl</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">location_feats</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
      <span class="n">tb</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">location_feats</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">hd</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">location_var</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
      <span class="n">location_hidden</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">location_hidden</span><span class="p">,</span> <span class="p">[</span><span class="n">tb</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">hd</span><span class="p">])</span>
      <span class="n">location_hidden</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">location_hidden</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
      <span class="n">sb</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">query_vec_reshaped</span><span class="p">)[</span><span class="mi">2</span><span class="p">]</span>
      <span class="n">bs_mult</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">query_vec_reshaped</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
      <span class="n">location_hidden</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">location_hidden</span><span class="p">,</span> <span class="p">[</span><span class="n">sl</span><span class="p">,</span> <span class="n">bs_mult</span><span class="p">,</span> <span class="n">sb</span><span class="p">,</span> <span class="n">hd</span><span class="p">])</span>

      <span class="c1"># Shape of summed is [sl, tb/sb, sb, hidden_dim].</span>
      <span class="n">summed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">concated_source_vecs</span> <span class="o">+</span> <span class="n">query_vec_reshaped</span> <span class="o">+</span>
                       <span class="n">location_hidden</span><span class="p">)</span>
      <span class="c1"># logits is of shape [sl * tb/sb * sb, 1]. Computes dot product</span>
      <span class="c1"># between v with every rows in &#39;summed&#39;. Then we reshape the</span>
      <span class="c1"># result to be of shape [sl, tb/sb, sb].</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">Matmul</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">summed</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">]),</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">hidden_v</span><span class="p">,</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">summed</span><span class="p">)[:</span><span class="mi">3</span><span class="p">])</span>
      <span class="k">return</span> <span class="n">logits</span>

    <span class="nd">@function</span><span class="o">.</span><span class="n">Defun</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="n">noinline</span><span class="o">=</span><span class="ow">not</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">use_tpu</span><span class="p">())</span>
    <span class="k">def</span> <span class="nf">AttenLogitsSameBatchSize</span><span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">,</span> <span class="n">query_vec_transformed</span><span class="p">,</span>
                                 <span class="n">hidden_v</span><span class="p">,</span> <span class="n">location_feats</span><span class="p">,</span> <span class="n">location_var</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Generates logits.</span>

<span class="sd">      Optimized code path for when the target and the source have the same batch</span>
<span class="sd">      size.</span>

<span class="sd">      Args:</span>
<span class="sd">        concated_source_vecs: Tensor of shape [sl, batch, dim]</span>
<span class="sd">        query_vec_transformed: Tensor of shape [batch, dim]</span>
<span class="sd">        hidden_v: Tensor of shape [dim]</span>
<span class="sd">        location_feats: Tensor of shape [batch, sl, location_feature_dim]</span>
<span class="sd">        location_var: Tensor of shape [location_feature_dim, dim]</span>

<span class="sd">      Returns:</span>
<span class="sd">        logits in the shape [sl, batch_size].</span>
<span class="sd">      &quot;&quot;&quot;</span>

      <span class="k">def</span> <span class="nf">CollapseOutDim</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>

      <span class="c1"># =&gt; [sl, batch, hd]</span>
      <span class="n">location_feats</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">location_feats</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
      <span class="n">location_hidden</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">Matmul</span><span class="p">(</span>
          <span class="n">CollapseOutDim</span><span class="p">(</span><span class="n">location_feats</span><span class="p">),</span> <span class="n">location_var</span><span class="p">)</span>
      <span class="n">sl</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">location_feats</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">tb</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">location_feats</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
      <span class="n">hd</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">location_var</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
      <span class="n">location_hidden</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">location_hidden</span><span class="p">,</span> <span class="p">[</span><span class="n">sl</span><span class="p">,</span> <span class="n">tb</span><span class="p">,</span> <span class="n">hd</span><span class="p">])</span>

      <span class="c1"># Shape of summed is [sl, sb, hidden_dim].</span>
      <span class="n">summed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">concated_source_vecs</span> <span class="o">+</span>
                       <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">query_vec_transformed</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span>
                       <span class="n">location_hidden</span><span class="p">)</span>
      <span class="c1"># logits is of shape [sl * sb, 1]. Computes dot product</span>
      <span class="c1"># between v with every rows in &#39;summed&#39;. Then we reshape the</span>
      <span class="c1"># result to be of shape [sl, tb].</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">Matmul</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">summed</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">]),</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">hidden_v</span><span class="p">,</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">summed</span><span class="p">)[:</span><span class="mi">2</span><span class="p">])</span>
      <span class="c1"># ==&gt; of shape [sl, tb]</span>
      <span class="k">return</span> <span class="n">logits</span>

    <span class="k">def</span> <span class="nf">Atten</span><span class="p">(</span><span class="n">hidden_var</span><span class="p">,</span> <span class="n">query_var</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span> <span class="n">concated_source_vecs</span><span class="p">,</span>
              <span class="n">concated_source_contexts</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">attention_state</span><span class="p">,</span>
              <span class="n">location_filter_var</span><span class="p">,</span> <span class="n">location_var</span><span class="p">,</span> <span class="n">per_step_source_padding</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Computes the attention context vector.&quot;&quot;&quot;</span>
      <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
      <span class="c1"># attention_state shape [batch, slen, len(p.location_features)]</span>
      <span class="c1"># it contains previous and accumulated attention probabilites.</span>
      <span class="n">attention_state</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span>
          <span class="n">attention_state</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">location_features</span><span class="p">)])</span>

      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
        <span class="n">location_feats</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">conv1d</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">attention_state</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">location_filter_var</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
            <span class="mi">1</span><span class="p">,</span>
            <span class="s1">&#39;SAME&#39;</span><span class="p">,</span>
            <span class="n">data_format</span><span class="o">=</span><span class="s1">&#39;NHWC&#39;</span><span class="p">)</span>
        <span class="n">location_feats</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">location_feats</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">location_feats</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">conv1d</span><span class="p">(</span>
            <span class="n">attention_state</span><span class="p">,</span> <span class="n">location_filter_var</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;SAME&#39;</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s1">&#39;NHWC&#39;</span><span class="p">)</span>
      <span class="c1"># concated_source_vecs is of shape [sl, sb, dims]</span>
      <span class="c1"># concated_source_contexts is of shape [sb, sl, context_dim]</span>
      <span class="c1"># query_vec is of shape [tb, dims]</span>
      <span class="n">sb</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
      <span class="n">tb</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">multiplier</span> <span class="o">=</span> <span class="n">tb</span> <span class="o">//</span> <span class="n">sb</span>
      <span class="c1"># concated_source_vecs is reshaped to [sl, 1, sb, hidden_dims]</span>
      <span class="n">concated_source_vecs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
      <span class="n">query_vec_transformed</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">Matmul</span><span class="p">(</span><span class="n">query_vec</span><span class="p">,</span> <span class="n">query_var</span><span class="p">)</span>
      <span class="c1"># query_vec is reshaped to [1, tb/sb, sb, hidden_dims].</span>
      <span class="n">query_vec_reshaped</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">query_vec_transformed</span><span class="p">,</span>
                                      <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">multiplier</span><span class="p">,</span> <span class="n">sb</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">])</span>
      <span class="c1"># logits is of shape [sl, tb/sb, sb]</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">AttenLogits</span><span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">,</span> <span class="n">query_vec_reshaped</span><span class="p">,</span> <span class="n">hidden_var</span><span class="p">,</span>
                           <span class="n">location_feats</span><span class="p">,</span> <span class="n">location_var</span><span class="p">)</span>
      <span class="c1"># Take out the padding states.</span>
      <span class="c1"># _source_padding is of shape [sl, sb].</span>
      <span class="c1"># reshaped to [sl, 1,  sb].</span>
      <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">source_padding</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
      <span class="n">per_step_source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">per_step_source_padding</span><span class="p">),</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">multiplier</span><span class="p">,</span> <span class="n">sb</span><span class="p">])</span>
      <span class="n">source_padding</span> <span class="o">+=</span> <span class="n">per_step_source_padding</span>
      <span class="c1"># Reshape logits to a matrix of shape [tb, sl] and takes the</span>
      <span class="c1"># softmax to compute the probabilities.</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">tb</span><span class="p">]))</span>
      <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">source_padding</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">tb</span><span class="p">]))</span>
      <span class="n">probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_PaddedSoftmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">)</span>
      <span class="c1"># Reshape probs to be of shape [tb/sb, sb, sl].</span>
      <span class="n">probs_reshaped</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="p">[</span><span class="n">multiplier</span><span class="p">,</span> <span class="n">sb</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="c1"># Transpose probs to be of shape [sb, tb/sb, sl]</span>
      <span class="n">probs_reshaped</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">probs_reshaped</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
      <span class="c1"># [sb, tb/sb, sl] * [sb, sl, context_dim] = [sb, tb/sb, context_dim]</span>
      <span class="n">summed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">probs_reshaped</span><span class="p">,</span> <span class="n">concated_source_contexts</span><span class="p">)</span>
      <span class="c1"># summed is of shape [tb/sb, sb, context_dim]</span>
      <span class="n">summed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">summed</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">summed</span><span class="p">,</span> <span class="p">[</span><span class="n">tb</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">probs</span>

    <span class="k">def</span> <span class="nf">AttenSameBatchSize</span><span class="p">(</span><span class="n">hidden_var</span><span class="p">,</span> <span class="n">query_var</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span>
                           <span class="n">concated_source_vecs</span><span class="p">,</span> <span class="n">concated_source_contexts</span><span class="p">,</span>
                           <span class="n">query_vec</span><span class="p">,</span> <span class="n">attention_state</span><span class="p">,</span> <span class="n">location_filter_var</span><span class="p">,</span>
                           <span class="n">location_var</span><span class="p">,</span> <span class="n">per_step_source_padding</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Computes the attention context vector.</span>

<span class="sd">      Optimized code path for when source and target have the same batch size.</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="k">del</span> <span class="n">per_step_source_padding</span>
      <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
      <span class="c1"># attention_state shape [batch, slen, len(p.location_features)]</span>
      <span class="c1"># it contains previous and accumulated attention probabilites.</span>
      <span class="n">attention_state</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span>
          <span class="n">attention_state</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">location_features</span><span class="p">)])</span>

      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
        <span class="n">location_feats</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">conv1d</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">attention_state</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">location_filter_var</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
            <span class="mi">1</span><span class="p">,</span>
            <span class="s1">&#39;SAME&#39;</span><span class="p">,</span>
            <span class="n">data_format</span><span class="o">=</span><span class="s1">&#39;NHWC&#39;</span><span class="p">)</span>
        <span class="n">location_feats</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">location_feats</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">location_feats</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">conv1d</span><span class="p">(</span>
            <span class="n">attention_state</span><span class="p">,</span> <span class="n">location_filter_var</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;SAME&#39;</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s1">&#39;NHWC&#39;</span><span class="p">)</span>
      <span class="n">query_vec_transformed</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">Matmul</span><span class="p">(</span><span class="n">query_vec</span><span class="p">,</span> <span class="n">query_var</span><span class="p">)</span>
      <span class="c1"># logits is of shape [sl, sb]</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">AttenLogitsSameBatchSize</span><span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">,</span>
                                        <span class="n">query_vec_transformed</span><span class="p">,</span> <span class="n">hidden_var</span><span class="p">,</span>
                                        <span class="n">location_feats</span><span class="p">,</span> <span class="n">location_var</span><span class="p">)</span>
      <span class="c1"># =&gt; [sl, tb]</span>
      <span class="n">logits</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">source_padding</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
      <span class="c1"># Reshape logits to a matrix of shape [tb, sl] and takes the</span>
      <span class="c1"># softmax to compute the probabilities.</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
      <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">source_padding</span><span class="p">)</span>
      <span class="n">probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_PaddedSoftmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">)</span>
      <span class="n">summed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">concated_source_contexts</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">summed</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">probs</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">same_batch_size</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_ctx_vec</span> <span class="o">=</span> <span class="n">AttenSameBatchSize</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_ctx_vec</span> <span class="o">=</span> <span class="n">Atten</span>

    <span class="k">def</span> <span class="nf">EncodeSource</span><span class="p">(</span><span class="n">src_w</span><span class="p">,</span> <span class="n">vecs</span><span class="p">,</span> <span class="n">ctxs</span><span class="p">):</span>
      <span class="n">time</span><span class="p">,</span> <span class="n">batch</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">vecs</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
      <span class="n">ctxs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">ctxs</span><span class="p">,</span> <span class="p">[</span><span class="n">time</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="n">transformed_vecs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
          <span class="n">py_utils</span><span class="o">.</span><span class="n">Matmul</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">vecs</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">source_dim</span><span class="p">]),</span> <span class="n">src_w</span><span class="p">),</span>
          <span class="p">[</span><span class="n">time</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="n">transposed_ctxs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">ctxs</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
      <span class="k">return</span> <span class="n">transformed_vecs</span><span class="p">,</span> <span class="n">transposed_ctxs</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_encode_source</span> <span class="o">=</span> <span class="n">EncodeSource</span>

<div class="viewcode-block" id="LocationSensitiveAttention.InitForSourcePacked"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.LocationSensitiveAttention.InitForSourcePacked">[docs]</a>  <span class="k">def</span> <span class="nf">InitForSourcePacked</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                          <span class="n">theta</span><span class="p">,</span>
                          <span class="n">source_vecs</span><span class="p">,</span>
                          <span class="n">source_contexts</span><span class="p">,</span>
                          <span class="n">source_padding</span><span class="p">,</span>
                          <span class="n">source_segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initialize attention for the given source vectors.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this</span>
<span class="sd">        layer and its children layers.</span>
<span class="sd">      source_vecs: A single tensor of shape [time, batch_size, source_dim].</span>
<span class="sd">      source_contexts: A single tensor of shape [time, batch_size, some_dim].</span>
<span class="sd">      source_padding: A tensor of shape [time, batch_size].</span>
<span class="sd">      source_segment_id: A tensor of shape [time, batch_size].</span>

<span class="sd">    Returns:</span>
<span class="sd">      Concated source vectors, concated source contexts, and source paddings.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_source_init_done</span> <span class="o">=</span> <span class="kc">True</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_source_padding</span> <span class="o">=</span> <span class="n">source_padding</span>
      <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_concated_source_vecs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_concated_source_contexts</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_encode_source</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">source_var</span><span class="p">,</span> <span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">))</span>
      <span class="k">if</span> <span class="n">source_segment_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">source_segment_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">source_padding</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_source_segment_id</span> <span class="o">=</span> <span class="n">source_segment_id</span>
    <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_concated_source_vecs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_concated_source_contexts</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_source_padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_source_segment_id</span><span class="p">)</span></div>

<div class="viewcode-block" id="LocationSensitiveAttention.ZeroAttentionState"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.LocationSensitiveAttention.ZeroAttentionState">[docs]</a>  <span class="k">def</span> <span class="nf">ZeroAttentionState</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source_seq_length</span><span class="p">,</span> <span class="n">decoder_batch_size</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">dtype</span>
    <span class="n">num_features</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">location_features</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="n">state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">decoder_batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_features</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">),</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
              <span class="p">[</span><span class="n">decoder_batch_size</span><span class="p">,</span> <span class="n">source_seq_length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_features</span><span class="p">],</span>
              <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
      <span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
      <span class="c1"># Having the last dim being 1 or 2 is very inefficient on tpu, and hence</span>
      <span class="c1"># we reshape to combine the last two dims.</span>
      <span class="n">state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="p">[</span><span class="n">decoder_batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="k">return</span> <span class="n">state</span></div>

<div class="viewcode-block" id="LocationSensitiveAttention.ComputeContextVectorWithSource"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.LocationSensitiveAttention.ComputeContextVectorWithSource">[docs]</a>  <span class="k">def</span> <span class="nf">ComputeContextVectorWithSource</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                     <span class="n">theta</span><span class="p">,</span>
                                     <span class="n">concated_source_vecs</span><span class="p">,</span>
                                     <span class="n">concated_source_contexts</span><span class="p">,</span>
                                     <span class="n">source_padding</span><span class="p">,</span>
                                     <span class="n">source_segment_id</span><span class="p">,</span>
                                     <span class="n">query_vec</span><span class="p">,</span>
                                     <span class="n">attention_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                     <span class="n">per_step_source_padding</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                     <span class="n">step_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                     <span class="n">query_segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the context vector given the current query output.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this</span>
<span class="sd">        layer and its children layers.</span>
<span class="sd">      concated_source_vecs: Concated source vectors with shape [time,</span>
<span class="sd">        batch_size, hidden_dim].</span>
<span class="sd">      concated_source_contexts: Concated source contexts with shape [</span>
<span class="sd">        batch_size, time, context_dim].</span>
<span class="sd">      source_padding: Source padding with shape [time, batch_size].</span>
<span class="sd">      source_segment_id: Source segment id with shape [time, batch_size].</span>
<span class="sd">      query_vec: a tensor of shape [batch_size, query_dim].</span>
<span class="sd">      attention_state: If</span>
<span class="sd">        `params().location_features == [&#39;PREV_PROBS&#39;, &#39;CUMULATIVE_PROBS&#39;]`,</span>
<span class="sd">        then `attention_state` is a tensor of shape [batch_size, src_len * 2].</span>

<span class="sd">        - attention_state[:, :, 0] contains previous attention probabilities</span>
<span class="sd">        - attention_state[:, :, 1] contains a sum over previous timesteps of</span>
<span class="sd">          attention probabilities.</span>
<span class="sd">      per_step_source_padding: Source sequence padding to apply at this step.</span>
<span class="sd">        If not None, it should be of shape [target_batch_size,</span>
<span class="sd">        source_seq_length].</span>
<span class="sd">      step_state: A `.NestedMap` containing &#39;global_step&#39; and &#39;time_step&#39;.</span>
<span class="sd">        Required for deterministic dropout.</span>
<span class="sd">      query_segment_id: Query segment id with shape [batch_size].</span>

<span class="sd">    Note: concated_source_vecs are the vectors that are used to compute the</span>
<span class="sd">    attention score between the query_vec and each concated_source_vec.</span>
<span class="sd">    The concated_source_contexts are the vectors that compose the result.</span>
<span class="sd">    The attention context vector is computed as a weighted average of the</span>
<span class="sd">    concated_source_contexts, using the scores that were computed using</span>
<span class="sd">    concated_source_vecs.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tuple of 3 elements.</span>
<span class="sd">        The attention context vector:</span>
<span class="sd">          [batch_size, context_dim]</span>
<span class="sd">        The attention probability vector:</span>
<span class="sd">          [batch_size, time]</span>
<span class="sd">        The new attention mechanism state:</span>
<span class="sd">          possibly nested tuple of tensors with dimensions [target_batch, ...]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">del</span> <span class="n">source_segment_id</span>
    <span class="k">del</span> <span class="n">query_segment_id</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">same_batch_size</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">per_step_source_padding</span> <span class="ow">is</span> <span class="kc">None</span>
    <span class="n">query_batch_size</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">source_seq_length</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">source_padding</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">per_step_source_padding</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">zero</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">query_vec</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="n">per_step_source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">fill</span><span class="p">([</span><span class="n">query_batch_size</span><span class="p">,</span> <span class="n">source_seq_length</span><span class="p">],</span>
                                        <span class="n">zero</span><span class="p">)</span>
    <span class="n">per_step_source_padding</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span>
        <span class="n">per_step_source_padding</span><span class="p">,</span> <span class="p">[</span><span class="n">query_batch_size</span><span class="p">,</span> <span class="n">source_seq_length</span><span class="p">])</span>

    <span class="n">hidden</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">AddPerStepVN</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">hidden_var</span><span class="p">)</span>
    <span class="n">query</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">AddPerStepVN</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">query_var</span><span class="p">)</span>
    <span class="n">location_filter</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">AddPerStepVN</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">location_filter_var</span><span class="p">)</span>
    <span class="n">location</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">AddPerStepVN</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">location_var</span><span class="p">)</span>

    <span class="n">bs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">attention_state</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">num_location_features</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">location_features</span><span class="p">)</span>
    <span class="n">attention_state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">attention_state</span><span class="p">,</span>
                                 <span class="p">[</span><span class="n">bs</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_location_features</span><span class="p">])</span>

    <span class="n">ctx_vec</span><span class="p">,</span> <span class="n">prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ctx_vec</span><span class="p">(</span>
        <span class="n">hidden</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span> <span class="n">concated_source_vecs</span><span class="p">,</span>
        <span class="n">concated_source_contexts</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">attention_state</span><span class="p">,</span> <span class="n">location_filter</span><span class="p">,</span>
        <span class="n">location</span><span class="p">,</span> <span class="n">per_step_source_padding</span><span class="p">)</span>

    <span class="n">new_feats</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;PREV_PROBS&#39;</span><span class="p">:</span> <span class="n">prob</span><span class="p">}</span>
    <span class="k">if</span> <span class="s1">&#39;CUMULATIVE_PROBS&#39;</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">location_features</span><span class="p">:</span>
      <span class="n">new_feats</span><span class="p">[</span><span class="s1">&#39;CUMULATIVE_PROBS&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
          <span class="n">prob</span> <span class="o">+</span> <span class="n">attention_state</span><span class="p">[:,</span> <span class="p">:,</span>
                                 <span class="n">p</span><span class="o">.</span><span class="n">location_features</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s1">&#39;CUMULATIVE_PROBS&#39;</span><span class="p">)])</span>
    <span class="n">new_attention_state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span>
        <span class="p">[</span><span class="n">new_feats</span><span class="p">[</span><span class="n">f</span><span class="p">]</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">location_features</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">new_attention_state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">new_attention_state</span><span class="p">,</span> <span class="p">[</span><span class="n">bs</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">ctx_vec</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">new_attention_state</span></div></div>


<div class="viewcode-block" id="MergeSourcePaddingWithPerStepSourcePadding"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.MergeSourcePaddingWithPerStepSourcePadding">[docs]</a><span class="k">def</span> <span class="nf">MergeSourcePaddingWithPerStepSourcePadding</span><span class="p">(</span><span class="n">source_padding</span><span class="p">,</span>
                                               <span class="n">per_step_source_padding</span><span class="p">,</span> <span class="n">tb</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Merges source padding with per-step source padding.</span>

<span class="sd">  Args:</span>
<span class="sd">    source_padding: [sl, sb].</span>
<span class="sd">    per_step_source_padding: [tb, sl].</span>
<span class="sd">    tb: target batch size.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A tensor of shape [tb, sl].</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># source_padding is of shape [sl, sb].</span>
  <span class="n">sl</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">source_padding</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
  <span class="n">sb</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">source_padding</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

  <span class="k">if</span> <span class="n">per_step_source_padding</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">zero</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">source_padding</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">per_step_source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">fill</span><span class="p">([</span><span class="n">tb</span><span class="p">,</span> <span class="n">sl</span><span class="p">],</span> <span class="n">zero</span><span class="p">)</span>
  <span class="n">per_step_source_padding</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">per_step_source_padding</span><span class="p">,</span> <span class="p">[</span><span class="n">tb</span><span class="p">,</span> <span class="n">sl</span><span class="p">])</span>

  <span class="c1"># Transpose and reshape source_padding to [1, sb,  sl].</span>
  <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">source_padding</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
  <span class="c1"># Merge source_padding and per_step_source_padding.</span>
  <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">source_padding</span><span class="p">,</span>
                              <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">per_step_source_padding</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">sb</span><span class="p">,</span> <span class="n">sl</span><span class="p">]))</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">source_padding</span><span class="p">,</span> <span class="p">[</span><span class="n">tb</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span></div>


<div class="viewcode-block" id="MonotonicAttention"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.MonotonicAttention">[docs]</a><span class="k">class</span> <span class="nc">MonotonicAttention</span><span class="p">(</span><span class="n">BaseAttentionLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;An attention mechanism which enforces monotonic alignments.</span>

<span class="sd">  This layer implements the monotonic attention mechanism described in</span>
<span class="sd">  Online and Linear-Time Attention by Enforcing Mononotonic Alignments</span>
<span class="sd">  (https://arxiv.org/abs/1704.00784).  It is used in exactly the same way as</span>
<span class="sd">  AdditiveAttention, but both the attention distribution and the energy function</span>
<span class="sd">  are different.</span>

<span class="sd">  Rather than using a softmax, this mechanism feeds the attention energy into a</span>
<span class="sd">  (hard or soft) sigmoid and treats the output as Bernoulli probabilities</span>
<span class="sd">  representing the probability of attending to a given entry in the input</span>
<span class="sd">  sequence, processed from left-to-right.  Based on this interpretation, the</span>
<span class="sd">  resulting distribution over input sequence entries is computed with a dynamic</span>
<span class="sd">  program.  The intended use is to train with soft sigmoids according to the</span>
<span class="sd">  expected output (setting param hard_sigmoid=False), then use hard sigmoids at</span>
<span class="sd">  test time to allow for online and linear-time decoding.  To encourge the train</span>
<span class="sd">  and test-time behavior to be similar, noise can optionally be added to the</span>
<span class="sd">  sigmoid activations during training (param pre_sigmoid_noise).  For the energy</span>
<span class="sd">  function, rather than computing::</span>

<span class="sd">    E = dot(v, tanh(dot(W, query) + dot(W, encoder_states)))</span>

<span class="sd">  it computes::</span>

<span class="sd">    E = dot(g*v/||v||, tanh(dot(W, query) + dot(W, encoder_states) + b)) + r</span>

<span class="sd">  where g and r are scalars and b is a vector, and ||v|| is the L2 norm of v.</span>
<span class="sd">  instead.  These modifications address the fact that the sigmoids in the</span>
<span class="sd">  monotonic attention mechanism are sensitive to offset and a bit harder to</span>
<span class="sd">  train compared to the softmax function.  It can be helpful to initialize the</span>
<span class="sd">  energy bias scalar r to a negative value (param hidden_bias_init).</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="MonotonicAttention.Params"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.MonotonicAttention.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Params for this MonotonicAttention class.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">MonotonicAttention</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;source_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of source nodes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;query_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of query nodes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;hidden_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of hidden nodes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;pre_sigmoid_noise&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Standard deviation of pre-sigmoid noise.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;hidden_bias_init&#39;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;Initial value of hidden bias.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;hard_sigmoid&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;Whether to use a hard sigmoid.&#39;</span><span class="p">)</span>
    <span class="c1"># Fill in reasonable default for params init</span>
    <span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="o">.</span><span class="n">method</span> <span class="o">=</span> <span class="s1">&#39;gaussian_sqrt_dim&#39;</span>
    <span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="nd">@base_layer</span><span class="o">.</span><span class="n">initializer</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs an MonotonicAttention object.&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">MonotonicAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;Packed input not supported for &#39;</span>
                                     <span class="s1">&#39;Monotonic Attention.&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">atten_dropout_prob</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s1">&#39;dropout is not supported&#39;</span><span class="p">)</span>

    <span class="c1"># When running eval, don&#39;t add pre-sigmoid noise, and use a hard sigmoid to</span>
    <span class="c1"># match behavior of online decoding.</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">is_eval</span><span class="p">:</span>
      <span class="n">p</span><span class="o">.</span><span class="n">pre_sigmoid_noise</span> <span class="o">=</span> <span class="mf">0.</span>
      <span class="n">p</span><span class="o">.</span><span class="n">hard_sigmoid</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="n">pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
          <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
          <span class="n">init</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="p">,</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;MonotonicAttention_vars&#39;</span><span class="p">])</span>
      <span class="c1"># source is the weight matrix for the memory/encoder states</span>
      <span class="n">source_var_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">source_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">]</span>
      <span class="n">pc</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">source_var_shape</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;source_var&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">AddGlobalVN</span><span class="p">)</span>
      <span class="c1"># query is the weight matrix for the query/decoder RNN state</span>
      <span class="n">query_var_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">query_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">]</span>
      <span class="n">pc</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">query_var_shape</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;query_var&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">AddGlobalVN</span><span class="p">)</span>
      <span class="c1"># hidden is the pre-softmax vector which converts from tanh to scalar</span>
      <span class="n">hidden_var_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">]</span>
      <span class="n">pc</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">hidden_var_shape</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;hidden_var&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">AddGlobalVN</span><span class="p">)</span>

      <span class="c1"># energy_bias is the bias vector which appears inside of tanh</span>
      <span class="n">energy_bias_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">]</span>
      <span class="n">pc</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">energy_bias_shape</span>
      <span class="c1"># Initialize the bias vector to all zeros</span>
      <span class="n">pc</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">method</span> <span class="o">=</span> <span class="s1">&#39;constant&#39;</span>
      <span class="n">pc</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="mf">0.0</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;energy_bias_var&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">)</span>
      <span class="c1"># hidden_scale is the weight normalization scale for hidden</span>
      <span class="n">hidden_scale_var_shape</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="n">pc</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">hidden_scale_var_shape</span>
      <span class="c1"># Initialize so that the initial scale is 1/sqrt(hidden_dim)</span>
      <span class="n">pc</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;hidden_scale_var&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">)</span>
      <span class="c1"># hidden_bias is the bias scalar applied before the sigmoid</span>
      <span class="n">hidden_bias_var_shape</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="n">pc</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">hidden_bias_var_shape</span>
      <span class="c1"># Use the hidden_bias_init hyperparam to set the initial value</span>
      <span class="n">pc</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_bias_init</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;hidden_bias_var&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">)</span>

      <span class="c1"># Create seeds for stateless random number generator.</span>
      <span class="n">random_seed_dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span>
      <span class="n">_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_step_counter</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span>
          <span class="n">name</span><span class="o">=</span><span class="s1">&#39;atten_step_counter&#39;</span><span class="p">,</span>
          <span class="n">params</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">([],</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
                                       <span class="n">random_seed_dtype</span><span class="p">),</span>
          <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
      <span class="n">vname</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_step_counter</span><span class="o">.</span><span class="n">name</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_prng_seed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span>
          <span class="n">py_utils</span><span class="o">.</span><span class="n">GenerateSeedFromName</span><span class="p">(</span><span class="n">vname</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">random_seed_dtype</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">random_seed</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prng_seed</span> <span class="o">+=</span> <span class="n">p</span><span class="o">.</span><span class="n">random_seed</span>

    <span class="k">def</span> <span class="nf">EncodeSource</span><span class="p">(</span><span class="n">src_w</span><span class="p">,</span> <span class="n">vecs</span><span class="p">,</span> <span class="n">ctxs</span><span class="p">):</span>
      <span class="n">time</span><span class="p">,</span> <span class="n">batch</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">vecs</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
      <span class="n">ctxs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">ctxs</span><span class="p">,</span> <span class="p">[</span><span class="n">time</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="n">transformed_vecs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
          <span class="n">py_utils</span><span class="o">.</span><span class="n">Matmul</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">vecs</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">source_dim</span><span class="p">]),</span> <span class="n">src_w</span><span class="p">),</span>
          <span class="p">[</span><span class="n">time</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="n">transposed_ctxs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">ctxs</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
      <span class="k">return</span> <span class="n">transformed_vecs</span><span class="p">,</span> <span class="n">transposed_ctxs</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_encode_source</span> <span class="o">=</span> <span class="n">EncodeSource</span>

<div class="viewcode-block" id="MonotonicAttention.PackSource"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.MonotonicAttention.PackSource">[docs]</a>  <span class="k">def</span> <span class="nf">PackSource</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">theta</span><span class="p">,</span>
                 <span class="n">source_vecs</span><span class="p">,</span>
                 <span class="n">source_contexts</span><span class="p">,</span>
                 <span class="n">source_padding</span><span class="p">,</span>
                 <span class="n">source_segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Packs source vectors. Does not change attention state.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this</span>
<span class="sd">        layer and its children layers.</span>
<span class="sd">      source_vecs: A single tensor of shape [time, batch_size, source_dim].</span>
<span class="sd">      source_contexts: A single tensor of shape [time, batch_size, some_dim].</span>
<span class="sd">      source_padding: A tensor of shape [time, batch_size].</span>
<span class="sd">      source_segment_id: A tensor of shape [time, batch_size].</span>

<span class="sd">    Returns:</span>
<span class="sd">      Concated source vectors, concated source contexts, and source paddings.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">,</span> <span class="n">concated_source_contexts</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_encode_source</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">source_var</span><span class="p">,</span> <span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">))</span>
      <span class="k">if</span> <span class="n">source_segment_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">source_segment_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">source_padding</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">,</span> <span class="n">concated_source_contexts</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span>
            <span class="n">source_segment_id</span><span class="p">)</span></div>

<div class="viewcode-block" id="MonotonicAttention.InitForSourcePacked"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.MonotonicAttention.InitForSourcePacked">[docs]</a>  <span class="k">def</span> <span class="nf">InitForSourcePacked</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                          <span class="n">theta</span><span class="p">,</span>
                          <span class="n">source_vecs</span><span class="p">,</span>
                          <span class="n">source_contexts</span><span class="p">,</span>
                          <span class="n">source_padding</span><span class="p">,</span>
                          <span class="n">source_segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initialize attention for the given source vectors.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this</span>
<span class="sd">        layer and its children layers.</span>
<span class="sd">      source_vecs: A single tensor of shape [time, batch_size, source_dim].</span>
<span class="sd">      source_contexts: A single tensor of shape [time, batch_size, some_dim].</span>
<span class="sd">      source_padding: A tensor of shape [time, batch_size].</span>
<span class="sd">      source_segment_id: A tensor of shape [time, batch_size].</span>

<span class="sd">    Returns:</span>
<span class="sd">      Concated source vectors, concated source contexts, and source paddings.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_source_init_done</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_concated_source_vecs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_concated_source_contexts</span><span class="p">,</span>
     <span class="bp">self</span><span class="o">.</span><span class="n">_source_padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_source_segment_id</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">PackSource</span><span class="p">(</span>
         <span class="n">theta</span><span class="p">,</span> <span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span> <span class="n">source_segment_id</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_concated_source_vecs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_concated_source_contexts</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_source_padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_source_segment_id</span><span class="p">)</span></div>

<div class="viewcode-block" id="MonotonicAttention.ZeroAttentionState"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.MonotonicAttention.ZeroAttentionState">[docs]</a>  <span class="k">def</span> <span class="nf">ZeroAttentionState</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source_seq_length</span><span class="p">,</span> <span class="n">decoder_batch_size</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">dtype</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="c1"># Set initial previous attention to [1, 0, ... 0] to avoid special-casing</span>
      <span class="n">emit_probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">decoder_batch_size</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
          <span class="n">source_seq_length</span><span class="p">,</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
          <span class="n">emit_probs</span><span class="o">=</span><span class="n">emit_probs</span><span class="p">,</span>
          <span class="c1"># stateless.stateless_random_normal() requires seeds of shape [2].</span>
          <span class="n">random_seed</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_prng_seed</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_step_counter</span><span class="p">]))</span></div>

<div class="viewcode-block" id="MonotonicAttention.ComputeProbabilities"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.MonotonicAttention.ComputeProbabilities">[docs]</a>  <span class="k">def</span> <span class="nf">ComputeProbabilities</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">concated_source_vecs</span><span class="p">,</span>
                           <span class="n">merged_source_padding</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">attention_state</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes probabilities of emissions.&quot;&quot;&quot;</span>

    <span class="c1"># concated_source_contexts is of shape [sb, sl, context_dim]</span>
    <span class="c1"># query_vec is of shape [tb, dims]</span>
    <span class="n">sb</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">tb</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">multiplier</span> <span class="o">=</span> <span class="n">tb</span> <span class="o">//</span> <span class="n">sb</span>

    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="c1"># noinline and compiled cannot be set at the same time</span>
    <span class="nd">@function</span><span class="o">.</span><span class="n">Defun</span><span class="p">(</span><span class="o">*</span><span class="p">([</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">*</span> <span class="mi">7</span><span class="p">),</span> <span class="n">noinline</span><span class="o">=</span><span class="ow">not</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">use_tpu</span><span class="p">())</span>
    <span class="k">def</span> <span class="nf">AttenLogits</span><span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">query_v</span><span class="p">,</span> <span class="n">energy_b</span><span class="p">,</span>
                    <span class="n">hidden_v</span><span class="p">,</span> <span class="n">hidden_g</span><span class="p">,</span> <span class="n">hidden_b</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Computes logits from source, query, and variables.</span>

<span class="sd">      Args:</span>
<span class="sd">        concated_source_vecs: [sl, sb, hidden_dims].</span>
<span class="sd">        query_vec: [tb, query_dim].</span>
<span class="sd">        query_v: [query_dim, hidden_dim]</span>
<span class="sd">        energy_b: [hidden_dim].</span>
<span class="sd">        hidden_v: [hidden_dim].</span>
<span class="sd">        hidden_g: [].</span>
<span class="sd">        hidden_b: [].</span>

<span class="sd">      Returns:</span>
<span class="sd">        logits: [tb, sl].</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="c1"># Apply query matrix to query. Becomes [tb, hidden_dim].</span>
      <span class="n">query_vec_transformed</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">Matmul</span><span class="p">(</span>
          <span class="n">query_vec</span><span class="p">,</span> <span class="n">query_v</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;query_transformation&#39;</span><span class="p">)</span>
      <span class="c1"># query_vec is reshaped to [1, tb/sb, sb, hidden_dim].</span>
      <span class="n">query_vec_reshaped</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">query_vec_transformed</span><span class="p">,</span>
                                      <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">multiplier</span><span class="p">,</span> <span class="n">sb</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">])</span>

      <span class="c1"># [sl, 1, sb, hidden_dim].</span>
      <span class="n">concated_source_vecs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
      <span class="n">energy_b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">energy_b</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="c1"># Shape of summed is [sl, tb/sb, sb, hidden_dim].</span>
      <span class="n">summed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">concated_source_vecs</span> <span class="o">+</span> <span class="n">query_vec_reshaped</span> <span class="o">+</span> <span class="n">energy_b</span><span class="p">)</span>
      <span class="n">hidden_v</span> <span class="o">=</span> <span class="n">hidden_g</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">l2_normalize</span><span class="p">(</span><span class="n">hidden_v</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      <span class="c1"># logits is of shape [sl * tb/sb * sb, 1]. Computes dot product</span>
      <span class="c1"># between v with every rows in &#39;summed&#39;. Then we reshape the</span>
      <span class="c1"># result to be of shape [sl, tb/sb, sb].</span>
      <span class="c1">#</span>
      <span class="c1"># Another equivalent way is to do:</span>
      <span class="c1">#  logits = tf.reduce_sum(summed *</span>
      <span class="c1">#                         tf.reshape(v, [1, 1, 1, hidden_dim]), 3)</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">Matmul</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">summed</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">]),</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">hidden_v</span><span class="p">,</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
      <span class="n">logits</span> <span class="o">+=</span> <span class="n">hidden_b</span>
      <span class="c1"># [tb, sl].</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">tb</span><span class="p">]),</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
      <span class="k">return</span> <span class="n">logits</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;logits&#39;</span><span class="p">):</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">AttenLogits</span><span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">query_var</span><span class="p">,</span>
                           <span class="n">theta</span><span class="o">.</span><span class="n">energy_bias_var</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">hidden_var</span><span class="p">,</span>
                           <span class="n">theta</span><span class="o">.</span><span class="n">hidden_scale_var</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">hidden_bias_var</span><span class="p">)</span>

    <span class="n">previous_attention</span> <span class="o">=</span> <span class="n">attention_state</span><span class="o">.</span><span class="n">emit_probs</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;prob&#39;</span><span class="p">):</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">hard_sigmoid</span><span class="p">:</span>
        <span class="c1"># If using a hard sigmoid, just compare against 0</span>
        <span class="n">p_choose_i</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">greater</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="c1"># Never choose padded values.</span>
        <span class="n">p_choose_i</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">merged_source_padding</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">,</span>
                              <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p_choose_i</span><span class="p">),</span> <span class="n">p_choose_i</span><span class="p">)</span>
        <span class="c1"># Compute probability distribution assuming hard probabilities</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">seq2seq</span><span class="o">.</span><span class="n">monotonic_attention</span><span class="p">(</span>
            <span class="n">p_choose_i</span><span class="p">,</span> <span class="n">previous_attention</span><span class="p">,</span> <span class="s1">&#39;hard&#39;</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Compute pre-sigmoid noise.</span>
        <span class="n">activation_noise</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">stateless</span><span class="o">.</span><span class="n">stateless_random_normal</span><span class="p">(</span>
            <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">logits</span><span class="p">),</span>
            <span class="n">attention_state</span><span class="o">.</span><span class="n">random_seed</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="c1"># Compute sigmoid probabilities.</span>
        <span class="n">p_choose_i</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span>
            <span class="n">logits</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">pre_sigmoid_noise</span> <span class="o">*</span> <span class="n">activation_noise</span><span class="p">)</span>
        <span class="c1"># Never choose padded values.</span>
        <span class="n">p_choose_i</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">merged_source_padding</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span>
                              <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p_choose_i</span><span class="p">),</span> <span class="n">p_choose_i</span><span class="p">)</span>
        <span class="c1"># Compute attention distribution</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">seq2seq</span><span class="o">.</span><span class="n">monotonic_attention</span><span class="p">(</span>
            <span class="n">p_choose_i</span><span class="p">,</span> <span class="n">previous_attention</span><span class="p">,</span> <span class="s1">&#39;parallel&#39;</span><span class="p">)</span>

    <span class="c1"># [tb, sl].</span>
    <span class="k">return</span> <span class="n">probs</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
        <span class="n">emit_probs</span><span class="o">=</span><span class="n">probs</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">attention_state</span><span class="o">.</span><span class="n">random_seed</span><span class="p">)</span></div>

<div class="viewcode-block" id="MonotonicAttention.ComputeContextVectorWithSource"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.MonotonicAttention.ComputeContextVectorWithSource">[docs]</a>  <span class="k">def</span> <span class="nf">ComputeContextVectorWithSource</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                     <span class="n">theta</span><span class="p">,</span>
                                     <span class="n">concated_source_vecs</span><span class="p">,</span>
                                     <span class="n">concated_source_contexts</span><span class="p">,</span>
                                     <span class="n">source_padding</span><span class="p">,</span>
                                     <span class="n">source_segment_id</span><span class="p">,</span>
                                     <span class="n">query_vec</span><span class="p">,</span>
                                     <span class="n">attention_state</span><span class="p">,</span>
                                     <span class="n">per_step_source_padding</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                     <span class="n">step_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                     <span class="n">query_segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the context vector given the current query output.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this</span>
<span class="sd">        layer and its children layers.</span>
<span class="sd">      concated_source_vecs: Concated source vectors with shape [time,</span>
<span class="sd">        batch_size, hidden_dim].</span>
<span class="sd">      concated_source_contexts: Concated source contexts with shape [</span>
<span class="sd">        batch_size, time, context_dim].</span>
<span class="sd">      source_padding: Source padding with shape [time, batch_size].</span>
<span class="sd">      source_segment_id: Source segment id with shape [time, batch_size].</span>
<span class="sd">      query_vec: a tensor of shape [batch_size, query_dim].</span>
<span class="sd">      attention_state: The attention probs computed at the previous timestep.</span>
<span class="sd">      per_step_source_padding: Source sequence padding to apply at this step.</span>
<span class="sd">        If not None, it should be of shape [target_batch_size,</span>
<span class="sd">        source_seq_length].</span>
<span class="sd">      step_state: A NestedMap containing &#39;global_step&#39; and &#39;time_step&#39;.</span>
<span class="sd">        Required for deterministic dropout.</span>
<span class="sd">      query_segment_id: a tensor of shape [batch_size].</span>

<span class="sd">    Note: concated_source_vecs are the vectors that are used to compute the</span>
<span class="sd">    attention score between the query_vec and each concated_source_vec.</span>
<span class="sd">    The concated_source_contexts are the vectors that compose the result.</span>
<span class="sd">    The attention context vector is computed as a weighted average of the</span>
<span class="sd">    concated_source_contexts, using the scores that were computed using</span>
<span class="sd">    concated_source_vecs.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tuple of 3 elements.</span>
<span class="sd">        The attention context vector:</span>
<span class="sd">          [batch_size, context_dim]</span>
<span class="sd">        The attention probability vector:</span>
<span class="sd">          [batch_size, time]</span>
<span class="sd">        The attention probability vector:</span>
<span class="sd">          (again, to be interpreted as state).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">del</span> <span class="n">source_segment_id</span>
    <span class="k">del</span> <span class="n">query_segment_id</span>
    <span class="n">sb</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">tb</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">multiplier</span> <span class="o">=</span> <span class="n">tb</span> <span class="o">//</span> <span class="n">sb</span>
    <span class="n">merged_source_padding</span> <span class="o">=</span> <span class="n">MergeSourcePaddingWithPerStepSourcePadding</span><span class="p">(</span>
        <span class="n">source_padding</span><span class="p">,</span> <span class="n">per_step_source_padding</span><span class="p">,</span> <span class="n">tb</span><span class="p">)</span>

    <span class="n">probs</span><span class="p">,</span> <span class="n">new_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ComputeProbabilities</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">concated_source_vecs</span><span class="p">,</span>
                                                 <span class="n">merged_source_padding</span><span class="p">,</span>
                                                 <span class="n">query_vec</span><span class="p">,</span> <span class="n">attention_state</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;sum&#39;</span><span class="p">):</span>
      <span class="c1"># Reshape probs to be of shape [tb/sb, sb, sl]</span>
      <span class="n">probs_reshaped</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="p">[</span><span class="n">multiplier</span><span class="p">,</span> <span class="n">sb</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="c1"># Transpose probs to be of shape [sb, tb/sb, sl]</span>
      <span class="n">probs_reshaped</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">probs_reshaped</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
      <span class="c1"># Batched matmul</span>
      <span class="c1"># [sb, tb/sb, sl] * [sb, sl, context_dim] = [sb, tb/sb, context_dim]</span>
      <span class="n">summed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">probs_reshaped</span><span class="p">,</span> <span class="n">concated_source_contexts</span><span class="p">)</span>
      <span class="c1"># summed is of shape [tb/sb, sb, context_dim]</span>
      <span class="n">summed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">summed</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
      <span class="n">ctx_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">summed</span><span class="p">,</span> <span class="p">[</span><span class="n">tb</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">ctx_vec</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">new_state</span></div>

<div class="viewcode-block" id="MonotonicAttention.PostTrainingStepUpdate"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.MonotonicAttention.PostTrainingStepUpdate">[docs]</a>  <span class="k">def</span> <span class="nf">PostTrainingStepUpdate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">global_step</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Update self._step_counter with the global_step value.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_step_counter</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">global_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_step_counter</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span></div></div>


<div class="viewcode-block" id="GmmMonotonicAttention"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.GmmMonotonicAttention">[docs]</a><span class="k">class</span> <span class="nc">GmmMonotonicAttention</span><span class="p">(</span><span class="n">BaseAttentionLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A GMM-based monotonic attention module.</span>

<span class="sd">  Based on &quot;Generating Sequences With Recurrent Neural Networks&quot; by Alex Graves.</span>
<span class="sd">  Eq [46-51] in https://arxiv.org/abs/1308.0850.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="GmmMonotonicAttention.Params"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.GmmMonotonicAttention.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Params for this MonotonicAttention class.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">GmmMonotonicAttention</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;source_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of source nodes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;query_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of query nodes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;gmm_mlp_hidden_dim&#39;</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span>
             <span class="s1">&#39;Number of hidden units for the MLP that predicts GMM params.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;max_offset&#39;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
             <span class="s1">&#39;Max offset to move attention pointer, Enabled only when &gt; 0.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_mixtures&#39;</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="s1">&#39;Number of location GMM components.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="nd">@base_layer</span><span class="o">.</span><span class="n">initializer</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a GMM-based monotonic attention module.&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">GmmMonotonicAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">atten_dropout_prob</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s1">&#39;dropout is not supported.&#39;</span><span class="p">)</span>

    <span class="c1"># TODO(ngyuzh): find a good initialize for both TTS and ASR.</span>
    <span class="c1"># Consider split the layer if it&#39;s very sensitive to the initialization</span>
    <span class="c1"># Compare Sigmoid and other activation functions.</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="n">gmm_params</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">FeedForwardNet</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
          <span class="n">name</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
          <span class="n">input_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">query_dim</span><span class="p">,</span>
          <span class="n">hidden_layer_dims</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">gmm_mlp_hidden_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_mixtures</span> <span class="o">*</span> <span class="mi">3</span><span class="p">],</span>
          <span class="n">activation</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;SIGMOID&#39;</span><span class="p">,</span> <span class="s1">&#39;NONE&#39;</span><span class="p">],</span>
          <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Xavier</span><span class="p">(</span><span class="mf">0.1</span><span class="p">))</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;GMM&#39;</span><span class="p">,</span> <span class="n">gmm_params</span><span class="p">)</span>

      <span class="c1"># TODO(ngyuzh): change variance to scale to make it simpler.</span>
      <span class="c1"># noinline and compiled cannot be set at the same time</span>
      <span class="nd">@function</span><span class="o">.</span><span class="n">Defun</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="n">noinline</span><span class="o">=</span><span class="ow">not</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">use_tpu</span><span class="p">())</span>
      <span class="k">def</span> <span class="nf">EvalGmmPdfs</span><span class="p">(</span><span class="n">encoder_positions</span><span class="p">,</span> <span class="n">priors</span><span class="p">,</span> <span class="n">means</span><span class="p">,</span> <span class="n">variances</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Evaluate the location GMMs on all encoder positions.&quot;&quot;&quot;</span>
        <span class="c1"># encoder_positions: [batch, 1, timesteps, 1]</span>
        <span class="c1"># [batch, tb / sb, 1, num_mixtures]</span>
        <span class="n">priors</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">priors</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">means</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">means</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">variances</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">variances</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="c1"># [batch, tb / sb, timesteps, num_mixtures]</span>
        <span class="n">pdfs</span> <span class="o">=</span> <span class="p">((</span><span class="n">priors</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">variances</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">))</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
            <span class="o">-</span><span class="p">(</span><span class="n">encoder_positions</span> <span class="o">-</span> <span class="n">means</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">variances</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)))</span>
        <span class="c1"># pdfs sized [batch, tb / sb, timesteps].</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">pdfs</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

      <span class="c1"># TODO(ngyuzh): remove unnecessary transpose.</span>
      <span class="k">def</span> <span class="nf">Atten</span><span class="p">(</span><span class="n">source_padding</span><span class="p">,</span> <span class="n">concated_source_vecs</span><span class="p">,</span> <span class="n">concated_source_contexts</span><span class="p">,</span>
                <span class="n">query_vec</span><span class="p">,</span> <span class="n">priors</span><span class="p">,</span> <span class="n">means</span><span class="p">,</span> <span class="n">variances</span><span class="p">,</span> <span class="n">encoder_positions</span><span class="p">,</span>
                <span class="n">per_step_source_padding</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Computes the attention context vector.&quot;&quot;&quot;</span>
        <span class="c1"># tb: target batch size</span>
        <span class="c1"># sb: source batch size</span>
        <span class="c1"># concated_source_vecs is of shape [sl, sb, context_dim]</span>
        <span class="c1"># query_vec is of shape [tb, dims]</span>
        <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
        <span class="n">sb</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">tb</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">multiplier</span> <span class="o">=</span> <span class="n">tb</span> <span class="o">//</span> <span class="n">sb</span>
        <span class="c1"># [sb, tb / sb, num_mixtures]</span>
        <span class="n">priors</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">priors</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">multiplier</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_mixtures</span><span class="p">])</span>
        <span class="n">means</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">means</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">multiplier</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_mixtures</span><span class="p">])</span>
        <span class="n">variances</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">variances</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">multiplier</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_mixtures</span><span class="p">])</span>

        <span class="n">probs</span> <span class="o">=</span> <span class="n">EvalGmmPdfs</span><span class="p">(</span><span class="n">encoder_positions</span><span class="p">,</span> <span class="n">priors</span><span class="p">,</span> <span class="n">means</span><span class="p">,</span> <span class="n">variances</span><span class="p">)</span>
        <span class="c1"># [sl, tb / sb, sb]</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">multiplier</span><span class="p">,</span> <span class="n">sb</span><span class="p">])</span>

        <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">source_padding</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">per_step_source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">per_step_source_padding</span><span class="p">),</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">multiplier</span><span class="p">,</span> <span class="n">sb</span><span class="p">])</span>
        <span class="n">source_padding</span> <span class="o">+=</span> <span class="n">per_step_source_padding</span>
        <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">source_padding</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>

        <span class="n">probs</span> <span class="o">*=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">source_padding</span><span class="p">)</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">AddDebugTensor</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;atten_probs&#39;</span><span class="p">)</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">tb</span><span class="p">]))</span>
        <span class="c1"># [tb/sb, sb, sl]</span>
        <span class="n">probs_reshaped</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="p">[</span><span class="n">multiplier</span><span class="p">,</span> <span class="n">sb</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="c1"># [sb, tb/sb, sl]</span>
        <span class="n">probs_reshaped</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">probs_reshaped</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
        <span class="c1"># Batched matmul</span>
        <span class="c1"># [sb, tb/sb, sl] * [sb, sl, context_dim] = [sb, tb/sb, context_dim]</span>
        <span class="n">context_vector</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">probs_reshaped</span><span class="p">,</span> <span class="n">concated_source_contexts</span><span class="p">)</span>
        <span class="n">context_vector</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">context_vector</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">context_vector</span><span class="p">,</span> <span class="p">[</span><span class="n">tb</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">probs</span>

      <span class="bp">self</span><span class="o">.</span><span class="n">_ctx_vec</span> <span class="o">=</span> <span class="n">Atten</span>

      <span class="k">def</span> <span class="nf">EncodeSource</span><span class="p">(</span><span class="n">vecs</span><span class="p">,</span> <span class="n">ctxs</span><span class="p">):</span>
        <span class="c1"># TODO(ngyuzh): combine with content-base attention.</span>
        <span class="n">time</span><span class="p">,</span> <span class="n">batch</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">vecs</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">ctxs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">ctxs</span><span class="p">,</span> <span class="p">[</span><span class="n">time</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">transposed_ctxs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">ctxs</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">vecs</span><span class="p">,</span> <span class="n">transposed_ctxs</span>

      <span class="bp">self</span><span class="o">.</span><span class="n">_encode_source</span> <span class="o">=</span> <span class="n">EncodeSource</span>

<div class="viewcode-block" id="GmmMonotonicAttention.InitForSourcePacked"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.GmmMonotonicAttention.InitForSourcePacked">[docs]</a>  <span class="k">def</span> <span class="nf">InitForSourcePacked</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                          <span class="n">theta</span><span class="p">,</span>
                          <span class="n">source_vecs</span><span class="p">,</span>
                          <span class="n">source_contexts</span><span class="p">,</span>
                          <span class="n">source_padding</span><span class="p">,</span>
                          <span class="n">source_segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initialize attention for the given source vectors.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this</span>
<span class="sd">        layer and its children layers.</span>
<span class="sd">      source_vecs: A single tensor of shape [time, batch_size, source_dim].</span>
<span class="sd">      source_contexts: A single tensor of shape [time, batch_size, some_dim].</span>
<span class="sd">      source_padding: A tensor of shape [time, batch_size].</span>
<span class="sd">      source_segment_id: A tensor of shape [time, batch_size].</span>

<span class="sd">    Returns:</span>
<span class="sd">      Concated source vectors, concated source contexts, source paddings</span>
<span class="sd">      and source_segment_id.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_source_init_done</span> <span class="o">=</span> <span class="kc">True</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_source_padding</span> <span class="o">=</span> <span class="n">source_padding</span>
      <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_concated_source_vecs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_concated_source_contexts</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_encode_source</span><span class="p">(</span><span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">))</span>
      <span class="k">if</span> <span class="n">source_segment_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">source_segment_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">source_padding</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_source_segment_id</span> <span class="o">=</span> <span class="n">source_segment_id</span>
    <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_concated_source_vecs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_concated_source_contexts</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_source_padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_source_segment_id</span><span class="p">)</span></div>

<div class="viewcode-block" id="GmmMonotonicAttention.ZeroAttentionState"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.GmmMonotonicAttention.ZeroAttentionState">[docs]</a>  <span class="k">def</span> <span class="nf">ZeroAttentionState</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source_seq_length</span><span class="p">,</span> <span class="n">decoder_batch_size</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">position</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">decoder_batch_size</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_mixtures</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">position_offsets</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
        <span class="p">[</span><span class="n">decoder_batch_size</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_mixtures</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">variances</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">decoder_batch_size</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_mixtures</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">priors</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">decoder_batch_size</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_mixtures</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">atten_states</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span>
        <span class="p">[</span><span class="n">position</span><span class="p">,</span> <span class="n">position_offsets</span><span class="p">,</span> <span class="n">variances</span><span class="p">,</span> <span class="n">priors</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">atten_states</span></div>

<div class="viewcode-block" id="GmmMonotonicAttention.ComputeContextVectorWithSource"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.GmmMonotonicAttention.ComputeContextVectorWithSource">[docs]</a>  <span class="k">def</span> <span class="nf">ComputeContextVectorWithSource</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                     <span class="n">theta</span><span class="p">,</span>
                                     <span class="n">concated_source_vecs</span><span class="p">,</span>
                                     <span class="n">concated_source_contexts</span><span class="p">,</span>
                                     <span class="n">source_padding</span><span class="p">,</span>
                                     <span class="n">source_segment_id</span><span class="p">,</span>
                                     <span class="n">query_vec</span><span class="p">,</span>
                                     <span class="n">attention_state</span><span class="p">,</span>
                                     <span class="n">per_step_source_padding</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                     <span class="n">step_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                     <span class="n">query_segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the context vector given the current query output.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this</span>
<span class="sd">        layer and its children layers.</span>
<span class="sd">      concated_source_vecs: Concated source vectors with shape [time,</span>
<span class="sd">        batch_size, hidden_dim].</span>
<span class="sd">      concated_source_contexts: Concated source contexts with shape [</span>
<span class="sd">        batch_size, time, context_dim].</span>
<span class="sd">      source_padding: Source padding with shape [time, batch_size].</span>
<span class="sd">      source_segment_id: Tensor of source segment ids, with shape [time,</span>
<span class="sd">        batch_size].</span>
<span class="sd">      query_vec: a tensor of shape [batch_size, query_dim].</span>
<span class="sd">      attention_state: previous attention state, a tensor of shape</span>
<span class="sd">        [batch_size, num_mixtures, 4].</span>

<span class="sd">        - attention_state[:, :, 0] contains previous location</span>
<span class="sd">        - attention_state[:, :, 1] contains previous offset.</span>
<span class="sd">        - attention_state[:, :, 2] contains previous variance.</span>
<span class="sd">        - attention_state[:, :, 3] contains previous prior.</span>
<span class="sd">      per_step_source_padding: Source sequence padding to apply at this step.</span>
<span class="sd">        If not None, it should be of shape [target_batch_size,</span>
<span class="sd">        source_seq_length].</span>
<span class="sd">      step_state: A NestedMap containing &#39;global_step&#39; and &#39;time_step&#39;.</span>
<span class="sd">        Required for deterministic dropout.</span>
<span class="sd">      query_segment_id: a tensor of shape [batch_size]</span>

<span class="sd">    Note: concated_source_vecs are the vectors that are used to compute the</span>
<span class="sd">    attention score between the query_vec and each concated_source_vec.</span>
<span class="sd">    The concated_source_contexts are the vectors that compose the result.</span>
<span class="sd">    The attention context vector is computed as a weighted average of the</span>
<span class="sd">    concated_source_contexts, using the scores that were computed using</span>
<span class="sd">    concated_source_vecs.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tuple of 3 elements.</span>
<span class="sd">        The attention context vector:</span>
<span class="sd">          [batch_size, context_dim]</span>
<span class="sd">        The attention probability vector:</span>
<span class="sd">          [batch_size, time]</span>
<span class="sd">        The new attention state vector:</span>
<span class="sd">          possibly nested tuple of tensors with dimensions [target_batch, ...]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">del</span> <span class="n">source_segment_id</span>
    <span class="k">del</span> <span class="n">query_segment_id</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">query_batch_size</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">source_seq_length</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">source_padding</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">per_step_source_padding</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">zero</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">query_vec</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="n">per_step_source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">fill</span><span class="p">([</span><span class="n">query_batch_size</span><span class="p">,</span> <span class="n">source_seq_length</span><span class="p">],</span>
                                        <span class="n">zero</span><span class="p">)</span>
    <span class="n">per_step_source_padding</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span>
        <span class="n">per_step_source_padding</span><span class="p">,</span> <span class="p">[</span><span class="n">query_batch_size</span><span class="p">,</span> <span class="n">source_seq_length</span><span class="p">])</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">GMM</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">GMM</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">)</span>
    <span class="n">priors_logits</span><span class="p">,</span> <span class="n">position_offset_logits</span><span class="p">,</span> <span class="n">log_variances</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
        <span class="n">out</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;GMM&#39;</span><span class="p">)</span>
    <span class="n">log_variances</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">log_variances</span><span class="p">,</span> <span class="n">layers</span><span class="o">.</span><span class="n">LOG_SCALE_CLAMP_BOUND</span><span class="p">)</span>
    <span class="n">variances</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_variances</span><span class="p">)</span>
    <span class="n">priors</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">priors_logits</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">max_offset</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
      <span class="n">position_offset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">position_offset_logits</span><span class="p">)</span>
      <span class="n">position_offset</span> <span class="o">*=</span> <span class="n">p</span><span class="o">.</span><span class="n">max_offset</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">position_offset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">position_offset_logits</span><span class="p">)</span>
    <span class="n">new_position</span> <span class="o">=</span> <span class="n">attention_state</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">position_offset</span>
    <span class="n">new_position</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">new_position</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">source_seq_length</span><span class="p">))</span>
    <span class="n">variances</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">AddDebugTensor</span><span class="p">(</span><span class="n">variances</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;variances&#39;</span><span class="p">)</span>
    <span class="n">priors</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">AddDebugTensor</span><span class="p">(</span><span class="n">priors</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;priors&#39;</span><span class="p">)</span>
    <span class="c1"># Tile and reshape encoder_positions to [batch, 1, timesteps, 1] so that</span>
    <span class="c1"># it can be evaluated by locations GMMs in a vectorized way.</span>
    <span class="n">source_batch_size</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">source_padding</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">encoder_positions</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">source_seq_length</span><span class="p">)),</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">encoder_positions</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">encoder_positions</span><span class="p">,</span> <span class="p">(</span><span class="n">source_batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="c1"># [batch, timesteps, 1].</span>
    <span class="n">encoder_positions</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">encoder_positions</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">encoder_positions</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">encoder_positions</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">ctx_vec</span><span class="p">,</span> <span class="n">prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ctx_vec</span><span class="p">(</span><span class="n">source_padding</span><span class="p">,</span> <span class="n">concated_source_vecs</span><span class="p">,</span>
                                  <span class="n">concated_source_contexts</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">priors</span><span class="p">,</span>
                                  <span class="n">new_position</span><span class="p">,</span> <span class="n">variances</span><span class="p">,</span> <span class="n">encoder_positions</span><span class="p">,</span>
                                  <span class="n">per_step_source_padding</span><span class="p">)</span>
    <span class="n">new_atten_states</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span>
        <span class="p">[</span><span class="n">new_position</span><span class="p">,</span> <span class="n">position_offset</span><span class="p">,</span> <span class="n">variances</span><span class="p">,</span> <span class="n">priors</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ctx_vec</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">new_atten_states</span></div></div>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../../',
            VERSION:'',
            LANGUAGE:'en',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>