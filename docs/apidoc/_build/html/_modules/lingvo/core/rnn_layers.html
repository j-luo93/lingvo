

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>lingvo.core.rnn_layers &mdash; lingvo  documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 

  
  <script src="../../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../../index.html" class="icon icon-home"> lingvo
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../lingvo.html">lingvo package</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">lingvo</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../index.html">Module code</a> &raquo;</li>
        
      <li>lingvo.core.rnn_layers</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for lingvo.core.rnn_layers</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2018 The TensorFlow Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="sd">&quot;&quot;&quot;Lingvo RNN layers.&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">from</span> <span class="nn">six.moves</span> <span class="k">import</span> <span class="nb">range</span>
<span class="kn">from</span> <span class="nn">six.moves</span> <span class="k">import</span> <span class="nb">zip</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="kn">from</span> <span class="nn">tensorflow.contrib.cudnn_rnn.python.ops</span> <span class="k">import</span> <span class="n">cudnn_rnn_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">function</span>

<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="k">import</span> <span class="n">attention</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="k">import</span> <span class="n">base_layer</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="k">import</span> <span class="n">cluster_factory</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="k">import</span> <span class="n">cudnn_rnn_utils</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="k">import</span> <span class="n">hyperparams</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="k">import</span> <span class="n">layers</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="k">import</span> <span class="n">layers_with_attention</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="k">import</span> <span class="n">py_utils</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="k">import</span> <span class="n">recurrent</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="k">import</span> <span class="n">rnn_cell</span>

<span class="n">assert_shape_match</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">assert_shape_match</span>


<div class="viewcode-block" id="_AssertCellParamsCuDNNCompatible"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers._AssertCellParamsCuDNNCompatible">[docs]</a><span class="k">def</span> <span class="nf">_AssertCellParamsCuDNNCompatible</span><span class="p">(</span><span class="n">p_cell</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">p_cell</span><span class="o">.</span><span class="n">cls</span> <span class="o">==</span> <span class="n">rnn_cell</span><span class="o">.</span><span class="n">LSTMCellCuDNNCompliant</span><span class="p">:</span>
    <span class="k">return</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">assert</span> <span class="n">p_cell</span><span class="o">.</span><span class="n">cls</span> <span class="o">==</span> <span class="n">rnn_cell</span><span class="o">.</span><span class="n">LSTMCellSimple</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">p_cell</span><span class="p">,</span> <span class="n">hyperparams</span><span class="o">.</span><span class="n">Params</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">p_cell</span><span class="o">.</span><span class="n">cell_value_cap</span> <span class="ow">is</span> <span class="kc">None</span>
    <span class="k">assert</span> <span class="n">p_cell</span><span class="o">.</span><span class="n">forget_gate_bias</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="n">p_cell</span><span class="o">.</span><span class="n">output_nonlinearity</span>
    <span class="k">assert</span> <span class="n">p_cell</span><span class="o">.</span><span class="n">zo_prob</span> <span class="o">==</span> <span class="mf">0.0</span></div>


<div class="viewcode-block" id="_ReversePaddedSequence"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers._ReversePaddedSequence">[docs]</a><span class="k">def</span> <span class="nf">_ReversePaddedSequence</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Reverse inputs based on paddings.</span>

<span class="sd">  Only reverse the unpadded portion of \&#39;inputs\&#39;. It assumes inputs are only</span>
<span class="sd">  padded in the end.</span>

<span class="sd">  Args:</span>
<span class="sd">    inputs: a tensor of [seq_length, batch_size, num_input_nodes].</span>
<span class="sd">    paddings: a tensor of float32/float64 zero or one of shape</span>
<span class="sd">      [seq_length, batch_size, 1].</span>
<span class="sd">  Returns:</span>
<span class="sd">    A reversed tensor of the same shape as \&#39;inputs\&#39;.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">inversed_paddings</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
  <span class="n">inputs_length</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">rint</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">inversed_paddings</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reverse_sequence</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">inputs_length</span><span class="p">,</span> <span class="n">seq_axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">batch_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></div>


<div class="viewcode-block" id="_GeneratePackedInputResetMask"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers._GeneratePackedInputResetMask">[docs]</a><span class="k">def</span> <span class="nf">_GeneratePackedInputResetMask</span><span class="p">(</span><span class="n">segment_id</span><span class="p">,</span> <span class="n">is_reverse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Generates mask inputs for RNN cells from segment_id.</span>

<span class="sd">    Args:</span>
<span class="sd">      segment_id: A tensor of shape [time, batch_size, 1]</span>
<span class="sd">      is_reverse: True if inputs are fed to the RNN in reverse order.</span>
<span class="sd">    Returns:</span>
<span class="sd">      reset_mask - a tensor of shape [time, batch_size, 1]. Set to 0 for samples</span>
<span class="sd">      where state needs to be reset (at example boundaries), and 1 otherwise.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">segment_id_left</span> <span class="o">=</span> <span class="n">segment_id</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
  <span class="n">segment_id_right</span> <span class="o">=</span> <span class="n">segment_id</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>

  <span class="c1"># Mask is a [t-1, bs, 1] tensor.</span>
  <span class="n">reset_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">segment_id_left</span><span class="p">,</span> <span class="n">segment_id_right</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">segment_id</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
  <span class="n">mask_padding_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
      <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
       <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">segment_id</span><span class="p">)[</span><span class="mi">1</span><span class="p">:]],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">mask_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">mask_padding_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">segment_id</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">is_reverse</span><span class="p">:</span>
    <span class="n">reset_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">reset_mask</span><span class="p">,</span> <span class="n">mask_padding</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">reset_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">mask_padding</span><span class="p">,</span> <span class="n">reset_mask</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">reset_mask</span></div>


<div class="viewcode-block" id="RNN"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.RNN">[docs]</a><span class="k">class</span> <span class="nc">RNN</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">LayerBase</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Statically unrolled RNN.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="RNN.Params"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.RNN.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">RNN</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;cell&#39;</span><span class="p">,</span> <span class="n">rnn_cell</span><span class="o">.</span><span class="n">LSTMCellSimple</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
             <span class="s1">&#39;Configs for the RNN cell.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;sequence_length&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Sequence length.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;reverse&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
             <span class="s1">&#39;Whether or not to unroll the sequence in reversed order.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;packed_input&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;To reset states for packed inputs.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="nd">@base_layer</span><span class="o">.</span><span class="n">initializer</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">RNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;Packed inputs are currently not &#39;</span>
                                     <span class="s1">&#39;supported by Static RNN&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">cell</span><span class="o">.</span><span class="n">reset_cell_state</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;cell&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">cell</span><span class="p">)</span>

<div class="viewcode-block" id="RNN.FProp"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.RNN.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">state0</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute RNN forward pass.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this</span>
<span class="sd">        layer and its children layers.</span>
<span class="sd">      inputs: A single tensor or a tuple of tensors with cardinality equal to</span>
<span class="sd">          rnn_cell.inputs_arity. For every input tensor, the first dimension is</span>
<span class="sd">          assumed to be time, second dimension batch, and third dimension depth.</span>
<span class="sd">      paddings: A tensor. First dim is time, second dim is batch, and third dim</span>
<span class="sd">          is expected to be 1.</span>
<span class="sd">      state0: If not None, the initial rnn state in a NestedMap. Defaults</span>
<span class="sd">        to the cell&#39;s zero-state.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tensor of [time, batch, dims].</span>
<span class="sd">      The final recurrent state.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">rcell</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cell</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">rcell</span><span class="p">,</span> <span class="p">(</span><span class="n">rnn_cell</span><span class="o">.</span><span class="n">RNNCell</span><span class="p">))</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="n">inputs_sequence</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">unstack</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">sequence_length</span><span class="p">)</span>
      <span class="n">paddings_sequence</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">unstack</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">sequence_length</span><span class="p">)</span>
      <span class="c1"># We start from all 0 states.</span>
      <span class="k">if</span> <span class="n">state0</span><span class="p">:</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">state0</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">inputs0</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
            <span class="n">act</span><span class="o">=</span><span class="p">[</span><span class="n">inputs_sequence</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="n">padding</span><span class="o">=</span><span class="n">paddings_sequence</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">rcell</span><span class="o">.</span><span class="n">zero_state</span><span class="p">(</span><span class="n">rcell</span><span class="o">.</span><span class="n">batch_size</span><span class="p">(</span><span class="n">inputs0</span><span class="p">))</span>
      <span class="n">outputs</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">sequence_length</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">reverse</span><span class="p">:</span>
        <span class="n">sequence</span> <span class="o">=</span> <span class="n">xrange</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">sequence_length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">sequence</span> <span class="o">=</span> <span class="n">xrange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">sequence_length</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
      <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">sequence</span><span class="p">:</span>
        <span class="n">cur_input</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">act</span><span class="o">=</span><span class="p">[</span><span class="n">inputs</span><span class="p">[</span><span class="n">idx</span><span class="p">]],</span> <span class="n">padding</span><span class="o">=</span><span class="n">paddings</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
        <span class="n">state</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">rcell</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">cell</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">cur_input</span><span class="p">)</span>
        <span class="n">outputs</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">rcell</span><span class="o">.</span><span class="n">GetOutput</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">outputs</span><span class="p">),</span> <span class="n">state</span></div></div>


<div class="viewcode-block" id="StackedRNNBase"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.StackedRNNBase">[docs]</a><span class="k">class</span> <span class="nc">StackedRNNBase</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">LayerBase</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Stacked RNN base class.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="StackedRNNBase.Params"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.StackedRNNBase.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">StackedRNNBase</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_layers&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;The number of RNN layers.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;skip_start&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;The first layer start skip connection.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;cell_tpl&#39;</span><span class="p">,</span> <span class="n">rnn_cell</span><span class="o">.</span><span class="n">LSTMCellSimple</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
        <span class="s1">&#39;Configs for the RNN cell(s). &#39;</span>
        <span class="s1">&#39;If cell_tpl is not a list/tuple, the same cell config is used &#39;</span>
        <span class="s1">&#39;for all layers. Otherwise, cell_tpl[i] is the config for &#39;</span>
        <span class="s1">&#39;i-th layer and cell_tpl[-1] is used for the rest of layers.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;dropout&#39;</span><span class="p">,</span> <span class="n">layers</span><span class="o">.</span><span class="n">DropoutLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
             <span class="s1">&#39;Dropout applied to each layer.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;packed_input&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;To reset states for packed inputs.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="nd">@base_layer</span><span class="o">.</span><span class="n">initializer</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">StackedRNNBase</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;Packed inputs are currently not &#39;</span>
                                     <span class="s1">&#39;supported by Static RNN Base&#39;</span><span class="p">)</span>

<div class="viewcode-block" id="StackedRNNBase._GetCellTpls"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.StackedRNNBase._GetCellTpls">[docs]</a>  <span class="k">def</span> <span class="nf">_GetCellTpls</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">cell_tpl</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
      <span class="n">cell_tpls</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">cell_tpl</span><span class="p">]</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">num_layers</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">cell_tpls</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">cell_tpl</span><span class="p">)</span>
      <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">cell_tpl</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_layers</span>
      <span class="n">last</span> <span class="o">=</span> <span class="n">cell_tpls</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
      <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">cell_tpls</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">p</span><span class="o">.</span><span class="n">num_layers</span><span class="p">:</span>
        <span class="n">cell_tpls</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">last</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">cell_tpl</span> <span class="ow">in</span> <span class="n">cell_tpls</span><span class="p">:</span>
      <span class="n">cell_tpl</span><span class="o">.</span><span class="n">reset_cell_state</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
      <span class="c1"># Because one layer&#39;s output needs to be fed into the next layer&#39;s</span>
      <span class="c1"># input, hence, we have this assertion. We can relax it later by</span>
      <span class="c1"># allowing more parameterization of the layers.</span>
      <span class="k">assert</span> <span class="n">cell_tpls</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">num_output_nodes</span> <span class="o">==</span> <span class="n">cell_tpls</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">num_input_nodes</span>
    <span class="k">return</span> <span class="n">cell_tpls</span></div></div>


<div class="viewcode-block" id="StackedFRNNLayerByLayer"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.StackedFRNNLayerByLayer">[docs]</a><span class="k">class</span> <span class="nc">StackedFRNNLayerByLayer</span><span class="p">(</span><span class="n">StackedRNNBase</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;An implemention of StackedRNNBase which computes layer-by-layer.&quot;&quot;&quot;</span>

  <span class="nd">@base_layer</span><span class="o">.</span><span class="n">initializer</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">StackedFRNNLayerByLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="n">rnn_params</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">cell_tpl</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_GetCellTpls</span><span class="p">()):</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">FRNN</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
        <span class="n">params</span><span class="o">.</span><span class="n">packed_input</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span>
        <span class="n">params</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;frnn_</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i</span>
        <span class="n">params</span><span class="o">.</span><span class="n">cell</span> <span class="o">=</span> <span class="n">cell_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
        <span class="n">params</span><span class="o">.</span><span class="n">cell</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">%s</span><span class="s1">_</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
        <span class="n">rnn_params</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChildren</span><span class="p">(</span><span class="s1">&#39;rnn&#39;</span><span class="p">,</span> <span class="n">rnn_params</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;dropout&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">dropout</span><span class="p">)</span>

<div class="viewcode-block" id="StackedFRNNLayerByLayer.zero_state"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.StackedFRNNLayerByLayer.zero_state">[docs]</a>  <span class="k">def</span> <span class="nf">zero_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">rnn</span><span class="o">=</span><span class="p">[])</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
      <span class="n">state0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">zero_state</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
      <span class="n">ret</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">state0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span></div>

<div class="viewcode-block" id="StackedFRNNLayerByLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.StackedFRNNLayerByLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">state0</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute RNN forward pass.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this</span>
<span class="sd">        layer and its children layers.</span>
<span class="sd">      inputs: A single tensor of shape [time, batch, dims].</span>
<span class="sd">      paddings: A single tensor of shape [time, batch, 1].</span>
<span class="sd">      state0: If not None, the initial rnn state in a NestedMap. Defaults</span>
<span class="sd">        to the init state.</span>

<span class="sd">    Returns:</span>
<span class="sd">      (outputs, state1)</span>
<span class="sd">      outputs: A tensor of [time, batch, dims].</span>
<span class="sd">      state1: The final state.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">state0</span><span class="p">:</span>
      <span class="n">state0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">zero_state</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">xs</span> <span class="o">=</span> <span class="n">inputs</span>
    <span class="n">state1</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">rnn</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">num_layers</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
      <span class="n">ys</span><span class="p">,</span> <span class="n">state1</span><span class="o">.</span><span class="n">rnn</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">rnn</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">xs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span>
                                            <span class="n">state0</span><span class="o">.</span><span class="n">rnn</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
      <span class="n">ys</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">ys</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="n">p</span><span class="o">.</span><span class="n">skip_start</span><span class="p">:</span>
        <span class="n">ys</span> <span class="o">+=</span> <span class="n">xs</span>
      <span class="n">xs</span> <span class="o">=</span> <span class="n">ys</span>
    <span class="k">return</span> <span class="n">xs</span><span class="p">,</span> <span class="n">state1</span></div></div>


<div class="viewcode-block" id="FRNN"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.FRNN">[docs]</a><span class="k">class</span> <span class="nc">FRNN</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">LayerBase</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Functional while based RNN.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="FRNN.Params"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.FRNN.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">FRNN</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;cell&#39;</span><span class="p">,</span> <span class="n">rnn_cell</span><span class="o">.</span><span class="n">LSTMCellSimple</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
             <span class="s1">&#39;Configs for the RNN cell.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;reverse&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
             <span class="s1">&#39;Whether or not to unroll the sequence in reversed order.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;packed_input&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;To reset states for packed inputs.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="nd">@base_layer</span><span class="o">.</span><span class="n">initializer</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">FRNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">p</span><span class="o">.</span><span class="n">cell</span><span class="o">.</span><span class="n">reset_cell_state</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;cell&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">cell</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">rnn_cell</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">cell</span>

<div class="viewcode-block" id="FRNN.zero_state"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.FRNN.zero_state">[docs]</a>  <span class="k">def</span> <span class="nf">zero_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">cell</span><span class="o">.</span><span class="n">zero_state</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span></div>

<div class="viewcode-block" id="FRNN.FProp"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.FRNN.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">state0</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute RNN forward pass.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this</span>
<span class="sd">        layer and its children layers.</span>
<span class="sd">      inputs: A single tensor or a tuple of tensors with cardinality equal to</span>
<span class="sd">          rnn_cell.inputs_arity. For every input tensor, the first dimension is</span>
<span class="sd">          assumed to be time, second dimension batch, and third dimension depth.</span>
<span class="sd">      paddings: A tensor. First dim is time, second dim is batch, and third dim</span>
<span class="sd">          is expected to be 1.</span>
<span class="sd">      state0: If not None, the initial rnn state in a NestedMap. Defaults</span>
<span class="sd">        to the cell&#39;s zero-state.</span>
<span class="sd">      segment_id: A tensor to support packed inputs. First dim is time, second</span>
<span class="sd">          dim is batch, and third dim is expected to be 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tensor of [time, batch, dims].</span>
<span class="sd">      The final recurrent state.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">rcell</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cell</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">rcell</span><span class="p">,</span> <span class="p">(</span><span class="n">rnn_cell</span><span class="o">.</span><span class="n">RNNCell</span><span class="p">))</span>

    <span class="nd">@function</span><span class="o">.</span><span class="n">Defun</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">FlipUpDown</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
      <span class="c1"># Reverse the first dimension (time)</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reverse</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">inputs</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">segment_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
      <span class="n">reset_mask</span> <span class="o">=</span> <span class="n">_GeneratePackedInputResetMask</span><span class="p">(</span>
          <span class="n">segment_id</span><span class="p">,</span> <span class="n">is_reverse</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">reverse</span><span class="p">)</span>
      <span class="n">reset_mask</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">reset_mask</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">paddings</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">reset_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">paddings</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">reverse</span><span class="p">:</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">FlipUpDown</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">]</span>
      <span class="n">paddings</span> <span class="o">=</span> <span class="n">FlipUpDown</span><span class="p">(</span><span class="n">paddings</span><span class="p">)</span>
      <span class="n">reset_mask</span> <span class="o">=</span> <span class="n">FlipUpDown</span><span class="p">(</span><span class="n">reset_mask</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">state0</span><span class="p">:</span>
      <span class="n">inputs0</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
          <span class="n">act</span><span class="o">=</span><span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">],</span>
          <span class="n">padding</span><span class="o">=</span><span class="n">paddings</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:],</span>
          <span class="n">reset_mask</span><span class="o">=</span><span class="n">reset_mask</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:])</span>
      <span class="n">state0</span> <span class="o">=</span> <span class="n">rcell</span><span class="o">.</span><span class="n">zero_state</span><span class="p">(</span><span class="n">rcell</span><span class="o">.</span><span class="n">batch_size</span><span class="p">(</span><span class="n">inputs0</span><span class="p">))</span>

    <span class="n">inputs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
        <span class="n">act</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">paddings</span><span class="p">,</span> <span class="n">reset_mask</span><span class="o">=</span><span class="n">reset_mask</span><span class="p">)</span>

    <span class="n">acc_state</span><span class="p">,</span> <span class="n">final_state</span> <span class="o">=</span> <span class="n">recurrent</span><span class="o">.</span><span class="n">Recurrent</span><span class="p">(</span>
        <span class="n">theta</span><span class="o">=</span><span class="n">theta</span><span class="o">.</span><span class="n">cell</span><span class="p">,</span>
        <span class="n">state0</span><span class="o">=</span><span class="n">state0</span><span class="p">,</span>
        <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
        <span class="n">cell_fn</span><span class="o">=</span><span class="n">rcell</span><span class="o">.</span><span class="n">FProp</span><span class="p">,</span>
        <span class="n">accumulator_layer</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
        <span class="n">allow_implicit_capture</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">allow_implicit_capture</span><span class="p">)</span>

    <span class="n">act</span> <span class="o">=</span> <span class="n">rcell</span><span class="o">.</span><span class="n">GetOutput</span><span class="p">(</span><span class="n">acc_state</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">reverse</span><span class="p">:</span>
      <span class="n">act</span> <span class="o">=</span> <span class="n">FlipUpDown</span><span class="p">(</span><span class="n">act</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">act</span><span class="p">,</span> <span class="n">final_state</span></div></div>


<div class="viewcode-block" id="BidirectionalFRNN"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.BidirectionalFRNN">[docs]</a><span class="k">class</span> <span class="nc">BidirectionalFRNN</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">LayerBase</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Bidirectional functional RNN.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="BidirectionalFRNN.Params"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.BidirectionalFRNN.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">BidirectionalFRNN</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;fwd&#39;</span><span class="p">,</span> <span class="n">rnn_cell</span><span class="o">.</span><span class="n">LSTMCellSimple</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
             <span class="s1">&#39;Configs for the forward RNN cell.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;bak&#39;</span><span class="p">,</span> <span class="n">rnn_cell</span><span class="o">.</span><span class="n">LSTMCellSimple</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
             <span class="s1">&#39;Configs for the backward RNN cell.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;packed_input&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;To reset states for packed inputs.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="nd">@base_layer</span><span class="o">.</span><span class="n">initializer</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">BidirectionalFRNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">params</span>
    <span class="n">cluster</span> <span class="o">=</span> <span class="n">cluster_factory</span><span class="o">.</span><span class="n">Current</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">use_tpu</span><span class="p">():</span>
      <span class="n">fwd_device</span> <span class="o">=</span> <span class="n">cluster</span><span class="o">.</span><span class="n">WorkerDeviceInModelSplit</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
      <span class="n">bwd_device</span> <span class="o">=</span> <span class="n">cluster</span><span class="o">.</span><span class="n">WorkerDeviceInModelSplit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">fwd_device</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
      <span class="n">bwd_device</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">fwd_device</span><span class="p">):</span>
      <span class="n">params_forward</span> <span class="o">=</span> <span class="n">FRNN</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
      <span class="n">params_forward</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;fwd&#39;</span>
      <span class="n">params_forward</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">dtype</span>
      <span class="n">params_forward</span><span class="o">.</span><span class="n">reverse</span> <span class="o">=</span> <span class="kc">False</span>
      <span class="n">params_forward</span><span class="o">.</span><span class="n">packed_input</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span>
      <span class="n">params_forward</span><span class="o">.</span><span class="n">cell</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">fwd</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;fwd_rnn&#39;</span><span class="p">,</span> <span class="n">params_forward</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">bwd_device</span><span class="p">):</span>
      <span class="n">params_backward</span> <span class="o">=</span> <span class="n">FRNN</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
      <span class="n">params_backward</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;bak&#39;</span>
      <span class="n">params_backward</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">dtype</span>
      <span class="n">params_backward</span><span class="o">.</span><span class="n">reverse</span> <span class="o">=</span> <span class="kc">True</span>
      <span class="n">params_backward</span><span class="o">.</span><span class="n">packed_input</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span>
      <span class="n">params_backward</span><span class="o">.</span><span class="n">cell</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">bak</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;bak_rnn&#39;</span><span class="p">,</span> <span class="n">params_backward</span><span class="p">)</span>

<div class="viewcode-block" id="BidirectionalFRNN.FProp"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.BidirectionalFRNN.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute bidi-RNN forward pass.</span>

<span class="sd">    `rcell_forward` unroll the sequence in the forward direction and</span>
<span class="sd">    `rcell_backward` unroll the sequence in the backward direction. The</span>
<span class="sd">    outputs are concatenated in the last output dim.</span>

<span class="sd">    See `FRNN.FProp` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this</span>
<span class="sd">        layer and its children layers.</span>
<span class="sd">      inputs: A single tensor or a tuple of tensors with cardinality equal to</span>
<span class="sd">          rnn_cell.inputs_arity. For every input tensor, the first dimension is</span>
<span class="sd">          assumed to be time, second dimension batch, and third dimension depth.</span>
<span class="sd">      paddings: A tensor. First dim is time, second dim is batch, and third dim</span>
<span class="sd">          is expected to be 1.</span>
<span class="sd">      segment_id: A tensor to support packed inputs. First dim is time, second</span>
<span class="sd">          dim is batch, and third dim is expected to be 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tensor of [time, batch, dims].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>

      <span class="k">def</span> <span class="nf">Fwd</span><span class="p">():</span>
        <span class="sd">&quot;&quot;&quot;Run the forward pass.&quot;&quot;&quot;</span>
        <span class="n">output_forward</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fwd_rnn</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span>
            <span class="n">theta</span><span class="o">.</span><span class="n">fwd_rnn</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">segment_id</span><span class="o">=</span><span class="n">segment_id</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output_forward</span>

      <span class="k">def</span> <span class="nf">Bwd</span><span class="p">():</span>
        <span class="sd">&quot;&quot;&quot;Run the backward pass.</span>

<span class="sd">        Returns:</span>
<span class="sd">          A tensor of [time, batch, dims]. The final recurrent state.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">output_backward</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bak_rnn</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span>
            <span class="n">theta</span><span class="o">.</span><span class="n">bak_rnn</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">segment_id</span><span class="o">=</span><span class="n">segment_id</span><span class="p">)</span>
        <span class="c1"># TODO(yonghui/zhifengc): In the current implementation, we copy</span>
        <span class="c1"># output_forward from gpu:0 to gpu:1, and then copy the concatenated</span>
        <span class="c1"># output from gpu:1 to gpu:0 to enable next layer computation. It might</span>
        <span class="c1"># be more efficient to only copy output_backward from gpu:1 to gpu:0 to</span>
        <span class="c1"># reduce cross-gpu data transfer.</span>
        <span class="k">return</span> <span class="n">output_backward</span>

      <span class="c1"># On TPU, we run both direction&#39;s RNNs on one device to reduce memory</span>
      <span class="c1"># usage.</span>
      <span class="n">cluster</span> <span class="o">=</span> <span class="n">cluster_factory</span><span class="o">.</span><span class="n">Current</span><span class="p">()</span>
      <span class="n">fwd_device</span> <span class="o">=</span> <span class="n">cluster</span><span class="o">.</span><span class="n">WorkerDeviceInModelSplit</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
      <span class="n">bwd_device</span> <span class="o">=</span> <span class="n">cluster</span><span class="o">.</span><span class="n">WorkerDeviceInModelSplit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">fwd_device</span><span class="p">):</span>
        <span class="n">output_forward</span> <span class="o">=</span> <span class="n">Fwd</span><span class="p">()</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">bwd_device</span><span class="p">):</span>
        <span class="n">output_backward</span> <span class="o">=</span> <span class="n">Bwd</span><span class="p">()</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">fwd_device</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">output_forward</span><span class="p">,</span> <span class="n">output_backward</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="BidirectionalRNN"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.BidirectionalRNN">[docs]</a><span class="k">class</span> <span class="nc">BidirectionalRNN</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">LayerBase</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Statically unrolled bidirectional RNN.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="BidirectionalRNN.Params"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.BidirectionalRNN.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">BidirectionalRNN</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;fwd&#39;</span><span class="p">,</span> <span class="n">rnn_cell</span><span class="o">.</span><span class="n">LSTMCellSimple</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
             <span class="s1">&#39;Configs for the forward RNN cell.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;bak&#39;</span><span class="p">,</span> <span class="n">rnn_cell</span><span class="o">.</span><span class="n">LSTMCellSimple</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
             <span class="s1">&#39;Configs for the backward RNN cell.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;sequence_length&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Sequence length.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;rnn&#39;</span><span class="p">,</span> <span class="n">RNN</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span> <span class="s1">&#39;Config for underlying RNNs&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;packed_input&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;To reset states for packed inputs.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="nd">@base_layer</span><span class="o">.</span><span class="n">initializer</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">BidirectionalRNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;Packed input is currently not supported &#39;</span>
                                     <span class="s1">&#39;by BiDirectionalRNN&#39;</span><span class="p">)</span>
    <span class="n">params_forward</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
    <span class="n">params_forward</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">%s</span><span class="s1">_forward&#39;</span> <span class="o">%</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span>
    <span class="n">params_forward</span><span class="o">.</span><span class="n">cell</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">fwd</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
    <span class="n">params_forward</span><span class="o">.</span><span class="n">sequence_length</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">sequence_length</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;fwd_rnn&#39;</span><span class="p">,</span> <span class="n">params_forward</span><span class="p">)</span>
    <span class="n">params_backward</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
    <span class="n">params_backward</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">%s</span><span class="s1">_backward&#39;</span> <span class="o">%</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span>
    <span class="n">params_backward</span><span class="o">.</span><span class="n">cell</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">bak</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
    <span class="n">params_backward</span><span class="o">.</span><span class="n">sequence_length</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">sequence_length</span>
    <span class="n">params_backward</span><span class="o">.</span><span class="n">reverse</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;bak_rnn&#39;</span><span class="p">,</span> <span class="n">params_backward</span><span class="p">)</span>

<div class="viewcode-block" id="BidirectionalRNN.FProp"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.BidirectionalRNN.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute bidi-RNN forward pass.</span>

<span class="sd">    `rcell_forward` is responsible for unrolling the sequence in the forward</span>
<span class="sd">    direction and `rcell_backward` in the backward direction. Output from</span>
<span class="sd">    forward and backward rnns are concatenated on the last output dim.</span>

<span class="sd">    See `RNN.FProp()` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this</span>
<span class="sd">        layer and its children layers.</span>
<span class="sd">      inputs: A single tensor or a tuple of tensors with cardinality equal to</span>
<span class="sd">          rnn_cell.inputs_arity. For every input tensor, the first dimension is</span>
<span class="sd">          assumed to be time, second dimension batch, and third dimension depth.</span>
<span class="sd">      paddings: A tensor. First dim is time, second dim is batch, and third dim</span>
<span class="sd">          is expected to be 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tensor of [time, batch, dims].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="n">outputs_forward</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fwd_rnn</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">fwd_rnn</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">)</span>
      <span class="n">outputs_backward</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bak_rnn</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">bak_rnn</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">outputs_forward</span><span class="p">,</span> <span class="n">outputs_backward</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="BidirectionalRNNV2"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.BidirectionalRNNV2">[docs]</a><span class="k">class</span> <span class="nc">BidirectionalRNNV2</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">LayerBase</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Statically unrolled bidirectional RNN.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="BidirectionalRNNV2.Params"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.BidirectionalRNNV2.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">BidirectionalRNNV2</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;fwd&#39;</span><span class="p">,</span> <span class="n">rnn_cell</span><span class="o">.</span><span class="n">LSTMCellSimple</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
             <span class="s1">&#39;Configs for the forward RNN cell.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;bak&#39;</span><span class="p">,</span> <span class="n">rnn_cell</span><span class="o">.</span><span class="n">LSTMCellSimple</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
             <span class="s1">&#39;Configs for the backward RNN cell.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;sequence_length&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Sequence length.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;packed_input&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;To reset states for packed inputs.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="nd">@base_layer</span><span class="o">.</span><span class="n">initializer</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">BidirectionalRNNV2</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">packed_input</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">,</span> <span class="p">(</span>
        <span class="s1">&#39;Packed input is currently not &#39;</span>
        <span class="s1">&#39;supported by BiDirectionalRNNV2&#39;</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">BidirectionalRNN</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">packed_input</span>
    <span class="n">p</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">%s</span><span class="s1">_brnn&#39;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">name</span>
    <span class="n">p</span><span class="o">.</span><span class="n">fwd</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">fwd</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">bak</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">bak</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">sequence_length</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">sequence_length</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;brnn&#39;</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>

<div class="viewcode-block" id="BidirectionalRNNV2._PadSequenceToLength"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.BidirectionalRNNV2._PadSequenceToLength">[docs]</a>  <span class="k">def</span> <span class="nf">_PadSequenceToLength</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t_input</span><span class="p">,</span> <span class="n">length</span><span class="p">,</span> <span class="n">pad_value</span><span class="p">):</span>
    <span class="n">t_input</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">with_dependencies</span><span class="p">(</span>
        <span class="p">[</span><span class="n">py_utils</span><span class="o">.</span><span class="n">assert_less_equal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">t_input</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">length</span><span class="p">)],</span> <span class="n">t_input</span><span class="p">)</span>
    <span class="n">pad_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([[</span><span class="n">length</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">t_input</span><span class="p">)[</span><span class="mi">0</span><span class="p">]],</span>
                           <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">t_input</span><span class="p">)[</span><span class="mi">1</span><span class="p">:]],</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">pad_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">t_input</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="o">+</span> <span class="n">pad_value</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">t_input</span><span class="p">,</span> <span class="n">padding</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span></div>

<div class="viewcode-block" id="BidirectionalRNNV2.FProp"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.BidirectionalRNNV2.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute bidi-RNN forward pass.</span>

<span class="sd">    rcell_forward is responsible for unrolling the sequence in the forward</span>
<span class="sd">    direction and rcell_backward in the backward direction. Output from forward</span>
<span class="sd">    and backward rnns are concatenated on the last output dim.</span>

<span class="sd">    See RNN.FProp() for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this</span>
<span class="sd">        layer and its children layers.</span>
<span class="sd">      inputs: A single tensor or a tuple of tensors with cardinality equal to</span>
<span class="sd">          rnn_cell.inputs_arity. For every input tensor, the first dimension is</span>
<span class="sd">          assumed to be time, second dimension batch, and third dimension depth.</span>
<span class="sd">      paddings: A tensor. First dim is time, second dim is batch, and third dim</span>
<span class="sd">          is expected to be 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tensor of [time, batch, dims].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">inputs</span><span class="p">]</span>
    <span class="n">seq_len</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">paddings</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_PadSequenceToLength</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">sequence_length</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">inputs</span>
    <span class="p">]</span>
    <span class="n">inputs_sequence</span> <span class="o">=</span> <span class="p">[</span>
        <span class="nb">list</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">unstack</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">sequence_length</span><span class="p">))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">inputs</span>
    <span class="p">]</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs_sequence</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
      <span class="n">inputs_sequence</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">inputs_sequence</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">inputs_sequence</span> <span class="o">=</span> <span class="n">inputs_sequence</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">paddings_sequence</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">unstack</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_PadSequenceToLength</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">sequence_length</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span>
            <span class="n">num</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">sequence_length</span><span class="p">))</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs_sequence</span><span class="p">)</span> <span class="o">==</span> <span class="n">p</span><span class="o">.</span><span class="n">sequence_length</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">paddings_sequence</span><span class="p">)</span> <span class="o">==</span> <span class="n">p</span><span class="o">.</span><span class="n">sequence_length</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">brnn</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">brnn</span><span class="p">,</span> <span class="n">inputs_sequence</span><span class="p">,</span> <span class="n">paddings_sequence</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">out</span><span class="p">)[:</span><span class="n">seq_len</span><span class="p">,]</span></div></div>


<div class="viewcode-block" id="CuDNNLSTM"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.CuDNNLSTM">[docs]</a><span class="k">class</span> <span class="nc">CuDNNLSTM</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">LayerBase</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A single layer of unidirectional LSTM with Cudnn impl.</span>

<span class="sd">  Runs training with CuDNN on GPU, and eval using a FRNN with properly</span>
<span class="sd">  configured `LSTMCellSimple` cell.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="CuDNNLSTM.Params"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.CuDNNLSTM.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">CuDNNLSTM</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;cell&#39;</span><span class="p">,</span> <span class="n">rnn_cell</span><span class="o">.</span><span class="n">LSTMCellSimple</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
             <span class="s1">&#39;Configs for the RNN cell used in eval mode.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;reverse&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
             <span class="s1">&#39;Whether or not to unroll the sequence in reversed order.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;packed_input&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;To reset states for packed inputs.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="nd">@base_layer</span><span class="o">.</span><span class="n">initializer</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">CuDNNLSTM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">packed_input</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;Packed input is currently not &#39;</span>
                                               <span class="s1">&#39;supported by CuDNNLSTM&#39;</span><span class="p">)</span>
    <span class="n">_AssertCellParamsCuDNNCompatible</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">cell</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">is_eval</span><span class="p">:</span>
      <span class="c1"># Use the cell&#39;s name as variable scope such that vars in train and eval</span>
      <span class="c1"># modes stay in the same scope.</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">cell</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
        <span class="n">cudnn_init_helper</span> <span class="o">=</span> <span class="n">cudnn_rnn_utils</span><span class="o">.</span><span class="n">CuDNNLSTMInitializer</span><span class="p">(</span>
            <span class="n">p</span><span class="o">.</span><span class="n">cell</span><span class="o">.</span><span class="n">num_input_nodes</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">cell</span><span class="o">.</span><span class="n">num_output_nodes</span><span class="p">)</span>
        <span class="n">wb_pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">init</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">collections</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_VariableCollections</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span>
            <span class="s1">&#39;wb&#39;</span><span class="p">,</span>
            <span class="n">wb_pc</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">AddGlobalVN</span><span class="p">,</span>
            <span class="n">init_wrapper</span><span class="o">=</span><span class="n">cudnn_init_helper</span><span class="o">.</span><span class="n">InitOpaqueParams</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vars</span><span class="o">.</span><span class="n">wb</span><span class="o">.</span><span class="n">approx_size</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">cudnn_init_helper</span><span class="o">.</span><span class="n">weight_size</span> <span class="o">+</span> <span class="n">cudnn_init_helper</span><span class="o">.</span><span class="n">bias_size</span><span class="p">)</span>
      <span class="c1"># Create saveable in the outer_scope, thus saved canonicals are in</span>
      <span class="c1"># variable_scope: $outer_scope/p.cell.name</span>
      <span class="n">saveable</span> <span class="o">=</span> <span class="n">cudnn_rnn_utils</span><span class="o">.</span><span class="n">CuDNNLSTMSaveable</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">vars</span><span class="o">.</span><span class="n">wb</span><span class="p">,</span>
          <span class="n">p</span><span class="o">.</span><span class="n">cell</span><span class="o">.</span><span class="n">num_output_nodes</span><span class="p">,</span>
          <span class="n">p</span><span class="o">.</span><span class="n">cell</span><span class="o">.</span><span class="n">num_input_nodes</span><span class="p">,</span>
          <span class="n">p</span><span class="o">.</span><span class="n">cell</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
          <span class="n">name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">vars</span><span class="o">.</span><span class="n">wb</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;_saveable&#39;</span><span class="p">)</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">SAVEABLE_OBJECTS</span><span class="p">,</span> <span class="n">saveable</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">frnn_p</span> <span class="o">=</span> <span class="n">FRNN</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
      <span class="n">frnn_p</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;rnn&#39;</span>
      <span class="n">frnn_p</span><span class="o">.</span><span class="n">cell</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">cell</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
      <span class="n">frnn_p</span><span class="o">.</span><span class="n">reverse</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">reverse</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;rnn&#39;</span><span class="p">,</span> <span class="n">frnn_p</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">rnn_cell</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">cell</span>

<div class="viewcode-block" id="CuDNNLSTM.zero_state"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.CuDNNLSTM.zero_state">[docs]</a>  <span class="k">def</span> <span class="nf">zero_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">is_eval</span><span class="p">:</span>
      <span class="n">zero_m</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">cell</span><span class="o">.</span><span class="n">num_output_nodes</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="n">zero_c</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">cell</span><span class="o">.</span><span class="n">num_output_nodes</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">m</span><span class="o">=</span><span class="n">zero_m</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">zero_c</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">cell</span><span class="o">.</span><span class="n">init_state</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span></div>

<div class="viewcode-block" id="CuDNNLSTM.FProp"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.CuDNNLSTM.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">state0</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute LSTM forward pass.</span>

<span class="sd">    Runs training pass with CuDNN. Eval is implemented without CuDNN to be</span>
<span class="sd">    compatible with different hardwares.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this</span>
<span class="sd">        layer and its children layers.</span>
<span class="sd">      inputs: A tensor of [seq_length, batch_size, num_input_nodes]</span>
<span class="sd">      paddings: A tensor of [seq_length, batch_size, 1]</span>
<span class="sd">      state0: If not None, the initial rnn state in a NestedMap. Defaults</span>
<span class="sd">        to the cell&#39;s zero-state.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tensor of [seq_length, batch_size, num_output_nodes] and the</span>
<span class="sd">      final recurrent state.  Because cudnn_rnn does not apply padding</span>
<span class="sd">      every step, the final state returned differs from other RNN</span>
<span class="sd">      layers even if the inputs, paddings and state0 given are</span>
<span class="sd">      identical.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">is_eval</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">rnn</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">state0</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">state0</span><span class="p">:</span>
      <span class="n">batch_dim</span> <span class="o">=</span> <span class="mi">1</span>
      <span class="n">state0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">zero_state</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">paddings</span><span class="p">)[</span><span class="n">batch_dim</span><span class="p">])</span>
    <span class="n">state_h</span><span class="p">,</span> <span class="n">state_c</span> <span class="o">=</span> <span class="n">state0</span><span class="o">.</span><span class="n">m</span><span class="p">,</span> <span class="n">state0</span><span class="o">.</span><span class="n">c</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">reverse</span><span class="p">:</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">_ReversePaddedSequence</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">)</span>
    <span class="n">output</span><span class="p">,</span> <span class="n">output_h</span><span class="p">,</span> <span class="n">output_c</span> <span class="o">=</span> <span class="n">cudnn_rnn_ops</span><span class="o">.</span><span class="n">cudnn_lstm</span><span class="p">(</span>
        <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
        <span class="n">input_h</span><span class="o">=</span><span class="n">state_h</span><span class="p">,</span>
        <span class="n">input_c</span><span class="o">=</span><span class="n">state_c</span><span class="p">,</span>
        <span class="n">params</span><span class="o">=</span><span class="n">theta</span><span class="o">.</span><span class="n">wb</span><span class="p">,</span>
        <span class="n">is_training</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">input_mode</span><span class="o">=</span><span class="s1">&#39;linear_input&#39;</span><span class="p">,</span>
        <span class="n">direction</span><span class="o">=</span><span class="s1">&#39;unidirectional&#39;</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">reverse</span><span class="p">:</span>
      <span class="n">output</span> <span class="o">=</span> <span class="n">_ReversePaddedSequence</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">paddings</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">m</span><span class="o">=</span><span class="n">output_h</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">output_c</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="BidirectionalNativeCuDNNLSTM"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.BidirectionalNativeCuDNNLSTM">[docs]</a><span class="k">class</span> <span class="nc">BidirectionalNativeCuDNNLSTM</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">LayerBase</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A single layer of bidirectional LSTM with native Cudnn impl.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="BidirectionalNativeCuDNNLSTM.Params"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.BidirectionalNativeCuDNNLSTM.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">BidirectionalNativeCuDNNLSTM</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;fwd&#39;</span><span class="p">,</span> <span class="n">rnn_cell</span><span class="o">.</span><span class="n">LSTMCellSimple</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
             <span class="s1">&#39;Configs for the forward RNN cell used in eval mode.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;bak&#39;</span><span class="p">,</span> <span class="n">rnn_cell</span><span class="o">.</span><span class="n">LSTMCellSimple</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
             <span class="s1">&#39;Configs for the backward RNN cell used in eval mode.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;packed_input&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;To reset states for packed inputs.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="nd">@base_layer</span><span class="o">.</span><span class="n">initializer</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">BidirectionalNativeCuDNNLSTM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">packed_input</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">,</span> <span class="p">(</span>
        <span class="s1">&#39;Packed input is currently not &#39;</span>
        <span class="s1">&#39;supported by BidirectionalNativeCuDNNLSTM&#39;</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">fwd</span><span class="o">.</span><span class="n">num_input_nodes</span> <span class="o">==</span> <span class="n">p</span><span class="o">.</span><span class="n">bak</span><span class="o">.</span><span class="n">num_input_nodes</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">fwd</span><span class="o">.</span><span class="n">num_output_nodes</span> <span class="o">==</span> <span class="n">p</span><span class="o">.</span><span class="n">bak</span><span class="o">.</span><span class="n">num_output_nodes</span>
    <span class="n">_AssertCellParamsCuDNNCompatible</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">fwd</span><span class="p">)</span>
    <span class="n">_AssertCellParamsCuDNNCompatible</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">bak</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">is_eval</span><span class="p">:</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
        <span class="n">cudnn_init_helper</span> <span class="o">=</span> <span class="n">cudnn_rnn_utils</span><span class="o">.</span><span class="n">CuDNNLSTMInitializer</span><span class="p">(</span>
            <span class="n">p</span><span class="o">.</span><span class="n">fwd</span><span class="o">.</span><span class="n">num_input_nodes</span><span class="p">,</span>
            <span class="n">p</span><span class="o">.</span><span class="n">fwd</span><span class="o">.</span><span class="n">num_output_nodes</span><span class="p">,</span>
            <span class="n">direction</span><span class="o">=</span><span class="n">cudnn_rnn_ops</span><span class="o">.</span><span class="n">CUDNN_RNN_BIDIRECTION</span><span class="p">)</span>
        <span class="n">wb_pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">init</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">collections</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_VariableCollections</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span>
            <span class="s1">&#39;wb&#39;</span><span class="p">,</span>
            <span class="n">wb_pc</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">AddGlobalVN</span><span class="p">,</span>
            <span class="n">init_wrapper</span><span class="o">=</span><span class="n">cudnn_init_helper</span><span class="o">.</span><span class="n">InitOpaqueParams</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vars</span><span class="o">.</span><span class="n">wb</span><span class="o">.</span><span class="n">approx_size</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">cudnn_init_helper</span><span class="o">.</span><span class="n">weight_size</span> <span class="o">+</span> <span class="n">cudnn_init_helper</span><span class="o">.</span><span class="n">bias_size</span><span class="p">)</span>
      <span class="c1"># Create saveable in the outer_scope, thus saved canonicals are in</span>
      <span class="c1"># variable_scope: $outer_scope/p.cell.name</span>
      <span class="n">saveable</span> <span class="o">=</span> <span class="n">cudnn_rnn_utils</span><span class="o">.</span><span class="n">BidiCuDNNLSTMSaveable</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">vars</span><span class="o">.</span><span class="n">wb</span><span class="p">,</span>
          <span class="n">p</span><span class="o">.</span><span class="n">fwd</span><span class="o">.</span><span class="n">num_output_nodes</span><span class="p">,</span>
          <span class="n">p</span><span class="o">.</span><span class="n">fwd</span><span class="o">.</span><span class="n">num_input_nodes</span><span class="p">,</span>
          <span class="n">p</span><span class="o">.</span><span class="n">fwd</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
          <span class="n">p</span><span class="o">.</span><span class="n">bak</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
          <span class="n">name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">vars</span><span class="o">.</span><span class="n">wb</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;_saveable&#39;</span><span class="p">)</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">SAVEABLE_OBJECTS</span><span class="p">,</span> <span class="n">saveable</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">bidi_frnn_p</span> <span class="o">=</span> <span class="n">BidirectionalFRNN</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
      <span class="n">bidi_frnn_p</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;rnn&#39;</span>
      <span class="n">bidi_frnn_p</span><span class="o">.</span><span class="n">fwd</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">fwd</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
      <span class="n">bidi_frnn_p</span><span class="o">.</span><span class="n">bak</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">bak</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;rnn&#39;</span><span class="p">,</span> <span class="n">bidi_frnn_p</span><span class="p">)</span>

<div class="viewcode-block" id="BidirectionalNativeCuDNNLSTM.zero_state"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.BidirectionalNativeCuDNNLSTM.zero_state">[docs]</a>  <span class="k">def</span> <span class="nf">zero_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">is_eval</span><span class="p">:</span>
      <span class="n">zero_m</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
          <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">fwd</span><span class="o">.</span><span class="n">num_output_nodes</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">fwd</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="n">zero_c</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
          <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">fwd</span><span class="o">.</span><span class="n">num_output_nodes</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">fwd</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">m</span><span class="o">=</span><span class="n">zero_m</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">zero_c</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">fwd</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fwd_rnn</span><span class="o">.</span><span class="n">cell</span><span class="o">.</span><span class="n">zero_state</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
      <span class="n">bak</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bak_rnn</span><span class="o">.</span><span class="n">cell</span><span class="o">.</span><span class="n">zero_state</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
          <span class="n">m</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">fwd</span><span class="o">.</span><span class="n">m</span><span class="p">,</span> <span class="n">bak</span><span class="o">.</span><span class="n">m</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
          <span class="n">c</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">fwd</span><span class="o">.</span><span class="n">c</span><span class="p">,</span> <span class="n">bak</span><span class="o">.</span><span class="n">c</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span></div>

<div class="viewcode-block" id="BidirectionalNativeCuDNNLSTM.FProp"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.BidirectionalNativeCuDNNLSTM.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute bidirectional LSTM forward pass.</span>

<span class="sd">    Runs training pass with CuDNN. Eval is implemented without CuDNN to be</span>
<span class="sd">    compatible with different hardwares.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this</span>
<span class="sd">        layer and its children layers.</span>
<span class="sd">      inputs: A tensor of [seq_length, batch_size, num_input_nodes]</span>
<span class="sd">      paddings: A tensor of [seq_length, batch_size, 1]</span>
<span class="sd">    Returns:</span>
<span class="sd">      A tensor of [seq_length, batch_size, 2 * num_output_nodes]</span>

<span class="sd">    See `DRNN.FProp()` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">is_eval</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">rnn</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="n">batch_dim</span> <span class="o">=</span> <span class="mi">1</span>
      <span class="n">state0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">zero_state</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)[</span><span class="n">batch_dim</span><span class="p">])</span>
      <span class="n">output</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">cudnn_rnn_ops</span><span class="o">.</span><span class="n">cudnn_lstm</span><span class="p">(</span>
          <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
          <span class="n">input_h</span><span class="o">=</span><span class="n">state0</span><span class="o">.</span><span class="n">m</span><span class="p">,</span>
          <span class="n">input_c</span><span class="o">=</span><span class="n">state0</span><span class="o">.</span><span class="n">c</span><span class="p">,</span>
          <span class="n">params</span><span class="o">=</span><span class="n">theta</span><span class="o">.</span><span class="n">wb</span><span class="p">,</span>
          <span class="n">is_training</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
          <span class="n">input_mode</span><span class="o">=</span><span class="s1">&#39;linear_input&#39;</span><span class="p">,</span>
          <span class="n">direction</span><span class="o">=</span><span class="s1">&#39;bidirectional&#39;</span><span class="p">,</span>
          <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">output</span></div></div>


<div class="viewcode-block" id="_ConcatLastDim"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers._ConcatLastDim">[docs]</a><span class="k">def</span> <span class="nf">_ConcatLastDim</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Concatenates all args along the last dimension.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span></div>


<div class="viewcode-block" id="_ShiftRight"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers._ShiftRight">[docs]</a><span class="k">def</span> <span class="nf">_ShiftRight</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">xs</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Shifts xs[:-1] one step to the right and attaches x0 on the left.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([[</span><span class="n">x0</span><span class="p">],</span> <span class="n">xs</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></div>


<div class="viewcode-block" id="FRNNWithAttention"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.FRNNWithAttention">[docs]</a><span class="k">class</span> <span class="nc">FRNNWithAttention</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">LayerBase</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;An RNN layer intertwined with an attention layer.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="FRNNWithAttention.Params"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.FRNNWithAttention.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">FRNNWithAttention</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;cell&#39;</span><span class="p">,</span> <span class="n">rnn_cell</span><span class="o">.</span><span class="n">LSTMCellSimple</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
             <span class="s1">&#39;Configs for the RNN cell.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;attention&#39;</span><span class="p">,</span> <span class="n">attention</span><span class="o">.</span><span class="n">AdditiveAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
             <span class="s1">&#39;Attention used by this layer.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;output_prev_atten_ctx&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s1">&#39;If True, output previous attention context for each position.&#39;</span>
        <span class="s1">&#39;Otherwise, output current attention context.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;use_zero_atten_state&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s1">&#39;To use zero attention state instead of computing attention with &#39;</span>
        <span class="s1">&#39;zero query vector.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;atten_context_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Size of attention context.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;packed_input&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;To reset states for packed inputs.&#39;</span><span class="p">)</span>
    <span class="c1"># Set p.attention.atten_dropout_deterministic to True by default.</span>
    <span class="n">p</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">atten_dropout_deterministic</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="nd">@base_layer</span><span class="o">.</span><span class="n">initializer</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">FRNNWithAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_zero_atten_state</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">atten_context_dim</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span>
          <span class="s1">&#39;atten_context_dim needs to be set when &#39;</span>
          <span class="s1">&#39;intializing attention state and context with 0.&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">use_zero_atten_state</span><span class="p">,</span> <span class="p">(</span>
          <span class="s1">&#39;Packed input is only supported when &#39;</span>
          <span class="s1">&#39;training with zero initial attention states.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">cell</span><span class="o">.</span><span class="n">reset_cell_state</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;cell&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">cell</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">packed_input</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;atten&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">attention</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">rnn_cell</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">cell</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">attention</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">atten</span>

<div class="viewcode-block" id="FRNNWithAttention.InitForSourcePacked"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.FRNNWithAttention.InitForSourcePacked">[docs]</a>  <span class="k">def</span> <span class="nf">InitForSourcePacked</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                          <span class="n">theta</span><span class="p">,</span>
                          <span class="n">src_encs</span><span class="p">,</span>
                          <span class="n">src_paddings</span><span class="p">,</span>
                          <span class="n">src_contexts</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                          <span class="n">src_segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A wrapper of InitForSourcePacked of child attention layer.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this</span>
<span class="sd">        layer and its children layers.</span>
<span class="sd">      src_encs: A tensor of shape [source_seq_length, batch_size, source_dim].</span>
<span class="sd">      src_paddings: A tensor of shape [source_seq_length, batch_size].</span>
<span class="sd">      src_contexts: [Optional] If specified, must be a tensor of shape</span>
<span class="sd">        [source_seq_length, batch_size, some_dim]. When specified, this tensor</span>
<span class="sd">        will be used as the source context vectors when computing attention</span>
<span class="sd">        context, and src_ends will be only used to compute the attention score</span>
<span class="sd">        for each context. If set to None, the &#39;src_encs&#39; will be used as</span>
<span class="sd">        source context.</span>
<span class="sd">      src_segment_id: A tensor of shape [source_seq_length, batch_size], to</span>
<span class="sd">        support packed inputs.</span>

<span class="sd">    Returns:</span>
<span class="sd">      packed_src - A `.NestedMap` containing packed source.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">atten</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">atten</span>

    <span class="k">if</span> <span class="n">src_contexts</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">src_contexts</span> <span class="o">=</span> <span class="n">src_encs</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">packed_input</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">src_segment_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

    <span class="c1"># Initial attention state.</span>
    <span class="p">(</span><span class="n">source_vec</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span>
     <span class="n">source_segment_id</span><span class="p">)</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">InitForSourcePacked</span><span class="p">(</span>
         <span class="n">theta</span><span class="o">=</span><span class="n">theta</span><span class="o">.</span><span class="n">atten</span><span class="p">,</span>
         <span class="n">source_vecs</span><span class="o">=</span><span class="n">src_encs</span><span class="p">,</span>
         <span class="n">source_contexts</span><span class="o">=</span><span class="n">src_contexts</span><span class="p">,</span>
         <span class="n">source_padding</span><span class="o">=</span><span class="n">src_paddings</span><span class="p">,</span>
         <span class="n">source_segment_id</span><span class="o">=</span><span class="n">src_segment_id</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
        <span class="n">source_vec</span><span class="o">=</span><span class="n">source_vec</span><span class="p">,</span>
        <span class="n">source_contexts</span><span class="o">=</span><span class="n">source_contexts</span><span class="p">,</span>
        <span class="n">source_padding</span><span class="o">=</span><span class="n">source_padding</span><span class="p">,</span>
        <span class="n">source_segment_id</span><span class="o">=</span><span class="n">source_segment_id</span><span class="p">)</span></div>

<div class="viewcode-block" id="FRNNWithAttention.zero_state"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.FRNNWithAttention.zero_state">[docs]</a>  <span class="k">def</span> <span class="nf">zero_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">theta</span><span class="p">,</span>
                 <span class="n">src_encs</span><span class="p">,</span>
                 <span class="n">packed_src</span><span class="p">,</span>
                 <span class="n">batch_size</span><span class="p">,</span>
                 <span class="n">atten_state_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initial state of this layer.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this</span>
<span class="sd">        layer and its children layers.</span>
<span class="sd">      src_encs: A tensor of shape [source_seq_length, batch_size, source_dim].</span>
<span class="sd">      packed_src: A `.NestedMap` containing packed source.</span>
<span class="sd">      batch_size: Batch size.</span>
<span class="sd">      atten_state_dim: Attention state dim when using zero_atten_state</span>

<span class="sd">    Returns:</span>
<span class="sd">      state0 - A `.NestedMap` containing initial states of RNN and attention.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">atten</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">atten</span>
    <span class="c1"># Initial RNN states.</span>
    <span class="n">state0</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">rnn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cell</span><span class="o">.</span><span class="n">zero_state</span><span class="p">(</span><span class="n">batch_size</span><span class="p">))</span>

    <span class="n">s_seq_len</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">src_encs</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">zero_atten_state</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">ZeroAttentionState</span><span class="p">(</span><span class="n">s_seq_len</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
    <span class="n">state0</span><span class="o">.</span><span class="n">step_state</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
        <span class="n">global_step</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">GetOrCreateGlobalStep</span><span class="p">(),</span>
        <span class="n">time_step</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_zero_atten_state</span><span class="p">:</span>
      <span class="n">zero_atten_context</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
          <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">atten_context_dim</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">src_encs</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="n">state0</span><span class="o">.</span><span class="n">atten</span> <span class="o">=</span> <span class="n">zero_atten_context</span>
      <span class="n">state0</span><span class="o">.</span><span class="n">atten_state</span> <span class="o">=</span> <span class="n">zero_atten_state</span>
      <span class="n">state0</span><span class="o">.</span><span class="n">atten_probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
          <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">s_seq_len</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">zero_atten_state</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">state0</span><span class="o">.</span><span class="n">atten</span><span class="p">,</span> <span class="n">state0</span><span class="o">.</span><span class="n">atten_probs</span><span class="p">,</span> <span class="n">state0</span><span class="o">.</span><span class="n">atten_state</span> <span class="o">=</span> <span class="p">(</span>
          <span class="n">atten</span><span class="o">.</span><span class="n">ComputeContextVectorWithSource</span><span class="p">(</span>
              <span class="n">theta</span><span class="o">.</span><span class="n">atten</span><span class="p">,</span>
              <span class="n">packed_src</span><span class="o">.</span><span class="n">source_vec</span><span class="p">,</span>
              <span class="n">packed_src</span><span class="o">.</span><span class="n">source_contexts</span><span class="p">,</span>
              <span class="n">packed_src</span><span class="o">.</span><span class="n">source_padding</span><span class="p">,</span>
              <span class="n">packed_src</span><span class="o">.</span><span class="n">source_segment_id</span><span class="p">,</span>
              <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                  <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">cell</span><span class="o">.</span><span class="n">num_output_nodes</span><span class="p">],</span>
                  <span class="n">dtype</span><span class="o">=</span><span class="n">packed_src</span><span class="o">.</span><span class="n">source_vec</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
              <span class="n">zero_atten_state</span><span class="p">,</span>
              <span class="n">step_state</span><span class="o">=</span><span class="n">state0</span><span class="o">.</span><span class="n">step_state</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">state0</span></div>

<div class="viewcode-block" id="FRNNWithAttention.reset_atten_state"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.FRNNWithAttention.reset_atten_state">[docs]</a>  <span class="k">def</span> <span class="nf">reset_atten_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="n">state</span><span class="o">.</span><span class="n">atten</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">reset_mask</span> <span class="o">*</span> <span class="n">state</span><span class="o">.</span><span class="n">atten</span>
    <span class="n">state</span><span class="o">.</span><span class="n">atten_state</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">reset_mask</span> <span class="o">*</span> <span class="n">state</span><span class="o">.</span><span class="n">atten_state</span>
    <span class="n">state</span><span class="o">.</span><span class="n">atten_probs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">reset_mask</span> <span class="o">*</span> <span class="n">state</span><span class="o">.</span><span class="n">atten_probs</span>
    <span class="k">return</span> <span class="n">state</span></div>

<div class="viewcode-block" id="FRNNWithAttention.FProp"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.FRNNWithAttention.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">theta</span><span class="p">,</span>
            <span class="n">src_encs</span><span class="p">,</span>
            <span class="n">src_paddings</span><span class="p">,</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">paddings</span><span class="p">,</span>
            <span class="n">src_contexts</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">state0</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">src_segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Forward propagate through a rnn layer with attention.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this</span>
<span class="sd">        layer and its children layers.</span>
<span class="sd">      src_encs: A tensor of shape [source_seq_length, batch_size, source_dim].</span>
<span class="sd">      src_paddings: A tensor of shape [source_seq_length, batch_size].</span>
<span class="sd">      inputs: A tensor of [time, batch, dims].</span>
<span class="sd">      paddings: A tensor of [time, batch, 1].</span>
<span class="sd">      src_contexts: [Optional] If specified, must be a tensor of shape</span>
<span class="sd">        [source_seq_length, batch_size, some_dim]. When specified, this tensor</span>
<span class="sd">        will be used as the source context vectors when computing attention</span>
<span class="sd">        context, and src_ends will be only used to compute the attention score</span>
<span class="sd">        for each context. If set to None, the &#39;src_encs&#39; will be used as</span>
<span class="sd">        source context.</span>
<span class="sd">      state0: [Optional] If not None, the initial rnn state and attention</span>
<span class="sd">        context in a NestedMap. Defaults to the cell&#39;s zero-state.</span>
<span class="sd">      src_segment_id: A tensor of shape [source_seq_length, batch_size] to</span>
<span class="sd">          support masking with packed inputs.</span>
<span class="sd">      segment_id: A tensor of [time, batch, 1].</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tuple (atten_context, rnn_output, atten_probs, final_state).</span>

<span class="sd">      - atten_context: a tensor of [time, batch, attention.hidden_dim].</span>
<span class="sd">      - rnn_output: a tensor of [time, batch, rcell.num_output_nodes].</span>
<span class="sd">      - atten_probs: a tensor of [time, batch, source_seq_length].</span>
<span class="sd">      - final_state: The final recurrent state.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">dtype</span>
    <span class="n">rcell</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cell</span>
    <span class="n">atten</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">atten</span>
    <span class="k">assert</span> <span class="n">dtype</span> <span class="o">==</span> <span class="n">rcell</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">dtype</span>
    <span class="k">assert</span> <span class="n">dtype</span> <span class="o">==</span> <span class="n">atten</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">dtype</span>
    <span class="k">assert</span> <span class="n">rcell</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">inputs_arity</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">rcell</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">inputs_arity</span> <span class="o">==</span> <span class="mi">2</span>
    <span class="k">if</span> <span class="n">segment_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">segment_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">paddings</span><span class="p">)</span>

    <span class="n">inputs_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">batch</span> <span class="o">=</span> <span class="n">inputs_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">inputs_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">input_dim</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)[</span><span class="mi">2</span><span class="p">]</span>

    <span class="n">packed_src</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">InitForSourcePacked</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">src_encs</span><span class="p">,</span> <span class="n">src_paddings</span><span class="p">,</span>
                                          <span class="n">src_contexts</span><span class="p">,</span> <span class="n">src_segment_id</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">state0</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">zero_atten_state_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">cell</span><span class="o">.</span><span class="n">num_input_nodes</span> <span class="o">-</span> <span class="n">input_dim</span>
      <span class="n">state0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">zero_state</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">src_encs</span><span class="p">,</span> <span class="n">packed_src</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span>
                               <span class="n">zero_atten_state_dim</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;packed input is only supported with &#39;</span>
                                       <span class="s1">&#39;default initial states.&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">CellFn</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">state0</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Computes one step forward.&quot;&quot;&quot;</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span><span class="p">:</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">act</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">state0_mod</span> <span class="o">=</span> <span class="n">state0</span><span class="o">.</span><span class="n">DeepCopy</span><span class="p">()</span>
        <span class="n">state0_mod</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reset_atten_state</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">state0_mod</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">state0_mod</span> <span class="o">=</span> <span class="n">state0</span>
      <span class="n">state1</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">step_state</span><span class="o">=</span><span class="n">state0_mod</span><span class="o">.</span><span class="n">step_state</span><span class="p">)</span>
      <span class="n">state1</span><span class="o">.</span><span class="n">rnn</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">rcell</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span>
          <span class="n">theta</span><span class="o">.</span><span class="n">rnn</span><span class="p">,</span> <span class="n">state0_mod</span><span class="o">.</span><span class="n">rnn</span><span class="p">,</span>
          <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
              <span class="n">act</span><span class="o">=</span><span class="p">[</span><span class="n">_ConcatLastDim</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">act</span><span class="p">,</span> <span class="n">state0_mod</span><span class="o">.</span><span class="n">atten</span><span class="p">)]</span>
              <span class="k">if</span> <span class="n">rcell</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">inputs_arity</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span>
              <span class="p">[</span><span class="n">inputs</span><span class="o">.</span><span class="n">act</span><span class="p">,</span> <span class="n">state0_mod</span><span class="o">.</span><span class="n">atten</span><span class="p">],</span>
              <span class="n">padding</span><span class="o">=</span><span class="n">inputs</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
              <span class="n">reset_mask</span><span class="o">=</span><span class="n">inputs</span><span class="o">.</span><span class="n">reset_mask</span><span class="p">))</span>

      <span class="n">state1</span><span class="o">.</span><span class="n">atten</span><span class="p">,</span> <span class="n">state1</span><span class="o">.</span><span class="n">atten_probs</span><span class="p">,</span> <span class="n">state1</span><span class="o">.</span><span class="n">atten_state</span> <span class="o">=</span> <span class="p">(</span>
          <span class="n">atten</span><span class="o">.</span><span class="n">ComputeContextVectorWithSource</span><span class="p">(</span>
              <span class="n">theta</span><span class="o">.</span><span class="n">atten</span><span class="p">,</span>
              <span class="n">theta</span><span class="o">.</span><span class="n">source_vec</span><span class="p">,</span>
              <span class="n">theta</span><span class="o">.</span><span class="n">source_contexts</span><span class="p">,</span>
              <span class="n">theta</span><span class="o">.</span><span class="n">source_padding</span><span class="p">,</span>
              <span class="n">theta</span><span class="o">.</span><span class="n">source_segment_id</span><span class="p">,</span>
              <span class="n">rcell</span><span class="o">.</span><span class="n">GetOutput</span><span class="p">(</span><span class="n">state1</span><span class="o">.</span><span class="n">rnn</span><span class="p">),</span>
              <span class="n">state0_mod</span><span class="o">.</span><span class="n">atten_state</span><span class="p">,</span>
              <span class="n">step_state</span><span class="o">=</span><span class="n">state0_mod</span><span class="o">.</span><span class="n">step_state</span><span class="p">,</span>
              <span class="n">query_segment_id</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">segment_id</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
      <span class="n">state1</span><span class="o">.</span><span class="n">step_state</span><span class="o">.</span><span class="n">time_step</span> <span class="o">+=</span> <span class="mi">1</span>
      <span class="k">return</span> <span class="n">state1</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span><span class="p">:</span>
      <span class="n">reset_mask</span> <span class="o">=</span> <span class="n">_GeneratePackedInputResetMask</span><span class="p">(</span><span class="n">segment_id</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">reset_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">paddings</span><span class="p">)</span>

    <span class="n">acc_state</span><span class="p">,</span> <span class="n">final_state</span> <span class="o">=</span> <span class="n">recurrent</span><span class="o">.</span><span class="n">Recurrent</span><span class="p">(</span>
        <span class="n">theta</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
            <span class="n">rnn</span><span class="o">=</span><span class="n">theta</span><span class="o">.</span><span class="n">cell</span><span class="p">,</span>
            <span class="n">source_vec</span><span class="o">=</span><span class="n">packed_src</span><span class="o">.</span><span class="n">source_vec</span><span class="p">,</span>
            <span class="n">source_contexts</span><span class="o">=</span><span class="n">packed_src</span><span class="o">.</span><span class="n">source_contexts</span><span class="p">,</span>
            <span class="n">source_padding</span><span class="o">=</span><span class="n">packed_src</span><span class="o">.</span><span class="n">source_padding</span><span class="p">,</span>
            <span class="n">source_segment_id</span><span class="o">=</span><span class="n">packed_src</span><span class="o">.</span><span class="n">source_segment_id</span><span class="p">,</span>
            <span class="n">atten</span><span class="o">=</span><span class="n">theta</span><span class="o">.</span><span class="n">atten</span><span class="p">),</span>
        <span class="n">state0</span><span class="o">=</span><span class="n">state0</span><span class="p">,</span>
        <span class="n">inputs</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
            <span class="n">act</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="n">paddings</span><span class="p">,</span>
            <span class="n">reset_mask</span><span class="o">=</span><span class="n">reset_mask</span><span class="p">,</span>
            <span class="n">segment_id</span><span class="o">=</span><span class="n">segment_id</span><span class="p">),</span>
        <span class="n">cell_fn</span><span class="o">=</span><span class="n">CellFn</span><span class="p">,</span>
        <span class="n">accumulator_layer</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
        <span class="n">allow_implicit_capture</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">allow_implicit_capture</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">output_prev_atten_ctx</span><span class="p">:</span>
      <span class="c1"># Add the initial attention context in and drop the attention context</span>
      <span class="c1"># in the last position so that the output atten_ctx is previous</span>
      <span class="c1"># attention context for each target position.</span>
      <span class="n">atten_ctx</span> <span class="o">=</span> <span class="n">_ShiftRight</span><span class="p">(</span><span class="n">state0</span><span class="o">.</span><span class="n">atten</span><span class="p">,</span> <span class="n">acc_state</span><span class="o">.</span><span class="n">atten</span><span class="p">)</span>
      <span class="n">atten_probs</span> <span class="o">=</span> <span class="n">_ShiftRight</span><span class="p">(</span><span class="n">state0</span><span class="o">.</span><span class="n">atten_probs</span><span class="p">,</span> <span class="n">acc_state</span><span class="o">.</span><span class="n">atten_probs</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">atten_ctx</span> <span class="o">=</span> <span class="n">acc_state</span><span class="o">.</span><span class="n">atten</span>
      <span class="n">atten_probs</span> <span class="o">=</span> <span class="n">acc_state</span><span class="o">.</span><span class="n">atten_probs</span>

    <span class="k">return</span> <span class="n">atten_ctx</span><span class="p">,</span> <span class="n">rcell</span><span class="o">.</span><span class="n">GetOutput</span><span class="p">(</span><span class="n">acc_state</span><span class="o">.</span><span class="n">rnn</span><span class="p">),</span> <span class="n">atten_probs</span><span class="p">,</span> <span class="n">final_state</span></div></div>


<div class="viewcode-block" id="MultiSourceFRNNWithAttention"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.MultiSourceFRNNWithAttention">[docs]</a><span class="k">class</span> <span class="nc">MultiSourceFRNNWithAttention</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">LayerBase</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;RNN layer intertwined with an attention layer for multiple sources.</span>

<span class="sd">  Allows different attention params per source, if attention is not shared.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="MultiSourceFRNNWithAttention.Params"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.MultiSourceFRNNWithAttention.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Params for this MultiSourceFRNNWithAttention class.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">MultiSourceFRNNWithAttention</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;cell&#39;</span><span class="p">,</span>
        <span class="n">rnn_cell</span><span class="o">.</span><span class="n">LSTMCellSimple</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
            <span class="n">params_init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="mf">0.04</span><span class="p">)),</span>
        <span class="s1">&#39;Configs for the RNN cell.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;attention_tpl&#39;</span><span class="p">,</span> <span class="n">attention</span><span class="o">.</span><span class="n">AdditiveAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
        <span class="s1">&#39;Attention used by this attention layer, can be overridden by &#39;</span>
        <span class="s1">&#39;source_name_to_attention_params.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;atten_merger&#39;</span><span class="p">,</span> <span class="n">layers_with_attention</span><span class="o">.</span><span class="n">MergerLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
        <span class="s1">&#39;Merger layer config for combining context vectors computed for &#39;</span>
        <span class="s1">&#39;different source encodings.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;source_names&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;List of source names.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;share_attention&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;If set single attention layer shared.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;source_name_to_attention_params&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s1">&#39;Can be set if share_attention is False. Allows defining &#39;</span>
        <span class="s1">&#39;different attention params per source in a dictionary, eg. &#39;</span>
        <span class="s1">&#39;{&quot;src1&quot;: atten_tpl1, &quot;src2&quot;: atten_tpl2}&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;packed_input&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;To reset states for packed inputs.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">rnn_cell</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Reference to the RNN cell of this layer.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">cell</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">attention</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Reference to the attention layer(s) of this layer.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">attentions</span>

  <span class="nd">@base_layer</span><span class="o">.</span><span class="n">initializer</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a MultiSourceFRNNWithAttention layer with params.&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">MultiSourceFRNNWithAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;packed input is not supported for &#39;</span>
                                     <span class="s1">&#39;MultiSourceFRNNWithAttention&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">atten_merger</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Merger layer cannot be none!&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">source_names</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">source_names</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Source names must be a non-empty list.&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">share_attention</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">source_name_to_attention_params</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s1">&#39;Cant specify source_name_to_attention_params with share_attention.&#39;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;cell&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">cell</span><span class="p">)</span>

    <span class="c1"># Initialize attention layer(s).</span>
    <span class="n">params_atten</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_source_dims</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">src_to_att</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">source_name_to_attention_params</span>
    <span class="k">for</span> <span class="n">src_name</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">source_names</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">src_to_att</span> <span class="ow">and</span> <span class="n">src_name</span> <span class="ow">in</span> <span class="n">src_to_att</span><span class="p">:</span>
        <span class="n">att_params</span> <span class="o">=</span> <span class="n">src_to_att</span><span class="p">[</span><span class="n">src_name</span><span class="p">]</span>
        <span class="n">att_params</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;atten_</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">src_name</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">att_params</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
        <span class="n">att_params</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;atten_shared&#39;</span>
                           <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">share_attention</span> <span class="k">else</span> <span class="s1">&#39;atten_</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">src_name</span><span class="p">))</span>
      <span class="k">if</span> <span class="n">att_params</span><span class="o">.</span><span class="n">params_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">att_params</span><span class="o">.</span><span class="n">params_init</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Gaussian</span><span class="p">(</span>
            <span class="mf">1.</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">att_params</span><span class="o">.</span><span class="n">source_dim</span> <span class="o">+</span> <span class="n">att_params</span><span class="o">.</span><span class="n">query_dim</span><span class="p">))</span>
      <span class="n">params_atten</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">att_params</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_source_dims</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">att_params</span><span class="o">.</span><span class="n">source_dim</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">share_attention</span><span class="p">:</span>
        <span class="k">break</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChildren</span><span class="p">(</span><span class="s1">&#39;attentions&#39;</span><span class="p">,</span> <span class="n">params_atten</span><span class="p">)</span>

    <span class="c1"># Initialize merger layer for attention layer(s).</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">atten_merger</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
    <span class="n">params</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;atten_merger&#39;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;atten_merger&#39;</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

<div class="viewcode-block" id="MultiSourceFRNNWithAttention.InitAttention"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.MultiSourceFRNNWithAttention.InitAttention">[docs]</a>  <span class="k">def</span> <span class="nf">InitAttention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">src_encs</span><span class="p">,</span> <span class="n">src_paddings</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes initial states for attention layer(s).</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this</span>
<span class="sd">        layer and its children layers.</span>
<span class="sd">      src_encs: A `.NestedMap` object containing source encoding tensors,</span>
<span class="sd">        each of shape [source_seq_length, batch_size, source_dim]. Children</span>
<span class="sd">        names of the nested map is defined by source_names.</span>
<span class="sd">      src_paddings: A `.NestedMap` object contraining source padding tensors,</span>
<span class="sd">        each of shape [source_seq_length, batch_size]. Children names of the</span>
<span class="sd">        nested map is defined by source_names.</span>
<span class="sd">      batch_size: Scalar Tensor of type int, for initial state shape.</span>

<span class="sd">    Returns:</span>
<span class="sd">      state0 - Initial attention-rnn state in a NestedMap. Zeros for the rnn</span>
<span class="sd">      initial state, and merger output for attention initial state.</span>

<span class="sd">      Transformed source vectors and transposed source vectors.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_params</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">dtype</span>
    <span class="n">rcell</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cell</span>

    <span class="c1"># Initial RNN states, theta and auxiliary variables.</span>
    <span class="n">state0</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">rnn</span><span class="o">=</span><span class="n">rcell</span><span class="o">.</span><span class="n">zero_state</span><span class="p">(</span><span class="n">batch_size</span><span class="p">))</span>
    <span class="n">query_vec0</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">cell</span><span class="o">.</span><span class="n">num_output_nodes</span><span class="p">],</span> <span class="n">dtype</span><span class="p">)</span>

    <span class="n">ctxs0</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">transformed_src_vecs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">()</span>
    <span class="n">transposed_src_ctxs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">()</span>
    <span class="n">src_ps</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">()</span>
    <span class="n">src_seg_ids</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">src_name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">source_names</span><span class="p">):</span>
      <span class="n">att_idx</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">share_attention</span> <span class="k">else</span> <span class="n">i</span><span class="p">)</span>

      <span class="p">(</span><span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span>
       <span class="n">source_segment_id</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attentions</span><span class="p">[</span><span class="n">att_idx</span><span class="p">]</span><span class="o">.</span><span class="n">InitForSourcePacked</span><span class="p">(</span>
           <span class="n">theta</span><span class="o">.</span><span class="n">attentions</span><span class="p">[</span><span class="n">att_idx</span><span class="p">],</span> <span class="n">src_encs</span><span class="p">[</span><span class="n">src_name</span><span class="p">],</span> <span class="n">src_encs</span><span class="p">[</span><span class="n">src_name</span><span class="p">],</span>
           <span class="n">src_paddings</span><span class="p">[</span><span class="n">src_name</span><span class="p">])</span>

      <span class="n">transformed_src_vecs</span><span class="p">[</span><span class="n">src_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">source_vecs</span>
      <span class="n">transposed_src_ctxs</span><span class="p">[</span><span class="n">src_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">source_contexts</span>
      <span class="n">src_ps</span><span class="p">[</span><span class="n">src_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">source_padding</span>
      <span class="n">src_seg_ids</span><span class="p">[</span><span class="n">src_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">source_segment_id</span>

      <span class="c1"># Initial attention state.</span>
      <span class="n">s_seq_len</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">src_encs</span><span class="p">[</span><span class="n">src_name</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">zero_atten_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attentions</span><span class="p">[</span><span class="n">att_idx</span><span class="p">]</span><span class="o">.</span><span class="n">ZeroAttentionState</span><span class="p">(</span>
          <span class="n">s_seq_len</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
      <span class="n">ctxs0</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">attentions</span><span class="p">[</span><span class="n">att_idx</span><span class="p">]</span><span class="o">.</span><span class="n">ComputeContextVectorWithSource</span><span class="p">(</span>
          <span class="n">theta</span><span class="o">.</span><span class="n">attentions</span><span class="p">[</span><span class="n">att_idx</span><span class="p">],</span> <span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span>
          <span class="n">source_padding</span><span class="p">,</span> <span class="n">source_segment_id</span><span class="p">,</span> <span class="n">query_vec0</span><span class="p">,</span> <span class="n">zero_atten_state</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>

    <span class="c1"># Initial attention state is the output of merger-op.</span>
    <span class="n">state0</span><span class="o">.</span><span class="n">atten</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">atten_merger</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">atten_merger</span><span class="p">,</span> <span class="n">ctxs0</span><span class="p">,</span>
                                           <span class="n">query_vec0</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">state0</span><span class="p">,</span> <span class="n">transformed_src_vecs</span><span class="p">,</span> <span class="n">transposed_src_ctxs</span><span class="p">,</span> <span class="n">src_ps</span><span class="p">,</span>
            <span class="n">src_seg_ids</span><span class="p">)</span></div>

<div class="viewcode-block" id="MultiSourceFRNNWithAttention.FProp"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.MultiSourceFRNNWithAttention.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">src_encs</span><span class="p">,</span> <span class="n">src_paddings</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Forward propagate through a RNN layer with attention(s).</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this</span>
<span class="sd">        layer and its children layers.</span>
<span class="sd">      src_encs: A `.NestedMap` object containing source encoding tensors,</span>
<span class="sd">        each of shape [source_seq_length, batch_size, source_dim]. Children</span>
<span class="sd">        names of the nested map is defined by source_names.</span>
<span class="sd">      src_paddings: A `.NestedMap` object contraining source padding tensors,</span>
<span class="sd">        each of shape [source_seq_length, batch_size]. Children names of the</span>
<span class="sd">        nested map is defined by source_names.</span>
<span class="sd">      inputs: A tensor of [time, batch, dims].</span>
<span class="sd">      paddings: A tensor of [time, batch, 1].</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tuple (atten_context, rnn_output)</span>

<span class="sd">      - atten_context: a tensor of [time, batch, attention.hidden_dim].</span>
<span class="sd">      - rnn_output: a tensor of [time, batch, p.cell.num_output_nodes].</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: dtype mismatch of attention layers.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">dtype</span>
    <span class="n">rcell</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cell</span>
    <span class="n">attentions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attentions</span>
    <span class="k">assert</span> <span class="n">rcell</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">inputs_arity</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">rcell</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">inputs_arity</span> <span class="o">==</span> <span class="mi">2</span>
    <span class="k">assert</span> <span class="n">dtype</span> <span class="o">==</span> <span class="n">rcell</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">dtype</span>
    <span class="k">for</span> <span class="n">atten</span> <span class="ow">in</span> <span class="n">attentions</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">dtype</span> <span class="o">!=</span> <span class="n">atten</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Data type mismatch!&#39;</span><span class="p">)</span>

    <span class="c1"># Check if all batch sizes and depths match for source encs and paddings.</span>
    <span class="n">src_name_0</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">source_names</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">src_encs</span><span class="p">[</span><span class="n">src_name_0</span><span class="p">]</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">with_dependencies</span><span class="p">([</span>
        <span class="n">assert_shape_match</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">src_encs</span><span class="p">[</span><span class="n">src_name_0</span><span class="p">])[</span><span class="mi">1</span><span class="p">],</span> <span class="n">source_dim</span><span class="p">],</span>
                           <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">src_encs</span><span class="p">[</span><span class="n">src_name_i</span><span class="p">])[</span><span class="o">-</span><span class="mi">2</span><span class="p">:])</span>
        <span class="k">for</span> <span class="n">src_name_i</span><span class="p">,</span> <span class="n">source_dim</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">source_names</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_source_dims</span><span class="p">)</span>
    <span class="p">],</span> <span class="n">src_encs</span><span class="p">[</span><span class="n">src_name_0</span><span class="p">])</span>
    <span class="n">src_paddings</span><span class="p">[</span><span class="n">src_name_0</span><span class="p">]</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">with_dependencies</span><span class="p">([</span>
        <span class="n">py_utils</span><span class="o">.</span><span class="n">assert_equal</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">src_paddings</span><span class="p">[</span><span class="n">src_name_0</span><span class="p">])[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">src_paddings</span><span class="p">[</span><span class="n">src_name_i</span><span class="p">])[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">src_name_i</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">source_names</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
    <span class="p">],</span> <span class="n">src_paddings</span><span class="p">[</span><span class="n">src_name_0</span><span class="p">])</span>

    <span class="c1"># Compute source transformations and initial rnn states.</span>
    <span class="p">(</span><span class="n">state0</span><span class="p">,</span> <span class="n">transformed_src_vecs</span><span class="p">,</span> <span class="n">transposed_src_ctxs</span><span class="p">,</span> <span class="n">src_padding</span><span class="p">,</span>
     <span class="n">src_seg_id</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">InitAttention</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">src_encs</span><span class="p">,</span> <span class="n">src_paddings</span><span class="p">,</span>
                                      <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span>

    <span class="c1"># Collect individual attention parameters for CellFn.</span>
    <span class="n">attens_theta</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">({</span>
        <span class="n">src_name</span><span class="p">:</span> <span class="n">theta</span><span class="o">.</span><span class="n">attentions</span><span class="p">[</span><span class="mi">0</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">share_attention</span> <span class="k">else</span> <span class="n">i</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">src_name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">source_names</span><span class="p">)</span>
    <span class="p">})</span>

    <span class="k">def</span> <span class="nf">CellFn</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">state0</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Computes one step forward.&quot;&quot;&quot;</span>
      <span class="n">state1</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">()</span>
      <span class="n">state1</span><span class="o">.</span><span class="n">rnn</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">rcell</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span>
          <span class="n">theta</span><span class="o">.</span><span class="n">rnn</span><span class="p">,</span> <span class="n">state0</span><span class="o">.</span><span class="n">rnn</span><span class="p">,</span>
          <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
              <span class="n">act</span><span class="o">=</span><span class="p">[</span><span class="n">_ConcatLastDim</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">act</span><span class="p">,</span> <span class="n">state0</span><span class="o">.</span><span class="n">atten</span><span class="p">)],</span>
              <span class="n">padding</span><span class="o">=</span><span class="n">inputs</span><span class="o">.</span><span class="n">padding</span><span class="p">))</span>

      <span class="c1"># The ordering in local_ctxs follows p.source_names.</span>
      <span class="n">local_ctxs</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="n">query_vec</span> <span class="o">=</span> <span class="n">rcell</span><span class="o">.</span><span class="n">GetOutput</span><span class="p">(</span><span class="n">state1</span><span class="o">.</span><span class="n">rnn</span><span class="p">)</span>
      <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">src_name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">source_names</span><span class="p">):</span>
        <span class="n">att_idx</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">share_attention</span> <span class="k">else</span> <span class="n">i</span><span class="p">)</span>
        <span class="n">local_ctxs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">attentions</span><span class="p">[</span><span class="n">att_idx</span><span class="p">]</span><span class="o">.</span><span class="n">ComputeContextVectorWithSource</span><span class="p">(</span>
            <span class="n">theta</span><span class="o">.</span><span class="n">attens</span><span class="p">[</span><span class="n">src_name</span><span class="p">],</span> <span class="n">theta</span><span class="o">.</span><span class="n">src_vecs</span><span class="p">[</span><span class="n">src_name</span><span class="p">],</span>
            <span class="n">theta</span><span class="o">.</span><span class="n">src_ctxs</span><span class="p">[</span><span class="n">src_name</span><span class="p">],</span> <span class="n">theta</span><span class="o">.</span><span class="n">src_p</span><span class="p">[</span><span class="n">src_name</span><span class="p">],</span>
            <span class="n">theta</span><span class="o">.</span><span class="n">src_seg_id</span><span class="p">[</span><span class="n">src_name</span><span class="p">],</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">state0</span><span class="o">.</span><span class="n">atten</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
      <span class="n">state1</span><span class="o">.</span><span class="n">atten</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">atten_merger</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">atten_merger</span><span class="p">,</span> <span class="n">local_ctxs</span><span class="p">,</span>
                                             <span class="n">query_vec</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">state1</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">()</span>

    <span class="c1"># Note that, we have a nested map for each parameter.</span>
    <span class="n">acc_state</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">recurrent</span><span class="o">.</span><span class="n">Recurrent</span><span class="p">(</span>
        <span class="n">theta</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
            <span class="n">rnn</span><span class="o">=</span><span class="n">theta</span><span class="o">.</span><span class="n">cell</span><span class="p">,</span>
            <span class="n">attens</span><span class="o">=</span><span class="n">attens_theta</span><span class="p">,</span>
            <span class="n">src_p</span><span class="o">=</span><span class="n">src_padding</span><span class="p">,</span>
            <span class="n">src_vecs</span><span class="o">=</span><span class="n">transformed_src_vecs</span><span class="p">,</span>
            <span class="n">src_ctxs</span><span class="o">=</span><span class="n">transposed_src_ctxs</span><span class="p">,</span>
            <span class="n">src_seg_id</span><span class="o">=</span><span class="n">src_seg_id</span><span class="p">,</span>
            <span class="n">atten_merger</span><span class="o">=</span><span class="n">theta</span><span class="o">.</span><span class="n">atten_merger</span><span class="p">),</span>
        <span class="n">state0</span><span class="o">=</span><span class="n">state0</span><span class="p">,</span>
        <span class="n">inputs</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">act</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">paddings</span><span class="p">),</span>
        <span class="n">cell_fn</span><span class="o">=</span><span class="n">CellFn</span><span class="p">,</span>
        <span class="n">accumulator_layer</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
        <span class="n">allow_implicit_capture</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">allow_implicit_capture</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">acc_state</span><span class="o">.</span><span class="n">atten</span><span class="p">,</span> <span class="n">rcell</span><span class="o">.</span><span class="n">GetOutput</span><span class="p">(</span><span class="n">acc_state</span><span class="o">.</span><span class="n">rnn</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="BidirectionalFRNNQuasi"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.BidirectionalFRNNQuasi">[docs]</a><span class="k">class</span> <span class="nc">BidirectionalFRNNQuasi</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">LayerBase</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Bidirectional functional Quasi-RNN.</span>

<span class="sd">  This is very similar to BidirectionalFRNN except the input is a list of the</span>
<span class="sd">  forward and backward inputs. It is split because quasi-rnns do the</span>
<span class="sd">  matrix/convolution unrolled over time outside of the recurrent part. Also,</span>
<span class="sd">  this uses quasi-rnn instead of LSTM.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="BidirectionalFRNNQuasi.Params"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.BidirectionalFRNNQuasi.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">BidirectionalFRNNQuasi</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;fwd&#39;</span><span class="p">,</span> <span class="n">rnn_cell</span><span class="o">.</span><span class="n">QRNNPoolingCell</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
             <span class="s1">&#39;Configs for the forward RNN cell.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;bak&#39;</span><span class="p">,</span> <span class="n">rnn_cell</span><span class="o">.</span><span class="n">QRNNPoolingCell</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
             <span class="s1">&#39;Configs for the backward RNN cell.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;packed_input&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;To reset states for packed inputs.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="nd">@base_layer</span><span class="o">.</span><span class="n">initializer</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">BidirectionalFRNNQuasi</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;packed input is not supported for &#39;</span>
                                     <span class="s1">&#39;BidirectionalFRNNQuasi&#39;</span><span class="p">)</span>
    <span class="n">params_forward</span> <span class="o">=</span> <span class="n">FRNN</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">params_forward</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;fwd&#39;</span>
    <span class="n">params_forward</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">dtype</span>
    <span class="n">params_forward</span><span class="o">.</span><span class="n">reverse</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">params_forward</span><span class="o">.</span><span class="n">cell</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">fwd</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;fwd_rnn&#39;</span><span class="p">,</span> <span class="n">params_forward</span><span class="p">)</span>

    <span class="n">params_backward</span> <span class="o">=</span> <span class="n">FRNN</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">params_backward</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;bak&#39;</span>
    <span class="n">params_backward</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">dtype</span>
    <span class="n">params_backward</span><span class="o">.</span><span class="n">reverse</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">params_backward</span><span class="o">.</span><span class="n">cell</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">bak</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;bak_rnn&#39;</span><span class="p">,</span> <span class="n">params_backward</span><span class="p">)</span>

<div class="viewcode-block" id="BidirectionalFRNNQuasi.FProp"><a class="viewcode-back" href="../../../lingvo.core.rnn_layers.html#lingvo.core.rnn_layers.BidirectionalFRNNQuasi.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute bidi-quasi-RNN forward pass.</span>

<span class="sd">    fwd_rnn unroll the sequence in the forward direction and</span>
<span class="sd">    bak_rnn unroll the sequence in the backward direction. The</span>
<span class="sd">    outputs are concatenated in the last output dim.</span>

<span class="sd">    See `FRNN.FProp` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this</span>
<span class="sd">        layer and its children layers.</span>
<span class="sd">      inputs: A list of the fwd and bak tensors. Each item in the list should be</span>
<span class="sd">        A single tensor or a tuple of tensors with cardinality equal to</span>
<span class="sd">        rnn_cell.inputs_arity. For every input tensor, the first dimension is</span>
<span class="sd">        assumed to be time, second dimension batch, and third dimension depth.</span>
<span class="sd">      paddings: A tensor. First dim is time, second dim is batch, and third dim</span>
<span class="sd">          is expected to be 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tensor of [time, batch, dims].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="n">cluster</span> <span class="o">=</span> <span class="n">cluster_factory</span><span class="o">.</span><span class="n">Current</span><span class="p">()</span>
      <span class="n">fwd_device</span> <span class="o">=</span> <span class="n">cluster</span><span class="o">.</span><span class="n">WorkerDeviceInModelSplit</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
      <span class="n">bwd_device</span> <span class="o">=</span> <span class="n">cluster</span><span class="o">.</span><span class="n">WorkerDeviceInModelSplit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">fwd_device</span><span class="p">):</span>
        <span class="n">output_forward</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fwd_rnn</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">fwd_rnn</span><span class="p">,</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                               <span class="n">paddings</span><span class="p">)</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">bwd_device</span><span class="p">):</span>
        <span class="n">output_backward</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bak_rnn</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">bak_rnn</span><span class="p">,</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                                                <span class="n">paddings</span><span class="p">)</span>
        <span class="n">output_forward</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">output_forward</span><span class="p">,</span>
                                           <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">output_backward</span><span class="p">))</span>
        <span class="n">out_rank</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">output_forward</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="c1"># TODO(yonghui/zhifengc): In the current implementation, we copy</span>
        <span class="c1"># output_forward from gpu:0 to gpu:1, and then copy the concatenated</span>
        <span class="c1"># output from gpu:1 to gpu:0 to enable next layer computation. It might</span>
        <span class="c1"># be more efficient to only copy output_backward from gpu:1 to gpu:0 to</span>
        <span class="c1"># reduce cross-gpu data transfer.</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">output_forward</span><span class="p">,</span> <span class="n">output_backward</span><span class="p">],</span> <span class="n">out_rank</span><span class="p">)</span></div></div>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../../',
            VERSION:'',
            LANGUAGE:'en',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>